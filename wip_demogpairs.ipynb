{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taweener11/darkSideUnmasked/blob/main/wip_demogpairs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYi8w2zpnKKE",
        "outputId": "680dd592-4361-472b-c07d-b67d4ac98300"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cores = os.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrb_gzkT-CCv",
        "outputId": "78a6fdc7-5370-4eca-8bbb-eac386cea9db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8TM7bpCk3kNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title shell pipeline for unzipping! this needs to run every time\n",
        "\n",
        "!unzip -q \"/content/drive/My Drive/Datasets/demogpairs/DemogPairs.zip\" -d \"/content/demogpairs/\""
      ],
      "metadata": {
        "id": "SS8uNZYunxqx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_metadata_file(filepath, gender_label, race_label):\n",
        "    \"\"\"\n",
        "    Read a DemogPairs metadata txt file and collect image paths with labels.\n",
        "\n",
        "    Args:\n",
        "      filepath (str): path to the metadata txt file\n",
        "      gender_label (int): 0 for female, 1 for male\n",
        "      race_label (str): string label for race, e.g. 'black', 'white', 'asian'\n",
        "\n",
        "    Returns:\n",
        "      List of tuples: (image_relative_path, gender_label, race_label)\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line or line.lower().startswith('db_code'):\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            # parts[1] is the image path relative to DemogPairs folder\n",
        "            img_path = parts[1]\n",
        "            samples.append((img_path, gender_label, race_label))\n",
        "    return samples\n"
      ],
      "metadata": {
        "id": "4uA1b3RswNS5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# metadata_dir = '/content/drive/My Drive/demogpairs/Metadata'  # adjust path\n",
        "metadata_dir = '/content/demogpairs/Metadata'  # edited this to run with the local environment\n",
        "\n",
        "\n",
        "\n",
        "# Map filenames to gender and race labels\n",
        "metadata_info = {\n",
        "    'Black_Females.txt': (0, 'black'),\n",
        "    'Black_Males.txt': (1, 'black'),\n",
        "    'White_Females.txt': (0, 'white'),\n",
        "    'White_Males.txt': (1, 'white'),\n",
        "    'Asian_Females.txt': (0, 'asian'),\n",
        "    'Asian_Males.txt': (1, 'asian')\n",
        "}\n",
        "\n",
        "all_samples = []\n",
        "\n",
        "for fname, (gender, race) in metadata_info.items():\n",
        "    full_path = os.path.join(metadata_dir, fname)\n",
        "    print(f\"Reading {full_path} ...\")\n",
        "    samples = read_metadata_file(full_path, gender, race)\n",
        "    all_samples.extend(samples)\n",
        "\n",
        "print(f\"Total samples loaded: {len(all_samples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwTPzuC57bCe",
        "outputId": "13e96b41-0ff9-4e00-9b4f-988b0682b22b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/demogpairs/Metadata/Black_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/Black_Males.txt ...\n",
            "Reading /content/demogpairs/Metadata/White_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/White_Males.txt ...\n",
            "Reading /content/demogpairs/Metadata/Asian_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/Asian_Males.txt ...\n",
            "Total samples loaded: 10800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "cn679gCpx6pD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a transform that is smaller per suggestion of rasmus\n",
        "\n",
        "image_size = 64\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                          std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "oLYdzNAHyveC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title preliminary analysis on the dataset -- determine the number of examples for subsetting\n",
        "demogpairs_root = '/content/demogpairs/DemogPairs'\n",
        "\n",
        "# Build final dataset list with full paths\n",
        "dataset = []\n",
        "for rel_path, gender, race in all_samples:\n",
        "    img_full_path = os.path.join(demogpairs_root, rel_path)\n",
        "    if os.path.isfile(img_full_path):\n",
        "        dataset.append((img_full_path, gender, race))\n",
        "    else:\n",
        "        print(f\"Missing file: {img_full_path}\")\n",
        "\n",
        "print(f\"Final dataset size after filtering missing files: {len(dataset)}\")\n",
        "\n",
        "# Extract identity labels from the image paths\n",
        "# Assuming identity is the directory name right after demogpairs_root\n",
        "identity_labels_list = []\n",
        "for img_path, gender, race in dataset:\n",
        "    # Split the path and get the second to last element (which should be the identity folder)\n",
        "    parts = img_path.split(os.sep)\n",
        "    # Find the index of demogpairs_root in the parts\n",
        "    try:\n",
        "        root_index = parts.index('DemogPairs')\n",
        "        # The identity folder is expected to be the element after 'DemogPairs'\n",
        "        if root_index + 1 < len(parts):\n",
        "            identity = parts[root_index + 1]\n",
        "            identity_labels_list.append(identity)\n",
        "        else:\n",
        "            # Handle cases where the path doesn't follow the expected structure\n",
        "            print(f\"Warning: Could not extract identity from path: {img_path}\")\n",
        "            identity_labels_list.append(\"unknown_identity\") # Or handle as appropriate\n",
        "    except ValueError:\n",
        "        # Handle cases where 'DemogPairs' is not in the path\n",
        "        print(f\"Warning: 'DemogPairs' not found in path: {img_path}\")\n",
        "        identity_labels_list.append(\"unknown_identity\") # Or handle as appropriate\n",
        "\n",
        "\n",
        "# Convert to a pandas Series for easier counting\n",
        "import pandas as pd\n",
        "# Use the created list of identity labels\n",
        "identity_series = pd.Series(identity_labels_list)\n",
        "\n",
        "identity_counts = identity_series.value_counts()\n",
        "# Select top 1000 identities. Ensure there are at least 1000 unique identities.\n",
        "if len(identity_counts) >= 1000:\n",
        "    top_1000_identities = identity_counts.nlargest(1000)\n",
        "else:\n",
        "    print(f\"Warning: Less than 1000 unique identities found. Using all {len(identity_counts)} identities.\")\n",
        "    top_1000_identities = identity_counts\n",
        "\n",
        "# Get the indices corresponding to the images belonging to the top 1000 identities\n",
        "# We need the original indices from the `dataset` list\n",
        "top_1000_identity_names = top_1000_identities.index.tolist()\n",
        "top_1000_indices = [i for i, (img_path, gender, race) in enumerate(dataset)\n",
        "                    if img_path.split(os.sep)[-2] in top_1000_identity_names]\n",
        "\n",
        "\n",
        "# Create a subset of the dataset containing only the top 1000 identities\n",
        "from torch.utils.data import Subset\n",
        "# You can create a Subset using the original list and the selected indices\n",
        "# Note: Subset is typically used with PyTorch Datasets, not plain Python lists.\n",
        "# If you intend to use this with PyTorch DataLoader later, you might need to\n",
        "# convert 'dataset' into a custom PyTorch Dataset first.\n",
        "# For now, let's just have the list of tuples for the top 1000 identities:\n",
        "dataset_top_1000 = [dataset[i] for i in top_1000_indices]\n",
        "\n",
        "\n",
        "min_samples = top_1000_identities.min()\n",
        "max_samples = top_1000_identities.max()\n",
        "\n",
        "print(f\"Minimum samples per identity: {min_samples}\")\n",
        "print(f\"Maximum samples per identity: {max_samples}\")\n",
        "print(f\"Number of samples in dataset_top_1000: {len(dataset_top_1000)}\")\n",
        "\n",
        "# printing the number of classes per group\n",
        "for key, value in metadata_info.items():\n",
        "    gender, race = value\n",
        "    gender_str = 'male' if gender ==1 else 'female'\n",
        "    count = len([s for s in dataset if s[1] == gender and s[2] == race])\n",
        "    print(f'Count of {race} {gender_str} =' + str(count))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUwGUcDF7fpg",
        "outputId": "a875e430-c107-4556-be59-39c9ca5162a3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataset size after filtering missing files: 10800\n",
            "Warning: Less than 1000 unique identities found. Using all 600 identities.\n",
            "Minimum samples per identity: 18\n",
            "Maximum samples per identity: 18\n",
            "Number of samples in dataset_top_1000: 10800\n",
            "Count of black female =1800\n",
            "Count of black male =1800\n",
            "Count of white female =1800\n",
            "Count of white male =1800\n",
            "Count of asian female =1800\n",
            "Count of asian male =1800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title accessing the relevant indices and subsetting for the testsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "dataset_gender_labels = np.array([s[1] for s in dataset])\n",
        "dataset_race_labels = np.array([s[2] for s in dataset])\n",
        "\n",
        "composite_labels = np.array([f\"{gender}_{race}\" for _, gender, race in dataset])\n",
        "\n",
        "\n",
        "full_indices = np.arange(len(dataset))\n",
        "\n",
        "# stratified train/test split by the composite gender-race label\n",
        "train_indices, test_indices = train_test_split(\n",
        "    full_indices,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=composite_labels # Use the composite labels here\n",
        ")\n",
        "print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
        "\n",
        "train_dataset = []\n",
        "# Iterate through the indices in train_indices\n",
        "for index in train_indices:\n",
        "    # Get the tuple (img_full_path, gender, race) from the original dataset list using the index\n",
        "    img_full_path, gender, race = dataset[index]\n",
        "    # Check if the file exists before adding to the train_dataset (optional, but good practice)\n",
        "    if os.path.isfile(img_full_path):\n",
        "        train_dataset.append((img_full_path, gender, race))\n",
        "    else:\n",
        "        print(f\"Missing file: {img_full_path}\")\n",
        "\n",
        "print(f\"Train dataset size after filtering missing files: {len(train_dataset)}\")\n",
        "\n",
        "# Repeat the same logic for the test_indices to create the test_dataset\n",
        "test_dataset = []\n",
        "for index in test_indices:\n",
        "    img_full_path, gender, race = dataset[index]\n",
        "    if os.path.isfile(img_full_path):\n",
        "        test_dataset.append((img_full_path, gender, race))\n",
        "    else:\n",
        "        print(f\"Missing file: {img_full_path}\")\n",
        "\n",
        "print(f\"Test dataset size after filtering missing files: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULPfCI0pQwFm",
        "outputId": "3da09ab0-eee7-4d3c-abf9-8a0022144e82"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 8640, Test samples: 2160\n",
            "Train dataset size after filtering missing files: 8640\n",
            "Test dataset size after filtering missing files: 2160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title subset maker for specified distribution\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Subset\n",
        "import pandas as pd # Import pandas for value_counts\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "races = ['black', 'white', 'asian']\n",
        "\n",
        "def make_train_subsets_from_list(dataset_list, proportions, subgroup = (0, 'asian')):\n",
        "    \"\"\"\n",
        "    Read a DemogPairs metadata txt file and collect image paths with labels.\n",
        "\n",
        "    Args:\n",
        "      dataset_list (list): A list of tuples (img_full_path, gender, race)\n",
        "      proportions (list): A list of proportions for the subgroup.\n",
        "      subgroup (tuple): (gender, race) for the subgroup to vary.\n",
        "\n",
        "    Returns:\n",
        "      Dict of torch.utils.data.Subset: Subsets of the original list, keyed by proportion.\n",
        "    \"\"\"\n",
        "    train_subsets = {}\n",
        "\n",
        "    # Extract identity labels, gender, and race directly from the input list\n",
        "    dataset_identity_labels = [img_path.split(os.sep)[-2] for img_full_path, _, _ in dataset_list] # Assuming identity is the folder name\n",
        "    dataset_gender_labels = [gender for _, gender, _ in dataset_list]\n",
        "    dataset_race_labels = [race for _, _, race in dataset_list]\n",
        "\n",
        "    # Calculate base_number based on the minimum samples per identity in this list\n",
        "    identity_counts = pd.Series(dataset_identity_labels).value_counts()\n",
        "    base_number = identity_counts.min() if not identity_counts.empty else 0\n",
        "\n",
        "    # Map original identity names to a numerical label for easier processing\n",
        "    unique_identities = np.unique(dataset_identity_labels)\n",
        "    identity_mapping = {name: i for i, name in enumerate(unique_identities)}\n",
        "    numerical_identity_labels = np.array([identity_mapping[name] for name in dataset_identity_labels])\n",
        "\n",
        "    for prop in proportions:\n",
        "        selected_original_indices_for_prop = [] # Collect original indices for the current proportion\n",
        "\n",
        "        # the indices 'c' here refer to the numerical identity labels\n",
        "        for c_num in np.unique(numerical_identity_labels):\n",
        "            # Get the actual identity name\n",
        "            identity_name = unique_identities[c_num]\n",
        "\n",
        "            # Indices within the *current dataset_list* that correspond to identity 'c_num'\n",
        "            indices_for_identity = np.where(numerical_identity_labels == c_num)[0]\n",
        "\n",
        "            # Separate indices by gender and race *within this identity*\n",
        "            main_sg_indices_for_identity = [\n",
        "                idx for idx in indices_for_identity\n",
        "                if dataset_gender_labels[idx] == subgroup[0] and dataset_race_labels[idx] == subgroup[1]\n",
        "            ]\n",
        "\n",
        "            rng.shuffle(main_sg_indices_for_identity)\n",
        "\n",
        "            # Determine number of samples for the main subgroup\n",
        "            if len(main_sg_indices_for_identity) < base_number:\n",
        "                n_main_sg = int(np.floor(len(main_sg_indices_for_identity) * prop))\n",
        "            else:\n",
        "                n_main_sg = int(np.floor(base_number * prop))\n",
        "\n",
        "            # Collect the original indices for the selected main subgroup samples\n",
        "            selected_original_indices_for_prop.extend(main_sg_indices_for_identity[:n_main_sg])\n",
        "\n",
        "            # selecting for the non-main subgroups *within this identity*\n",
        "            for gender in range(2):\n",
        "                for race in races:\n",
        "                    if race != subgroup[1]:\n",
        "                        subgroup_indices_for_identity = [\n",
        "                            idx for idx in indices_for_identity\n",
        "                            if dataset_gender_labels[idx] == gender and dataset_race_labels[idx] == race\n",
        "                        ]\n",
        "                        rng.shuffle(subgroup_indices_for_identity)\n",
        "\n",
        "                        # Calculate how many samples from this subgroup to select\n",
        "                        available_other_subgroups_count = 0\n",
        "                        for g_other in range(2):\n",
        "                            for r_other in races:\n",
        "                                if r_other != subgroup[1]:\n",
        "                                     if any(dataset_gender_labels[idx] == g_other and dataset_race_labels[idx] == r_other for idx in indices_for_identity):\n",
        "                                         available_other_subgroups_count += 1\n",
        "\n",
        "                        if available_other_subgroups_count > 0:\n",
        "                            target_per_other_subgroup = int(np.floor((base_number * (1-prop)) / available_other_subgroups_count))\n",
        "                        else:\n",
        "                             target_per_other_subgroup = 0\n",
        "\n",
        "                        # Number of samples to select for this specific non-main subgroup\n",
        "                        n_subgroup = min(len(subgroup_indices_for_identity), target_per_other_subgroup)\n",
        "\n",
        "                        # Collect the original indices for the selected non-main subgroup samples\n",
        "                        selected_original_indices_for_prop.extend(subgroup_indices_for_identity[:n_subgroup])\n",
        "\n",
        "        # Shuffle the collected original indices for the current proportion\n",
        "        rng.shuffle(selected_original_indices_for_prop)\n",
        "\n",
        "        # Create the Subset using the original list and the selected original indices\n",
        "        train_subsets[prop] = Subset(dataset_list, selected_original_indices_for_prop)\n",
        "\n",
        "    return train_subsets\n"
      ],
      "metadata": {
        "id": "S5ctmKK9q_hw"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_subsets = make_train_subsets_from_list(train_dataset, [0.25, 0.5, 0.75], subgroup = (0, 'asian'))\n",
        "\n",
        "# get the first sample (index 0) from the Subset\n",
        "first_sample = train_subsets[0.25][0]\n",
        "\n",
        "# The structure of first_sample depends on your original dataset's __getitem__ method.\n",
        "# Based on your 'dataset' list and 'make_train_subsets_from_list' function,\n",
        "# the original dataset is a list of tuples (img_full_path, gender, race).\n",
        "# A Subset wrapping this list will return the tuple directly.\n",
        "# The 'transform' passed to the function is not applied by the Subset itself,\n",
        "# but is intended to be used by a DataLoader or a custom Dataset class.\n",
        "\n",
        "print(\"First sample (path, gender, race):\")\n",
        "print(first_sample)\n",
        "\n",
        "# If you want to see more samples, you can loop\n",
        "print(\"\\nFirst 3 samples:\")\n",
        "for i in range(min(3, len(train_subsets[0.25]))):\n",
        "    print(train_subsets[0.25][i])\n",
        "\n",
        "# If your intention was to see the transformations applied, you would use a DataLoader:\n",
        "train_loader_0_25 = DataLoader(train_subsets[0.25], batch_size=1, shuffle=False)\n",
        "first_batch = next(iter(train_loader_0_25))\n",
        "print(\"\\nFirst batch from DataLoader (transformed image tensor, labels):\")\n",
        "print(first_batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SrmwmO_fyZA",
        "outputId": "72c2690e-5c8a-42f3-d4d7-57f397a9527a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First sample (path, gender, race):\n",
            "('/content/demogpairs/DemogPairs/elizabeth_mitchell/043.jpg', 0, 'white')\n",
            "\n",
            "First 3 samples:\n",
            "('/content/demogpairs/DemogPairs/elizabeth_mitchell/043.jpg', 0, 'white')\n",
            "('/content/demogpairs/DemogPairs/jamie_hector/0246_03.jpg', 1, 'black')\n",
            "('/content/demogpairs/DemogPairs/cameron_crowe/010.jpg', 1, 'white')\n",
            "\n",
            "First batch from DataLoader (transformed image tensor, labels):\n",
            "[('/content/demogpairs/DemogPairs/elizabeth_mitchell/043.jpg',), tensor([0]), ('white',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def in_class(predict, label, classes):\n",
        "    probs = torch.zeros(classes)\n",
        "    for i in range(classes):\n",
        "        # in_class_id = torch.tensor(label == i, dtype= torch.float)\n",
        "        in_class_id = (label == i).clone().detach().float()\n",
        "        # correct_predict = torch.tensor(predict == label, dtype= torch.float)\n",
        "        correct_predict = (predict == label).clone().detach().float()\n",
        "        in_class_correct_predict = (correct_predict) * (in_class_id)\n",
        "        acc = torch.sum(in_class_correct_predict).item() / torch.sum(in_class_id).item()\n",
        "        probs[i] = acc\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "84njo_qjRAPK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title pipeline for wandb\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "id": "5WRIPY5ESbcx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "V4HgisJbFAtR"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "goy8JCnuN5y8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Various utility functions (not in utils yet)"
      ],
      "metadata": {
        "id": "a2MxhhfvS2CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=8/2550,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ozdMyUbKyike"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Selecting the first column of y (assuming it's the identity label)\n",
        "                loss = nn.CrossEntropyLoss()(output, y[:, 0])\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "P0OJ1YNLy6DT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initializing a wandb run\n",
        "\n",
        "# api key: bd1c08839d0c8c49e7c3efe9aabe2d9c644befb6\n",
        "\n",
        "wandb.init(project=\"face-adv-fairness\", name=\"demogpairs-demo\", config={\"learning_rate\": 0.001, \"epochs\": 20})\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "HIWp5na8TIuD",
        "outputId": "eca09647-ccbb-4b9c-8e37-70584a8ce5a4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-demo</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/22mfim6h' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/22mfim6h</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_042040-22mfim6h/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_042100-iv7l7zfm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/iv7l7zfm' target=\"_blank\">demogpairs-demo</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/iv7l7zfm' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/iv7l7zfm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-demo</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/iv7l7zfm' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/iv7l7zfm</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_042100-iv7l7zfm/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title utils: pgd-attack\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                loss = nn.CrossEntropyLoss()(output, y)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "iI3HBGKtzrWA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title utils: eval_test, eval_robust\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "def eval_test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n",
        "\n",
        "def mixup_data(x, y, mixup_alpha=1.0):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=0.003,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Lp-OlBD7pJmB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        labels = targets[:, 0] # the first column is the identity label\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer)\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "id": "fKvyFN-QiJQD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title resnet module\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n"
      ],
      "metadata": {
        "id": "4nLHgl8ekD7z"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title modified train and test functions for celeba\n",
        "\n",
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = targets[:, 0] # Assuming the first column is the identity label\n",
        "\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            # Pass the original multi-dimensional targets to the attack\n",
        "            adv_x = pgd_attack(inputs, targets) # The attack will extract labels internally\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=labels, optimizer=optimizer)\n",
        "\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels.\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, labels)\n",
        "        #     adv_x = pgd_attack(inputs, targets) # Pass original targets to attack\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels from adv_x?\n",
        "        #     # This part of mixup with adversarial training might need careful consideration of how targets are handled.\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, labels) # Using extracted labels\n",
        "\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     # Use the extracted 1D labels for criterion\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "            wandb.log({f\"train_loss {train_loader.dataset}\": loss.item()}, step=epoch)\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Using Adam as in your failing block, but only for model\n",
        "\n",
        "    best_acc = 0.0 # Keep track of best average accuracy across genders\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        # Pass the extracted labels in train_ep as modified above\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "        val_acc_f = 0.0\n",
        "        val_acc_m = 0.0\n",
        "        val_loss_f = 0.0\n",
        "        val_loss_m = 0.0\n",
        "\n",
        "        if val_loader_f and len(val_loader_f.dataset) > 0:\n",
        "            val_loss_f, val_acc_f = eval_test_celeba(model, val_loader_f, device, name = 'female')\n",
        "            robust_loss_f, robust_accuracy_f = eval_robust_celeba(model, val_loader_f, pgd, device, name='female', epoch = epoch)\n",
        "\n",
        "\n",
        "        if val_loader_m and len(val_loader_m.dataset) > 0:\n",
        "            val_loss_m, val_acc_m = eval_test_celeba(model, val_loader_m, device, name = 'male')\n",
        "            robust_loss_m, robust_accuracy_m = eval_robust_celeba(model, val_loader_m, pgd, device, name = 'male', epoch = epoch)\n",
        "\n",
        "\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Average accuracy: {val_acc:.2f}, female: {val_acc_f:.2f}, male: {val_acc_m:.2f}')\n",
        "\n",
        "        wandb.log({\"val_loss_female\": val_loss_f, \"val_accuracy_female\": val_acc_f,\n",
        "               \"val_loss_male\": val_loss_m, \"val_accuracy_male\": val_acc_m,\n",
        "               \"average_val_accuracy\": val_acc}, step=epoch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval_test_celeba(model, dataloader, device, name):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # Extract identity label\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, labels).item() * inputs.size(0)\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "    test_loss /= total if total > 0 else 1\n",
        "    accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    # print(f'Test: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)')\n",
        "    # wandb.log(f\"clean_test_loss {name}: {test_loss}\", step=epoch)\n",
        "    # wandb.log(f\"clean_test_accuracy {name}: {accuracy}\", step=epoch)\n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "# convenience funtion to log predictions for a batch of test images\n",
        "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):\n",
        "  # obtain confidence scores for all classes\n",
        "  scores = F.softmax(outputs.data, dim=1)\n",
        "  log_scores = scores.cpu().numpy()\n",
        "  log_images = images.cpu().numpy()\n",
        "  log_labels = labels.cpu().numpy()\n",
        "  log_preds = predicted.cpu().numpy()\n",
        "  # adding ids based on the order of the images\n",
        "  _id = 0\n",
        "  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):\n",
        "    # add required info to data table:\n",
        "    # id, image pixels, model's guess, true label, scores for all classes\n",
        "    img_id = str(_id) + \"_\" + str(log_counter)\n",
        "    test_table.add_data(img_id, wandb.Image(i), p, l, *s)\n",
        "    _id += 1\n",
        "    if _id == batch_size:\n",
        "      break\n",
        "\n",
        "\n",
        "NUM_BATCHES_TO_LOG = 10\n",
        "\n",
        "def eval_robust_celeba(model, dataloader, pgd_attack, device, name, epoch):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    success_count = 0\n",
        "    log_counter = 0\n",
        "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
        "    for image_id in dataloader.dataset.indices:\n",
        "      columns.append(\"score_\" + str(image_id))\n",
        "    test_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # extract identity label\n",
        "\n",
        "            outputs_clean = model(inputs)\n",
        "            pred_clean = outputs_clean.max(1, keepdim=True)[1]\n",
        "\n",
        "\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs_adv = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs_adv, labels).item()\n",
        "            pred_adv = outputs_adv.max(1, keepdim=True)[1]\n",
        "            correct += pred_adv.eq(labels.view_as(pred_adv)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "\n",
        "            if log_counter < NUM_BATCHES_TO_LOG:\n",
        "              log_test_predictions(inputs, labels, outputs_adv, pred_adv, test_table, log_counter)\n",
        "              log_counter += 1\n",
        "\n",
        "            # keeping track of successful attacks\n",
        "            mask = pred_clean == labels\n",
        "            succesful_attacks = (pred_adv != labels) & mask\n",
        "            success_count += succesful_attacks.sum().item()\n",
        "\n",
        "\n",
        "\n",
        "    attack_success_rate = success_count / correct if correct > 0 else 0\n",
        "    print(f'Attack success rate: {attack_success_rate:.2f}%')\n",
        "    robust_loss /= len(dataloader.dataset) if total > 0 else 1\n",
        "    robust_accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    print(f'LinfPGD Attack: Average loss: {robust_loss:.4f}, Robust Accuracy: {robust_accuracy:.0f}%)')\n",
        "\n",
        "    wandb.log({f\"robust_loss_{name}\": robust_loss}, step=epoch)\n",
        "    wandb.log({f\"robust_accuracy_{name}\": robust_accuracy}, step=epoch)\n",
        "    wandb.log({f\"attack_success_rate_{name}\": attack_success_rate}, step=epoch)\n",
        "\n",
        "\n",
        "    #  W&B: Log predictions table to wandb\n",
        "    wandb.log({\"test_predictions\" : test_table})\n",
        "\n",
        "    return robust_loss, robust_accuracy\n"
      ],
      "metadata": {
        "id": "aZbU70I7gShk"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title small sanity check\n",
        "\n",
        "wandb.init(project=\"face-adv-fairness\", name=\"celeba-sanity-check\", config={\"learning_rate\": 0.001, \"epochs\": 1})\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = ResNet18(num_classes=1000).to(device) # ResNet for identity classification\n",
        "val_loader_f = DataLoader(test_subsets_f[0.25], batch_size=64, shuffle=False) # Shuffle usually False for validation\n",
        "val_loader_m = DataLoader(test_subsets_m[0.25], batch_size=64, shuffle=False) # Shuffle usually False for validation\n",
        "pgd = LinfPGDAttack(model, epsilon=8/255, step_size = 2/255, steps = 10)\n",
        "\n",
        "robust_loss, robust_accuracy = eval_robust_celeba(model, val_loader_f, pgd, device, name = 'female', epoch = 0)"
      ],
      "metadata": {
        "id": "pqwPuQ6xvxYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training run: old\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "batch_size = 64\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=1000).to(device) # ResNet for identity classification\n",
        "    # Note: number of classes (1000) should match the number of unique identities\n",
        "    # we filtered initially by top 1000 identitites but this might be limiting perhaps?\n",
        "    # it gives very few examples on the test set\n",
        "    # make a new run for each example\n",
        "    wandb.init(project=\"face-adv-fairness\", name=f\"celeba-gender-{proportion}\", config={\"learning_rate\": 0.001, \"epochs\": 30})\n",
        "\n",
        "\n",
        "    num_identity_classes = 1000 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    if proportion in test_subsets_f and len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "    if proportion in test_subsets_m and len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "\n",
        "\n",
        "    # call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader_f=val_loader_f, val_loader_m=val_loader_m,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=20) # Save checkpoints with proportion\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-THEtjHpv_T_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convenience funtion to log predictions for a batch of test images\n",
        "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):\n",
        "  # obtain confidence scores for all classes\n",
        "  scores = F.softmax(outputs.data, dim=1)\n",
        "  log_scores = scores.cpu().numpy()\n",
        "  log_images = images.cpu().numpy()\n",
        "  log_labels = labels.cpu().numpy()\n",
        "  log_preds = predicted.cpu().numpy()\n",
        "  # adding ids based on the order of the images\n",
        "  _id = 0\n",
        "  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):\n",
        "    # Transpose image dimensions from (C, H, W) to (H, W, C) for wandb.Image\n",
        "    i_transposed = np.transpose(i, (1, 2, 0))\n",
        "\n",
        "    # add required info to data table:\n",
        "    # id, image pixels, model's guess, true label, scores for all classes\n",
        "    img_id = str(_id) + \"_\" + str(log_counter)\n",
        "    # Use the transposed image data\n",
        "    test_table.add_data(img_id, wandb.Image(i_transposed), p, l, *s)\n",
        "    _id += 1\n",
        "    if _id == batch_size:\n",
        "      break"
      ],
      "metadata": {
        "id": "QclN7fi5tVWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title modified train and test functions for celeba\n",
        "\n",
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = targets[:, 0] # Assuming the first column is the identity label\n",
        "\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            # Pass the original multi-dimensional targets to the attack\n",
        "            # The attack will extract the identity label internally using targets[:, 0]\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            # For trades_loss, you need to pass the identity labels as it's used directly in the loss calculation.\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=labels, optimizer=optimizer)\n",
        "\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels.\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, labels)\n",
        "        #     # Pass original targets to attack\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels from adv_x?\n",
        "        #     # This part of mixup with adversarial training might need careful consideration of how targets are handled.\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, labels) # Using extracted labels\n",
        "\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     # Use the extracted 1D labels for criterion\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "            # It seems like train_loader.dataset is a Subset, not a string name.\n",
        "            # Using the proportion from the loop might be better.\n",
        "            # You might need to pass the proportion to train_ep or handle logging outside this loop.\n",
        "            # For now, removing the dataset name from log key to avoid issues.\n",
        "            wandb.log({\"train_loss\": loss.item()}, step=epoch)\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Using Adam as in your failing block, but only for model\n",
        "\n",
        "    best_acc = 0.0 # Keep track of best average accuracy across genders\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        # Pass the extracted labels in train_ep as modified above\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "        val_acc_f = 0.0\n",
        "        val_acc_m = 0.0\n",
        "        val_loss_f = 0.0\n",
        "        val_loss_m = 0.0\n",
        "\n",
        "        # Get the number of output classes from the model's linear layer\n",
        "        num_classes = model.linear.out_features\n",
        "\n",
        "        if val_loader_f and len(val_loader_f.dataset) > 0:\n",
        "            val_loss_f, val_acc_f = eval_test_celeba(model, val_loader_f, device, name = 'female', epoch = epoch)\n",
        "            # Pass num_classes to eval_robust_celeba\n",
        "            robust_loss_f, robust_accuracy_f = eval_robust_celeba(model, val_loader_f, pgd_attack, device, name='female', epoch = epoch, num_classes=num_classes)\n",
        "\n",
        "\n",
        "        if val_loader_m and len(val_loader_m.dataset) > 0:\n",
        "            val_loss_m, val_acc_m = eval_test_celeba(model, val_loader_m, device, name = 'male', epoch = epoch)\n",
        "            # Pass num_classes to eval_robust_celeba\n",
        "            robust_loss_m, robust_accuracy_m = eval_robust_celeba(model, val_loader_m, pgd_attack, device, name = 'male', epoch = epoch, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Average accuracy: {val_acc:.2f}, female: {val_acc_f:.2f}, male: {val_acc_m:.2f}')\n",
        "\n",
        "        wandb.log({\"val_loss_female\": val_loss_f, \"val_accuracy_female\": val_acc_f,\n",
        "               \"val_loss_male\": val_loss_m, \"val_accuracy_male\": val_acc_m,\n",
        "               \"average_val_accuracy\": val_acc}, step=epoch)\n",
        "\n",
        "\n",
        "def eval_test_celeba(model, dataloader, device, name, epoch): # Added epoch parameter for logging\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # Extract identity label\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, labels).item() * inputs.size(0)\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "    test_loss /= total if total > 0 else 1\n",
        "    accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    print(f'Test {name}: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)')\n",
        "    # Log clean test loss and accuracy\n",
        "    wandb.log({f\"clean_test_loss_{name}\": test_loss}, step=epoch)\n",
        "    wandb.log({f\"clean_test_accuracy_{name}\": accuracy}, step=epoch)\n",
        "    return test_loss, accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "XwjcH741u6uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convenience funtion to log predictions for a batch of test images\n",
        "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter, batch_size_for_log):\n",
        "  # obtain confidence scores for all classes\n",
        "  scores = F.softmax(outputs.data, dim=1)\n",
        "  # only log the score with the highest probability as the guess\n",
        "  # log_scores = scores.cpu().numpy()\n",
        "  log_images = images.cpu().numpy()\n",
        "  log_labels = labels.cpu().numpy()\n",
        "  log_preds = predicted.cpu().numpy()\n",
        "  # adding ids based on the order of the images\n",
        "  _id = 0\n",
        "  for i, l, p in zip(log_images, log_labels, log_preds):\n",
        "    # Transpose image dimensions from (C, H, W) to (H, W, C) for wandb.Image\n",
        "    if p != l:\n",
        "      # Transpose image dimensions from (C, H, W) to (H, W, C) for wandb.Image\n",
        "      i_transposed = np.transpose(i, (1, 2, 0))\n",
        "\n",
        "      # add required info to data table:\n",
        "      # id, image pixels, model's incorrect guess, true label\n",
        "      img_id = str(_id) + \"_\" + str(log_counter)\n",
        "      # Use the transposed image data\n",
        "      test_table.add_data(img_id, wandb.Image(i_transposed), p, l)\n",
        "\n",
        "\n",
        "    _id += 1\n",
        "    # Use the provided batch_size_for_log for comparison\n",
        "    if _id == batch_size_for_log:\n",
        "      break\n",
        "\n",
        "\n",
        "NUM_BATCHES_TO_LOG = 10\n",
        "\n",
        "# Added num_classes parameter\n",
        "def eval_robust_celeba(model, dataloader, pgd_attack, device, name, epoch, num_classes):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    success_count = 0\n",
        "    log_counter = 0\n",
        "    # Initialize columns based on the number of classes\n",
        "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
        "    # took out the other classes as cols bc it was tedious\n",
        "    test_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # extract identity label\n",
        "\n",
        "            outputs_clean = model(inputs)\n",
        "            pred_clean = outputs_clean.max(1, keepdim=True)[1]\n",
        "\n",
        "            # Pass the original targets to the attack\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs_adv = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs_adv, labels).item() * inputs.size(0)\n",
        "            pred_adv = outputs_adv.max(1, keepdim=True)[1]\n",
        "            correct += pred_adv.eq(labels.view_as(pred_adv)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "\n",
        "            if log_counter < NUM_BATCHES_TO_LOG:\n",
        "                # Pass the actual batch size of the current inputs to log_test_predictions\n",
        "                log_test_predictions(inputs, labels, outputs_adv, pred_adv, test_table, log_counter, inputs.size(0))\n",
        "                log_counter += 1\n",
        "\n",
        "\n",
        "            # keeping track of successful attacks\n",
        "            # Ensure mask uses the correct comparison (pred_clean vs labels)\n",
        "            mask = pred_clean.view_as(labels) == labels\n",
        "            succesful_attacks = (pred_adv.view_as(labels) != labels) & mask\n",
        "            success_count += succesful_attacks.sum().item()\n",
        "\n",
        "    robust_loss /= total if total > 0 else 1\n",
        "    robust_accuracy = 100. * correct / total if total > 0 else 0\n",
        "    attack_success_rate = success_count / correct if correct > 0 else 0 # Calculate attack success rate based on correct predictions\n",
        "\n",
        "\n",
        "    print(f'LinfPGD Attack {name}: Average loss: {robust_loss:.4f}, Robust Accuracy: {robust_accuracy:.0f}%)')\n",
        "    print(f'Attack success rate {name}: {attack_success_rate:.2f}%')\n",
        "\n",
        "    wandb.log({f\"robust_loss_{name}\": robust_loss}, step=epoch)\n",
        "    wandb.log({f\"robust_accuracy_{name}\": robust_accuracy}, step=epoch)\n",
        "    wandb.log({f\"attack_success_rate_{name}\": attack_success_rate}, step=epoch)\n",
        "\n",
        "\n",
        "    #  W&B: Log predictions table to wandb\n",
        "    wandb.log({\"test_predictions\" : test_table}, step=epoch) # Log table at each epoch step\n",
        "\n",
        "    return robust_loss, robust_accuracy"
      ],
      "metadata": {
        "id": "Dcv4h_tGytYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training run: new, with balanced datasets\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "batch_size = 64\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=999).to(device) # ResNet for identity classification\n",
        "    # Note: number of classes (1000) should match the number of unique identities\n",
        "    # it gives very few examples on the test set\n",
        "\n",
        "\n",
        "    # make a new run for each example\n",
        "    wandb.init(project=\"face-adv-fairness\", name=f\"celeba-gender-w-{proportion}\", config={\"learning_rate\": 0.001, \"epochs\": 30})\n",
        "\n",
        "\n",
        "    num_identity_classes = 999 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    train_loader = DataLoader(train_subsets_new[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    if proportion in test_subsets_f and len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "    if proportion in test_subsets_m and len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "\n",
        "\n",
        "    # call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader_f=val_loader_f, val_loader_m=val_loader_m,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=20) # Save checkpoints with proportion\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6175MiPJr5en",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0af2b5e2-3588-486a-e912-94ee861a9ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_female</td><td></td></tr><tr><td>attack_success_rate_male</td><td></td></tr><tr><td>average_val_accuracy</td><td></td></tr><tr><td>clean_test_accuracy_female</td><td></td></tr><tr><td>clean_test_accuracy_male</td><td></td></tr><tr><td>clean_test_loss_female</td><td></td></tr><tr><td>clean_test_loss_male</td><td></td></tr><tr><td>robust_accuracy_female</td><td></td></tr><tr><td>robust_accuracy_male</td><td></td></tr><tr><td>robust_loss_female</td><td></td></tr><tr><td>robust_loss_male</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy_female</td><td></td></tr><tr><td>val_accuracy_male</td><td></td></tr><tr><td>val_loss_female</td><td></td></tr><tr><td>val_loss_male</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_female</td><td>0.16429</td></tr><tr><td>attack_success_rate_male</td><td>0.18495</td></tr><tr><td>average_val_accuracy</td><td>85.86093</td></tr><tr><td>clean_test_accuracy_female</td><td>90.19868</td></tr><tr><td>clean_test_accuracy_male</td><td>85.82781</td></tr><tr><td>clean_test_loss_female</td><td>0.23455</td></tr><tr><td>clean_test_loss_male</td><td>0.3507</td></tr><tr><td>robust_accuracy_female</td><td>74.17219</td></tr><tr><td>robust_accuracy_male</td><td>73.04636</td></tr><tr><td>robust_loss_female</td><td>0.49018</td></tr><tr><td>robust_loss_male</td><td>0.49035</td></tr><tr><td>train_loss</td><td>0.41478</td></tr><tr><td>val_accuracy_female</td><td>85.89404</td></tr><tr><td>val_accuracy_male</td><td>85.82781</td></tr><tr><td>val_loss_female</td><td>0.34227</td></tr><tr><td>val_loss_male</td><td>0.3507</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">celeba-gender-w-0.25</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/me8r6y2i' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/me8r6y2i</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 16 media file(s), 596 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_001353-me8r6y2i/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_002525-uvakn8wd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/uvakn8wd' target=\"_blank\">celeba-gender-w-0.25</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/uvakn8wd' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/uvakn8wd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 7.094592\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.469209\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.467024\n",
            "Train Epoch: 0 [09664/39936 (97%)]\t Loss: 0.402396\n",
            "Test female: Average loss: 0.3671, Accuracy: 1364/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3810, Robust Accuracy: 90%)\n",
            "Attack success rate female: 0.00%\n",
            "Test male: Average loss: 0.3903, Accuracy: 1352/1510 (90%)\n",
            "LinfPGD Attack male: Average loss: 0.4038, Robust Accuracy: 90%)\n",
            "Attack success rate male: 0.00%\n",
            "Average accuracy: 89.93, female: 90.33, male: 89.54\n",
            "Train Epoch: 1 [00064/39936 (1%)]\t Loss: 0.654551\n",
            "Train Epoch: 1 [03264/39936 (33%)]\t Loss: 0.225233\n",
            "Train Epoch: 1 [06464/39936 (65%)]\t Loss: 0.207366\n",
            "Train Epoch: 1 [09664/39936 (97%)]\t Loss: 0.321055\n",
            "Test female: Average loss: 0.3067, Accuracy: 1364/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3323, Robust Accuracy: 90%)\n",
            "Attack success rate female: 0.00%\n",
            "Test male: Average loss: 0.3216, Accuracy: 1352/1510 (90%)\n",
            "LinfPGD Attack male: Average loss: 0.3471, Robust Accuracy: 90%)\n",
            "Attack success rate male: 0.00%\n",
            "Average accuracy: 89.93, female: 90.33, male: 89.54\n",
            "Train Epoch: 2 [00064/39936 (1%)]\t Loss: 0.230942\n",
            "Train Epoch: 2 [03264/39936 (33%)]\t Loss: 0.363713\n",
            "Train Epoch: 2 [06464/39936 (65%)]\t Loss: 0.395510\n",
            "Train Epoch: 2 [09664/39936 (97%)]\t Loss: 0.309033\n",
            "Test female: Average loss: 0.2885, Accuracy: 1364/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3157, Robust Accuracy: 90%)\n",
            "Attack success rate female: 0.00%\n",
            "Test male: Average loss: 0.3124, Accuracy: 1352/1510 (90%)\n",
            "LinfPGD Attack male: Average loss: 0.3378, Robust Accuracy: 90%)\n",
            "Attack success rate male: 0.00%\n",
            "Average accuracy: 89.93, female: 90.33, male: 89.54\n",
            "Train Epoch: 3 [00064/39936 (1%)]\t Loss: 0.200161\n",
            "Train Epoch: 3 [03264/39936 (33%)]\t Loss: 0.464738\n",
            "Train Epoch: 3 [06464/39936 (65%)]\t Loss: 0.301613\n",
            "Train Epoch: 3 [09664/39936 (97%)]\t Loss: 0.319840\n",
            "Test female: Average loss: 0.3836, Accuracy: 1339/1510 (89%)\n",
            "LinfPGD Attack female: Average loss: 0.4091, Robust Accuracy: 86%)\n",
            "Attack success rate female: 0.04%\n",
            "Test male: Average loss: 0.3863, Accuracy: 1333/1510 (88%)\n",
            "LinfPGD Attack male: Average loss: 0.4115, Robust Accuracy: 86%)\n",
            "Attack success rate male: 0.04%\n",
            "Average accuracy: 88.48, female: 88.68, male: 88.28\n",
            "Train Epoch: 4 [00064/39936 (1%)]\t Loss: 0.331450\n",
            "Train Epoch: 4 [03264/39936 (33%)]\t Loss: 0.265078\n",
            "Train Epoch: 4 [06464/39936 (65%)]\t Loss: 0.263735\n",
            "Train Epoch: 4 [09664/39936 (97%)]\t Loss: 0.173778\n",
            "Test female: Average loss: 0.2859, Accuracy: 1364/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3052, Robust Accuracy: 90%)\n",
            "Attack success rate female: 0.00%\n",
            "Test male: Average loss: 0.3085, Accuracy: 1352/1510 (90%)\n",
            "LinfPGD Attack male: Average loss: 0.3210, Robust Accuracy: 90%)\n",
            "Attack success rate male: 0.00%\n",
            "Average accuracy: 89.93, female: 90.33, male: 89.54\n",
            "Train Epoch: 5 [00064/39936 (1%)]\t Loss: 0.342483\n",
            "Train Epoch: 5 [03264/39936 (33%)]\t Loss: 0.258210\n",
            "Train Epoch: 5 [06464/39936 (65%)]\t Loss: 0.412725\n",
            "Train Epoch: 5 [09664/39936 (97%)]\t Loss: 0.386955\n",
            "Test female: Average loss: 0.3002, Accuracy: 1353/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3551, Robust Accuracy: 86%)\n",
            "Attack success rate female: 0.04%\n",
            "Test male: Average loss: 0.3027, Accuracy: 1348/1510 (89%)\n",
            "LinfPGD Attack male: Average loss: 0.3569, Robust Accuracy: 86%)\n",
            "Attack success rate male: 0.04%\n",
            "Average accuracy: 89.44, female: 89.60, male: 89.27\n",
            "Train Epoch: 6 [00064/39936 (1%)]\t Loss: 0.279607\n",
            "Train Epoch: 6 [03264/39936 (33%)]\t Loss: 0.247167\n",
            "Train Epoch: 6 [06464/39936 (65%)]\t Loss: 0.287540\n",
            "Train Epoch: 6 [09664/39936 (97%)]\t Loss: 0.281733\n",
            "Test female: Average loss: 0.2688, Accuracy: 1364/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3136, Robust Accuracy: 90%)\n",
            "Attack success rate female: 0.00%\n",
            "Test male: Average loss: 0.2877, Accuracy: 1352/1510 (90%)\n",
            "LinfPGD Attack male: Average loss: 0.3334, Robust Accuracy: 90%)\n",
            "Attack success rate male: 0.00%\n",
            "Average accuracy: 89.93, female: 90.33, male: 89.54\n",
            "Train Epoch: 7 [00064/39936 (1%)]\t Loss: 0.249342\n",
            "Train Epoch: 7 [03264/39936 (33%)]\t Loss: 0.321116\n",
            "Train Epoch: 7 [06464/39936 (65%)]\t Loss: 0.328664\n",
            "Train Epoch: 7 [09664/39936 (97%)]\t Loss: 0.266120\n",
            "Test female: Average loss: 0.2545, Accuracy: 1364/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3212, Robust Accuracy: 90%)\n",
            "Attack success rate female: 0.00%\n",
            "Test male: Average loss: 0.2564, Accuracy: 1352/1510 (90%)\n",
            "LinfPGD Attack male: Average loss: 0.3205, Robust Accuracy: 89%)\n",
            "Attack success rate male: 0.00%\n",
            "Average accuracy: 89.93, female: 90.33, male: 89.54\n",
            "Train Epoch: 8 [00064/39936 (1%)]\t Loss: 0.191932\n",
            "Train Epoch: 8 [03264/39936 (33%)]\t Loss: 0.208310\n",
            "Train Epoch: 8 [06464/39936 (65%)]\t Loss: 0.234850\n",
            "Train Epoch: 8 [09664/39936 (97%)]\t Loss: 0.266328\n",
            "Test female: Average loss: 0.2357, Accuracy: 1364/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3025, Robust Accuracy: 89%)\n",
            "Attack success rate female: 0.01%\n",
            "Test male: Average loss: 0.2365, Accuracy: 1357/1510 (90%)\n",
            "LinfPGD Attack male: Average loss: 0.3022, Robust Accuracy: 89%)\n",
            "Attack success rate male: 0.01%\n",
            "Average accuracy: 90.10, female: 90.33, male: 89.87\n",
            "Train Epoch: 9 [00064/39936 (1%)]\t Loss: 0.237418\n",
            "Train Epoch: 9 [03264/39936 (33%)]\t Loss: 0.253463\n",
            "Train Epoch: 9 [06464/39936 (65%)]\t Loss: 0.282465\n",
            "Train Epoch: 9 [09664/39936 (97%)]\t Loss: 0.217426\n",
            "Test female: Average loss: 0.2610, Accuracy: 1344/1510 (89%)\n",
            "LinfPGD Attack female: Average loss: 0.3604, Robust Accuracy: 84%)\n",
            "Attack success rate female: 0.06%\n",
            "Test male: Average loss: 0.2679, Accuracy: 1333/1510 (88%)\n",
            "LinfPGD Attack male: Average loss: 0.3616, Robust Accuracy: 82%)\n",
            "Attack success rate male: 0.08%\n",
            "Average accuracy: 88.64, female: 89.01, male: 88.28\n",
            "Train Epoch: 10 [00064/39936 (1%)]\t Loss: 0.294068\n",
            "Train Epoch: 10 [03264/39936 (33%)]\t Loss: 0.219043\n",
            "Train Epoch: 10 [06464/39936 (65%)]\t Loss: 0.145601\n",
            "Train Epoch: 10 [09664/39936 (97%)]\t Loss: 0.279209\n",
            "Test female: Average loss: 0.2606, Accuracy: 1364/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3215, Robust Accuracy: 90%)\n",
            "Attack success rate female: 0.00%\n",
            "Test male: Average loss: 0.2602, Accuracy: 1352/1510 (90%)\n",
            "LinfPGD Attack male: Average loss: 0.3183, Robust Accuracy: 90%)\n",
            "Attack success rate male: 0.00%\n",
            "Average accuracy: 89.93, female: 90.33, male: 89.54\n",
            "Train Epoch: 11 [00064/39936 (1%)]\t Loss: 0.368249\n",
            "Train Epoch: 11 [03264/39936 (33%)]\t Loss: 0.209702\n",
            "Train Epoch: 11 [06464/39936 (65%)]\t Loss: 0.231616\n",
            "Train Epoch: 11 [09664/39936 (97%)]\t Loss: 0.120968\n",
            "Test female: Average loss: 0.2691, Accuracy: 1348/1510 (89%)\n",
            "LinfPGD Attack female: Average loss: 0.3511, Robust Accuracy: 85%)\n",
            "Attack success rate female: 0.05%\n",
            "Test male: Average loss: 0.2682, Accuracy: 1333/1510 (88%)\n",
            "LinfPGD Attack male: Average loss: 0.3458, Robust Accuracy: 84%)\n",
            "Attack success rate male: 0.05%\n",
            "Average accuracy: 88.77, female: 89.27, male: 88.28\n",
            "Train Epoch: 12 [00064/39936 (1%)]\t Loss: 0.258328\n",
            "Train Epoch: 12 [03264/39936 (33%)]\t Loss: 0.301702\n",
            "Train Epoch: 12 [06464/39936 (65%)]\t Loss: 0.281179\n",
            "Train Epoch: 12 [09664/39936 (97%)]\t Loss: 0.212756\n",
            "Test female: Average loss: 0.2317, Accuracy: 1364/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3060, Robust Accuracy: 88%)\n",
            "Attack success rate female: 0.02%\n",
            "Test male: Average loss: 0.2285, Accuracy: 1362/1510 (90%)\n",
            "LinfPGD Attack male: Average loss: 0.2948, Robust Accuracy: 88%)\n",
            "Attack success rate male: 0.02%\n",
            "Average accuracy: 90.26, female: 90.33, male: 90.20\n",
            "Train Epoch: 13 [00064/39936 (1%)]\t Loss: 0.254724\n",
            "Train Epoch: 13 [03264/39936 (33%)]\t Loss: 0.195404\n",
            "Train Epoch: 13 [06464/39936 (65%)]\t Loss: 0.200650\n",
            "Train Epoch: 13 [09664/39936 (97%)]\t Loss: 0.190477\n",
            "Test female: Average loss: 0.2526, Accuracy: 1354/1510 (90%)\n",
            "LinfPGD Attack female: Average loss: 0.3820, Robust Accuracy: 84%)\n",
            "Attack success rate female: 0.07%\n",
            "Test male: Average loss: 0.2512, Accuracy: 1345/1510 (89%)\n",
            "LinfPGD Attack male: Average loss: 0.3784, Robust Accuracy: 83%)\n",
            "Attack success rate male: 0.07%\n",
            "Average accuracy: 89.37, female: 89.67, male: 89.07\n",
            "Train Epoch: 14 [00064/39936 (1%)]\t Loss: 0.290207\n",
            "Train Epoch: 14 [03264/39936 (33%)]\t Loss: 0.125442\n",
            "Train Epoch: 14 [06464/39936 (65%)]\t Loss: 0.156991\n",
            "Train Epoch: 14 [09664/39936 (97%)]\t Loss: 0.242897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxypbLzaZh01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}