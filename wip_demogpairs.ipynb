{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taweener11/darkSideUnmasked/blob/main/wip_demogpairs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYi8w2zpnKKE",
        "outputId": "0d13170e-2f5d-4595-d5d4-d1a547a6e68d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cores = os.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrb_gzkT-CCv",
        "outputId": "e07d2b26-3d9f-4503-b3dd-a77b2765ca3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8TM7bpCk3kNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title shell pipeline for unzipping! this needs to run every time\n",
        "\n",
        "!unzip -q \"/content/drive/My Drive/Datasets/demogpairs/DemogPairs.zip\" -d \"/content/demogpairs/\""
      ],
      "metadata": {
        "id": "SS8uNZYunxqx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_metadata_file(filepath, gender_label, race_label):\n",
        "    \"\"\"\n",
        "    Read a DemogPairs metadata txt file and collect image paths with labels.\n",
        "\n",
        "    Args:\n",
        "      filepath (str): path to the metadata txt file\n",
        "      gender_label (int): 0 for female, 1 for male\n",
        "      race_label (str): string label for race, e.g. 'black', 'white', 'asian'\n",
        "\n",
        "    Returns:\n",
        "      List of tuples: (image_relative_path, gender_label, race_label)\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line or line.lower().startswith('db_code'):\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            # parts[1] is the image path relative to DemogPairs folder\n",
        "            img_path = parts[1]\n",
        "            samples.append((img_path, gender_label, race_label))\n",
        "    return samples\n"
      ],
      "metadata": {
        "id": "4uA1b3RswNS5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# metadata_dir = '/content/drive/My Drive/demogpairs/Metadata'  # adjust path\n",
        "metadata_dir = '/content/demogpairs/Metadata'  # edited this to run with the local environment\n",
        "\n",
        "\n",
        "\n",
        "# Map filenames to gender and race labels\n",
        "metadata_info = {\n",
        "    'Black_Females.txt': (0, 'black'),\n",
        "    'Black_Males.txt': (1, 'black'),\n",
        "    'White_Females.txt': (0, 'white'),\n",
        "    'White_Males.txt': (1, 'white'),\n",
        "    'Asian_Females.txt': (0, 'asian'),\n",
        "    'Asian_Males.txt': (1, 'asian')\n",
        "}\n",
        "\n",
        "all_samples = []\n",
        "\n",
        "for fname, (gender, race) in metadata_info.items():\n",
        "    full_path = os.path.join(metadata_dir, fname)\n",
        "    print(f\"Reading {full_path} ...\")\n",
        "    samples = read_metadata_file(full_path, gender, race)\n",
        "    all_samples.extend(samples)\n",
        "\n",
        "print(f\"Total samples loaded: {len(all_samples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwTPzuC57bCe",
        "outputId": "23f5928d-0c83-4813-8650-5ec4289b9e96"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/demogpairs/Metadata/Black_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/Black_Males.txt ...\n",
            "Reading /content/demogpairs/Metadata/White_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/White_Males.txt ...\n",
            "Reading /content/demogpairs/Metadata/Asian_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/Asian_Males.txt ...\n",
            "Total samples loaded: 10800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "cn679gCpx6pD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a transform that is smaller per suggestion of rasmus\n",
        "\n",
        "image_size = 64\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                          std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "oLYdzNAHyveC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title preliminary analysis on the dataset -- determine the number of examples for subsetting\n",
        "demogpairs_root = '/content/demogpairs/DemogPairs'\n",
        "\n",
        "# Build final dataset list with full paths\n",
        "dataset = []\n",
        "for rel_path, gender, race in all_samples:\n",
        "    img_full_path = os.path.join(demogpairs_root, rel_path)\n",
        "    if os.path.isfile(img_full_path):\n",
        "        dataset.append((img_full_path, gender, race))\n",
        "    else:\n",
        "        print(f\"Missing file: {img_full_path}\")\n",
        "\n",
        "print(f\"Final dataset size after filtering missing files: {len(dataset)}\")\n",
        "\n",
        "# Extract identity labels from the image paths\n",
        "# Assuming identity is the directory name right after demogpairs_root\n",
        "identity_labels_list = []\n",
        "for img_path, gender, race in dataset:\n",
        "    # Split the path and get the second to last element (which should be the identity folder)\n",
        "    parts = img_path.split(os.sep)\n",
        "    # Find the index of demogpairs_root in the parts\n",
        "    try:\n",
        "        root_index = parts.index('DemogPairs')\n",
        "        # The identity folder is expected to be the element after 'DemogPairs'\n",
        "        if root_index + 1 < len(parts):\n",
        "            identity = parts[root_index + 1]\n",
        "            identity_labels_list.append(identity)\n",
        "        else:\n",
        "            # Handle cases where the path doesn't follow the expected structure\n",
        "            print(f\"Warning: Could not extract identity from path: {img_path}\")\n",
        "            identity_labels_list.append(\"unknown_identity\") # Or handle as appropriate\n",
        "    except ValueError:\n",
        "        # Handle cases where 'DemogPairs' is not in the path\n",
        "        print(f\"Warning: 'DemogPairs' not found in path: {img_path}\")\n",
        "        identity_labels_list.append(\"unknown_identity\") # Or handle as appropriate\n",
        "\n",
        "\n",
        "# Convert to a pandas Series for easier counting\n",
        "import pandas as pd\n",
        "# Use the created list of identity labels\n",
        "identity_series = pd.Series(identity_labels_list)\n",
        "\n",
        "identity_counts = identity_series.value_counts()\n",
        "# Select top 1000 identities. Ensure there are at least 1000 unique identities.\n",
        "if len(identity_counts) >= 1000:\n",
        "    top_1000_identities = identity_counts.nlargest(1000)\n",
        "else:\n",
        "    print(f\"Warning: Less than 1000 unique identities found. Using all {len(identity_counts)} identities.\")\n",
        "    top_1000_identities = identity_counts\n",
        "\n",
        "# Get the indices corresponding to the images belonging to the top 1000 identities\n",
        "# We need the original indices from the `dataset` list\n",
        "top_1000_identity_names = top_1000_identities.index.tolist()\n",
        "top_1000_indices = [i for i, (img_path, gender, race) in enumerate(dataset)\n",
        "                    if img_path.split(os.sep)[-2] in top_1000_identity_names]\n",
        "\n",
        "\n",
        "# Create a subset of the dataset containing only the top 1000 identities\n",
        "from torch.utils.data import Subset\n",
        "# You can create a Subset using the original list and the selected indices\n",
        "# Note: Subset is typically used with PyTorch Datasets, not plain Python lists.\n",
        "# If you intend to use this with PyTorch DataLoader later, you might need to\n",
        "# convert 'dataset' into a custom PyTorch Dataset first.\n",
        "# For now, let's just have the list of tuples for the top 1000 identities:\n",
        "dataset_top_1000 = [dataset[i] for i in top_1000_indices]\n",
        "\n",
        "\n",
        "min_samples = top_1000_identities.min()\n",
        "max_samples = top_1000_identities.max()\n",
        "\n",
        "print(f\"Minimum samples per identity: {min_samples}\")\n",
        "print(f\"Maximum samples per identity: {max_samples}\")\n",
        "print(f\"Number of samples in dataset_top_1000: {len(dataset_top_1000)}\")\n",
        "\n",
        "# printing the number of classes per group\n",
        "for key, value in metadata_info.items():\n",
        "    gender, race = value\n",
        "    gender_str = 'male' if gender ==1 else 'female'\n",
        "    count = len([s for s in dataset if s[1] == gender and s[2] == race])\n",
        "    print(f'Count of {race} {gender_str} =' + str(count))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUwGUcDF7fpg",
        "outputId": "3c5ec804-11c0-4315-fcc5-cce2f752c391"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataset size after filtering missing files: 10800\n",
            "Warning: Less than 1000 unique identities found. Using all 600 identities.\n",
            "Minimum samples per identity: 18\n",
            "Maximum samples per identity: 18\n",
            "Number of samples in dataset_top_1000: 10800\n",
            "Count of black female =1800\n",
            "Count of black male =1800\n",
            "Count of white female =1800\n",
            "Count of white male =1800\n",
            "Count of asian female =1800\n",
            "Count of asian male =1800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title create a custom PyTorch Dataset for DemogPairs\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class DemogPairsDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for DemogPairs.\"\"\"\n",
        "\n",
        "    def __init__(self, samples, transform=None, race_to_int_mapping=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            samples (list): List of tuples (img_full_path, identity_label, gender_label, race_label).\n",
        "                           race_label is a string ('black', 'white', 'asian').\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "            race_to_int_mapping (dict): Mapping from race string to integer index.\n",
        "        \"\"\"\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "        self.race_to_int_mapping = race_to_int_mapping or {'black': 0, 'white': 1, 'asian': 2} # Default mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, identity_label, gender_label, race_label_str = self.samples[idx]\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transform\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Convert race string label to integer using the mapping\n",
        "        race_label_int = self.race_to_int_mapping.get(race_label_str, -1) # Use -1 for unknown race\n",
        "\n",
        "        # Return image tensor and a target tensor containing identity, gender, and race\n",
        "        # Ensure all target components are numerical (identity, gender are already int, race is converted)\n",
        "        target = torch.tensor([identity_label, gender_label, race_label_int], dtype=torch.long)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "demogpairs_root = '/content/demogpairs/DemogPairs'\n",
        "\n",
        "# Build final dataset list with full paths and extract identities\n",
        "dataset_with_paths = [] # Store list of (img_full_path, gender, race)\n",
        "identity_labels_list = [] # Store list of identity folder names\n",
        "\n",
        "for rel_path, gender, race in all_samples:\n",
        "    img_full_path = os.path.join(demogpairs_root, rel_path)\n",
        "    if os.path.isfile(img_full_path):\n",
        "        dataset_with_paths.append((img_full_path, gender, race))\n",
        "        # Extract identity label (folder name)\n",
        "        parts = img_full_path.split(os.sep)\n",
        "        try:\n",
        "            root_index = parts.index('DemogPairs')\n",
        "            if root_index + 1 < len(parts):\n",
        "                identity = parts[root_index + 1]\n",
        "                identity_labels_list.append(identity)\n",
        "            else:\n",
        "                print(f\"Warning: Could not extract identity from path: {img_full_path}\")\n",
        "                identity_labels_list.append(\"unknown_identity\")\n",
        "        except ValueError:\n",
        "            print(f\"Warning: 'DemogPairs' not found in path: {img_full_path}\")\n",
        "            identity_labels_list.append(\"unknown_identity\")\n",
        "\n",
        "\n",
        "print(f\"Final dataset size after filtering missing files: {len(dataset_with_paths)}\")\n",
        "\n",
        "\n",
        "# Create a mapping from identity folder name to integer ID\n",
        "unique_identities = sorted(list(set(identity_labels_list))) # Sort for consistent mapping\n",
        "identity_name_to_int = {name: i for i, name in enumerate(unique_identities)}\n",
        "num_identity_classes = len(unique_identities)\n",
        "print(f\"Number of unique identity classes: {num_identity_classes}\")\n",
        "\n",
        "# Create a race string to integer mapping for use in the Dataset and in_class\n",
        "race_to_int = {'black': 0, 'white': 1, 'asian': 2}\n",
        "int_to_race = {v: k for k, v in race_to_int.items()} # For converting back in in_class\n",
        "\n",
        "\n",
        "# Rebuild the 'dataset' list to include image path, identity label (int), gender (int), and race (string for now)\n",
        "# The Dataset's __getitem__ will convert the race string to an int.\n",
        "dataset = []\n",
        "for img_full_path, gender, race_str in dataset_with_paths:\n",
        "    identity_name = img_full_path.split(os.sep)[-2] # Re-extract identity name\n",
        "    identity_int = identity_name_to_int.get(identity_name, -1) # Get integer ID\n",
        "    if identity_int != -1: # Only include samples with a valid identity mapping\n",
        "         dataset.append((img_full_path, identity_int, gender, race_str))\n",
        "    else:\n",
        "         print(f\"Skipping sample with unknown identity: {img_full_path}\")\n",
        "\n",
        "\n",
        "print(f\"Dataset size with identity mapping: {len(dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqCBrmOoDiOT",
        "outputId": "7d0bca30-57e1-4cde-e318-818a4d35a5ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataset size after filtering missing files: 10800\n",
            "Number of unique identity classes: 600\n",
            "Dataset size with identity mapping: 10800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title subset maker for specified distribution\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Subset\n",
        "import pandas as pd # Import pandas for value_counts\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "races = ['black', 'white', 'asian']\n",
        "\n",
        "# Update the function signature to match the dataset structure (img_full_path, identity_label, gender_label, race_label_str)\n",
        "def make_train_subsets_from_list(dataset_list, proportions, subgroup = (0, 'asian')):\n",
        "    \"\"\"\n",
        "    Create subsets of the dataset list based on specified proportions\n",
        "    for a given gender/race subgroup, while maintaining balanced identities.\n",
        "\n",
        "    Args:\n",
        "      dataset_list (list): A list of tuples (img_full_path, identity_int, gender_int, race_str)\n",
        "      proportions (list): A list of proportions (float) for the subgroup in the output subsets.\n",
        "      subgroup (tuple): (gender_int, race_str) for the subgroup to vary.\n",
        "\n",
        "    Returns:\n",
        "      Dict of torch.utils.data.Subset: Subsets of the original list, keyed by proportion.\n",
        "                                       These Subsets wrap the original dataset_list.\n",
        "    \"\"\"\n",
        "    train_subsets = {}\n",
        "\n",
        "    # Extract identity labels, gender, and race directly from the input list\n",
        "    # The input list items are (img_full_path, identity_int, gender_int, race_str)\n",
        "    dataset_identity_labels_int = [item[1] for item in dataset_list] # identity is at index 1 (int)\n",
        "    dataset_gender_labels_int = [item[2] for item in dataset_list] # gender is at index 2 (int)\n",
        "    dataset_race_labels_str = [item[3] for item in dataset_list] # race is at index 3 (str)\n",
        "\n",
        "    # Calculate base_number based on the minimum samples per identity in this list\n",
        "    # Use the integer identity labels for value counts\n",
        "    identity_counts = pd.Series(dataset_identity_labels_int).value_counts()\n",
        "    base_number = identity_counts.min() if not identity_counts.empty else 0\n",
        "\n",
        "    # Unique identity integer IDs in this list\n",
        "    unique_identity_ints = np.unique(dataset_identity_labels_int)\n",
        "\n",
        "\n",
        "    for prop in proportions:\n",
        "        selected_original_indices_for_prop = [] # Collect original indices for the current proportion\n",
        "\n",
        "        # Iterate through the unique integer identity labels\n",
        "        for identity_int in unique_identity_ints:\n",
        "\n",
        "            # Indices within the *current dataset_list* that correspond to this identity_int\n",
        "            # Need to find indices where the identity label (index 1) matches identity_int\n",
        "            indices_for_identity = [i for i, item in enumerate(dataset_list) if item[1] == identity_int]\n",
        "\n",
        "\n",
        "            # Separate indices by gender and race *within this identity*\n",
        "            # Main subgroup indices for this identity\n",
        "            main_sg_indices_for_identity = [\n",
        "                idx for idx in indices_for_identity\n",
        "                if dataset_list[idx][2] == subgroup[0] and dataset_list[idx][3] == subgroup[1] # Check gender (index 2) and race (index 3)\n",
        "            ]\n",
        "\n",
        "            rng.shuffle(main_sg_indices_for_identity)\n",
        "\n",
        "            # Determine number of samples for the main subgroup\n",
        "            if len(main_sg_indices_for_identity) < base_number:\n",
        "                 # if an identity has fewer samples than base_number, use its total count as the maximum\n",
        "                 n_main_sg = int(np.floor(len(main_sg_indices_for_identity) * prop))\n",
        "            else:\n",
        "                 # otherwise, use base_number\n",
        "                n_main_sg = int(np.floor(base_number * prop))\n",
        "\n",
        "\n",
        "            # Collect the original indices for the selected main subgroup samples\n",
        "            selected_original_indices_for_prop.extend(main_sg_indices_for_identity[:n_main_sg])\n",
        "\n",
        "            # selecting for the non-main subgroups *within this identity*\n",
        "            # Iterate through all possible gender (0, 1) and race (strings) combinations\n",
        "            for gender in range(2):\n",
        "                for race_str in races:\n",
        "                    # Skip the main subgroup\n",
        "                    if not (gender == subgroup[0] and race_str == subgroup[1]):\n",
        "                        subgroup_indices_for_identity = [\n",
        "                            idx for idx in indices_for_identity\n",
        "                            if dataset_list[idx][2] == gender and dataset_list[idx][3] == race_str # Check gender (index 2) and race (index 3)\n",
        "                        ]\n",
        "                        rng.shuffle(subgroup_indices_for_identity)\n",
        "\n",
        "                        # Calculate how many samples from this subgroup to select\n",
        "                        # Need to count how many *other* subgroups are present for *this identity*\n",
        "                        available_other_subgroups_count = 0\n",
        "                        for g_other in range(2):\n",
        "                            for r_other in races:\n",
        "                                # Check if this gender+race combination is present for the current identity\n",
        "                                if any(dataset_list[idx][2] == g_other and dataset_list[idx][3] == r_other for idx in indices_for_identity):\n",
        "                                     if not (g_other == subgroup[0] and r_other == subgroup[1]): # Exclude the main subgroup itself\n",
        "                                        available_other_subgroups_count += 1\n",
        "\n",
        "\n",
        "                        if available_other_subgroups_count > 0:\n",
        "                             # Distribute the remaining (1-prop) samples equally among available other subgroups for this identity\n",
        "                             target_per_other_subgroup = int(np.floor((base_number * (1-prop)) / available_other_subgroups_count))\n",
        "                        else:\n",
        "                             target_per_other_subgroup = 0\n",
        "\n",
        "\n",
        "                        # Number of samples to select for this specific non-main subgroup, up to available count\n",
        "                        n_subgroup = min(len(subgroup_indices_for_identity), target_per_other_subgroup)\n",
        "\n",
        "\n",
        "                        # Collect the original indices for the selected non-main subgroup samples\n",
        "                        selected_original_indices_for_prop.extend(subgroup_indices_for_identity[:n_subgroup])\n",
        "\n",
        "        # Shuffle the collected original indices for the current proportion\n",
        "        rng.shuffle(selected_original_indices_for_prop)\n",
        "\n",
        "        # Create the Subset using the original list and the selected original indices\n",
        "        train_subsets[prop] = Subset(dataset_list, selected_original_indices_for_prop)\n",
        "\n",
        "    return train_subsets\n",
        "\n",
        "# %%\n",
        "#@title sanity check\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Before creating the subsets, you need a list to create subsets *from*.\n",
        "# The previous cell created 'dataset' which is a list of (img_full_path, identity_int, gender_int, race_str)\n",
        "# You also defined train_indices and test_indices.\n",
        "# Let's create the training list to pass to make_train_subsets_from_list\n",
        "\n",
        "train_dataset_list = [dataset[i] for i in train_indices] # List of (img_full_path, identity_int, gender_int, race_str) for the training split\n",
        "\n",
        "# Now call the function with this list\n",
        "train_subsets = make_train_subsets_from_list(train_dataset_list, [0.25, 0.5, 0.75], subgroup = (0, 'asian')) # subgroup is (gender_int, race_str)\n",
        "\n",
        "# get the first sample (index 0) from the Subset\n",
        "# The structure of the items in the subset is still (img_full_path, identity_int, gender_int, race_str)\n",
        "first_sample = train_subsets[0.25][0]\n",
        "\n",
        "print(\"First sample (path, identity_int, gender_int, race_str):\")\n",
        "print(first_sample)\n",
        "\n",
        "# If you want to see more samples, you can loop\n",
        "print(\"\\nFirst 3 samples:\")\n",
        "for i in range(min(3, len(train_subsets[0.25]))):\n",
        "    print(train_subsets[0.25][i])\n",
        "\n",
        "# To use this with a DataLoader and apply transformations, you need to wrap the Subset\n",
        "# in your custom DemogPairsDataset. The DemogPairsDataset expects a list of samples.\n",
        "# A Subset behaves like a list, providing access to elements of the original list.\n",
        "# So, you can pass a Subset directly to the DemogPairsDataset constructor.\n",
        "\n",
        "# Re-using the transform defined earlier (ipython-input-6)\n",
        "# Re-using race_to_int mapping defined in ipython-input-11\n",
        "# num_identity_classes defined in ipython-input-11\n",
        "\n",
        "batch_size = 64\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "train_demogpair_datasets = {}\n",
        "train_demogpair_loaders = {}\n",
        "for prop in proportions:\n",
        "    train_demogpair_datasets[prop] = DemogPairsDataset(samples=train_subsets[prop], transform=transform, race_to_int_mapping=race_to_int)\n",
        "    train_demogpair_loaders[prop] = DataLoader(train_demogpair_datasets[prop], batch_size, shuffle=False)\n",
        "\n",
        "# Do the same for the test set.\n",
        "test_dataset_list = [dataset[i] for i in test_indices] # List of (img_full_path, identity_int, gender_int, race_str) for the test split\n",
        "\n",
        "# Create a DemogPairsDataset for the test list\n",
        "test_dataset_full = DemogPairsDataset(samples=test_dataset_list, transform=transform, race_to_int_mapping=race_to_int)\n",
        "\n",
        "# Create a DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset_full, batch_size, shuffle=False)\n",
        "\n",
        "first_batch_test = next(iter(test_loader))\n",
        "print(\"\\nFirst batch from Test DataLoader (transformed image tensor, target tensor):\")\n",
        "# The target tensor from DemogPairsDataset should be [identity_label, gender_label, race_label_int]\n",
        "print(first_batch_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIw5nzJwCXU_",
        "outputId": "34077a1a-5a33-4a28-858d-b9522756d4d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First sample (path, identity_int, gender_int, race_str):\n",
            "('/content/demogpairs/DemogPairs/lenda_murray/0073_01.jpg', 311, 0, 'black')\n",
            "\n",
            "First 3 samples:\n",
            "('/content/demogpairs/DemogPairs/lenda_murray/0073_01.jpg', 311, 0, 'black')\n",
            "('/content/demogpairs/DemogPairs/shun_oguri/079.jpg', 496, 1, 'asian')\n",
            "('/content/demogpairs/DemogPairs/josh_cooke/019.jpg', 247, 1, 'white')\n",
            "\n",
            "First batch from Test DataLoader (transformed image tensor, target tensor):\n",
            "[tensor([[[[-0.2078, -0.1686,  0.0667,  ..., -0.1922, -0.1765, -0.1529],\n",
            "          [-0.1765, -0.1529,  0.2000,  ..., -0.2000, -0.1765, -0.1451],\n",
            "          [-0.1216,  0.0039,  0.2627,  ..., -0.2078, -0.1686, -0.1373],\n",
            "          ...,\n",
            "          [ 0.7882,  0.8588,  0.8118,  ..., -0.7098, -0.7412, -0.7804],\n",
            "          [ 0.9451,  0.9216,  0.8431,  ..., -0.7333, -0.7569, -0.8039],\n",
            "          [ 0.9686,  0.9216,  0.8667,  ..., -0.7412, -0.7569, -0.7961]],\n",
            "\n",
            "         [[-0.5451, -0.5843, -0.5373,  ..., -0.5529, -0.5373, -0.5216],\n",
            "          [-0.5451, -0.5686, -0.6078,  ..., -0.5608, -0.5373, -0.5137],\n",
            "          [-0.5608, -0.5765, -0.6941,  ..., -0.5686, -0.5294, -0.5137],\n",
            "          ...,\n",
            "          [-0.0039,  0.0353, -0.0275,  ..., -0.8588, -0.8980, -0.9294],\n",
            "          [ 0.1216,  0.0824, -0.0196,  ..., -0.8745, -0.9059, -0.9451],\n",
            "          [ 0.1686,  0.1059,  0.0196,  ..., -0.8824, -0.9059, -0.9294]],\n",
            "\n",
            "         [[-0.7098, -0.7569, -0.7725,  ..., -0.7333, -0.7176, -0.7098],\n",
            "          [-0.7255, -0.7725, -0.7725,  ..., -0.7412, -0.7176, -0.7098],\n",
            "          [-0.7490, -0.7647, -0.8039,  ..., -0.7490, -0.7098, -0.7020],\n",
            "          ...,\n",
            "          [-0.5608, -0.5765, -0.5922,  ..., -0.9137, -0.9529, -0.9765],\n",
            "          [-0.5294, -0.5686, -0.6157,  ..., -0.9216, -0.9608, -0.9843],\n",
            "          [-0.5059, -0.5608, -0.5922,  ..., -0.9294, -0.9608, -0.9765]]],\n",
            "\n",
            "\n",
            "        [[[-1.0000, -1.0000, -1.0000,  ..., -0.9843, -0.7412,  0.0118],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -0.9686, -0.6627, -0.0588],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -0.9059, -0.5529, -0.0588],\n",
            "          ...,\n",
            "          [ 0.0039,  0.0980,  0.7176,  ...,  0.7725,  0.9529,  0.8667],\n",
            "          [-0.1373,  0.4824,  0.7176,  ...,  0.7490,  0.7490,  0.7412],\n",
            "          [-0.2863,  0.5765,  0.8275,  ...,  0.9216,  0.6941,  0.5765]],\n",
            "\n",
            "         [[-0.9843, -0.9843, -0.9843,  ..., -0.9765, -0.7255,  0.0275],\n",
            "          [-0.9922, -0.9922, -0.9922,  ..., -0.9529, -0.6471, -0.0431],\n",
            "          [-0.9922, -0.9922, -0.9922,  ..., -0.8902, -0.5373, -0.0431],\n",
            "          ...,\n",
            "          [ 0.0196,  0.1137,  0.7333,  ...,  0.7725,  0.9529,  0.8667],\n",
            "          [-0.1216,  0.4980,  0.7333,  ...,  0.7490,  0.7490,  0.7412],\n",
            "          [-0.2706,  0.5922,  0.8431,  ...,  0.9216,  0.6941,  0.5765]],\n",
            "\n",
            "         [[-1.0000, -1.0000, -1.0000,  ..., -0.9843, -0.7490,  0.0039],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -0.9686, -0.6706, -0.0667],\n",
            "          [-1.0000, -1.0000, -1.0000,  ..., -0.9137, -0.5608, -0.0667],\n",
            "          ...,\n",
            "          [ 0.0118,  0.1059,  0.7255,  ...,  0.7725,  0.9529,  0.8667],\n",
            "          [-0.1294,  0.4902,  0.7255,  ...,  0.7490,  0.7490,  0.7412],\n",
            "          [-0.2784,  0.5843,  0.8353,  ...,  0.9216,  0.6941,  0.5765]]],\n",
            "\n",
            "\n",
            "        [[[-0.3333, -0.2235, -0.2314,  ...,  0.4275,  0.3961,  0.2863],\n",
            "          [-0.2706, -0.1843, -0.2000,  ...,  0.5922,  0.5843,  0.3961],\n",
            "          [-0.2157, -0.1373,  0.2863,  ...,  0.6471,  0.6627,  0.4196],\n",
            "          ...,\n",
            "          [ 0.6549,  0.7647,  0.8353,  ...,  0.9216,  0.7725, -0.0510],\n",
            "          [ 0.5765,  0.7020,  0.8196,  ...,  0.9373,  0.8588,  0.1137],\n",
            "          [ 0.3882,  0.6549,  0.7882,  ...,  0.9529,  0.8980,  0.3098]],\n",
            "\n",
            "         [[-0.6392, -0.5451, -0.5451,  ..., -0.1451, -0.1922, -0.3020],\n",
            "          [-0.6157, -0.5529, -0.5922,  ..., -0.0039, -0.0118, -0.2157],\n",
            "          [-0.5765, -0.5529, -0.1608,  ...,  0.0431,  0.0510, -0.2078],\n",
            "          ...,\n",
            "          [ 0.4196,  0.5137,  0.5451,  ...,  0.9137,  0.7647, -0.0588],\n",
            "          [ 0.3569,  0.4667,  0.5373,  ...,  0.9294,  0.8510,  0.1059],\n",
            "          [ 0.1843,  0.4196,  0.5216,  ...,  0.9451,  0.8902,  0.3020]],\n",
            "\n",
            "         [[-0.8980, -0.8275, -0.8196,  ..., -0.6784, -0.7176, -0.8353],\n",
            "          [-0.8980, -0.8588, -0.8353,  ..., -0.5843, -0.5922, -0.7961],\n",
            "          [-0.8824, -0.8431, -0.4667,  ..., -0.5686, -0.5608, -0.8118],\n",
            "          ...,\n",
            "          [ 0.1843,  0.2549,  0.2706,  ...,  0.8902,  0.7333, -0.0902],\n",
            "          [ 0.1216,  0.2078,  0.2627,  ...,  0.8980,  0.8196,  0.0745],\n",
            "          [-0.0510,  0.1608,  0.2392,  ...,  0.9216,  0.8588,  0.2706]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.4039,  0.4039,  0.4039,  ...,  0.8118,  0.7961,  0.8588],\n",
            "          [ 0.4353,  0.4118,  0.4039,  ...,  0.7961,  0.8039,  0.8588],\n",
            "          [ 0.4118,  0.4039,  0.3961,  ...,  0.8039,  0.8118,  0.8667],\n",
            "          ...,\n",
            "          [ 0.8275,  0.6471,  0.3176,  ..., -0.3725, -0.2549,  0.2627],\n",
            "          [ 0.4667,  0.2392,  0.0745,  ..., -0.1294, -0.2078, -0.1059],\n",
            "          [ 0.0745,  0.0118, -0.0510,  ..., -0.0353,  0.1137,  0.1529]],\n",
            "\n",
            "         [[ 0.1294,  0.1294,  0.1373,  ...,  0.7569,  0.7412,  0.8039],\n",
            "          [ 0.1765,  0.1529,  0.1373,  ...,  0.7412,  0.7490,  0.8039],\n",
            "          [ 0.1451,  0.1451,  0.1373,  ...,  0.7490,  0.7569,  0.8118],\n",
            "          ...,\n",
            "          [ 0.7804,  0.5608,  0.2078,  ..., -0.5608, -0.4667,  0.0353],\n",
            "          [ 0.3412,  0.0745, -0.1216,  ..., -0.3333, -0.4353, -0.3255],\n",
            "          [-0.1922, -0.2471, -0.2863,  ..., -0.2784, -0.1373, -0.0980]],\n",
            "\n",
            "         [[-0.1059, -0.1216, -0.1373,  ...,  0.6157,  0.6000,  0.6627],\n",
            "          [-0.0510, -0.0902, -0.1294,  ...,  0.6078,  0.6078,  0.6627],\n",
            "          [-0.0902, -0.0980, -0.1294,  ...,  0.6157,  0.6235,  0.6784],\n",
            "          ...,\n",
            "          [ 0.7412,  0.5137,  0.1294,  ..., -0.6941, -0.6157, -0.1373],\n",
            "          [ 0.2941, -0.0353, -0.2471,  ..., -0.5137, -0.6078, -0.4980],\n",
            "          [-0.3333, -0.3961, -0.4353,  ..., -0.4667, -0.3333, -0.2863]]],\n",
            "\n",
            "\n",
            "        [[[-0.0902, -0.0902, -0.0902,  ..., -0.9059, -0.9137, -0.9137],\n",
            "          [-0.0745, -0.0902, -0.0824,  ..., -0.9059, -0.8980, -0.9059],\n",
            "          [-0.1137, -0.0980, -0.1059,  ..., -0.8980, -0.9059, -0.9137],\n",
            "          ...,\n",
            "          [-0.0902, -0.1216, -0.1608,  ..., -0.0902, -0.1373, -0.1686],\n",
            "          [-0.1137, -0.1294, -0.1608,  ..., -0.1608, -0.0353, -0.0275],\n",
            "          [-0.1294, -0.1373, -0.1608,  ..., -0.2941, -0.0745,  0.0431]],\n",
            "\n",
            "         [[-0.5059, -0.5216, -0.4980,  ..., -0.9059, -0.9137, -0.9137],\n",
            "          [-0.5059, -0.5373, -0.4902,  ..., -0.9059, -0.8980, -0.9059],\n",
            "          [-0.5216, -0.5137, -0.4824,  ..., -0.8980, -0.9059, -0.9137],\n",
            "          ...,\n",
            "          [-0.1608, -0.2000, -0.2392,  ..., -0.2235, -0.2549, -0.2863],\n",
            "          [-0.1373, -0.1529, -0.1765,  ..., -0.2941, -0.1608, -0.1451],\n",
            "          [-0.0980, -0.0980, -0.1216,  ..., -0.4275, -0.2000, -0.0824]],\n",
            "\n",
            "         [[-0.7569, -0.7490, -0.7020,  ..., -0.9059, -0.9137, -0.9137],\n",
            "          [-0.7255, -0.7333, -0.6941,  ..., -0.9059, -0.8980, -0.9059],\n",
            "          [-0.7020, -0.6941, -0.6863,  ..., -0.8980, -0.9059, -0.9137],\n",
            "          ...,\n",
            "          [-0.2471, -0.2863, -0.3333,  ..., -0.3961, -0.4510, -0.4980],\n",
            "          [-0.1922, -0.2235, -0.2627,  ..., -0.4431, -0.3569, -0.3804],\n",
            "          [-0.1608, -0.1765, -0.2157,  ..., -0.5765, -0.3882, -0.3098]]],\n",
            "\n",
            "\n",
            "        [[[-0.6706, -0.6157, -0.6314,  ..., -0.4745, -0.4824, -0.4902],\n",
            "          [-0.6549, -0.6314, -0.5843,  ..., -0.4667, -0.4902, -0.4980],\n",
            "          [-0.6549, -0.6549, -0.6078,  ..., -0.4824, -0.4824, -0.4980],\n",
            "          ...,\n",
            "          [-0.3255, -0.4196, -0.4510,  ...,  0.0510,  0.0275,  0.0196],\n",
            "          [-0.2941, -0.4039, -0.4431,  ...,  0.1137,  0.0824,  0.0902],\n",
            "          [-0.2549, -0.3882, -0.4431,  ...,  0.1294,  0.1059,  0.1451]],\n",
            "\n",
            "         [[-0.6784, -0.6471, -0.6784,  ..., -0.4588, -0.4667, -0.4745],\n",
            "          [-0.6941, -0.6784, -0.6549,  ..., -0.4510, -0.4745, -0.4824],\n",
            "          [-0.6941, -0.6863, -0.6627,  ..., -0.4667, -0.4667, -0.4667],\n",
            "          ...,\n",
            "          [-0.4745, -0.5059, -0.5216,  ..., -0.1843, -0.2078, -0.2314],\n",
            "          [-0.4588, -0.5059, -0.5216,  ..., -0.1843, -0.2157, -0.2549],\n",
            "          [-0.4510, -0.4980, -0.5451,  ..., -0.1294, -0.2235, -0.2706]],\n",
            "\n",
            "         [[-0.6627, -0.6157, -0.6471,  ..., -0.4980, -0.5059, -0.5137],\n",
            "          [-0.6627, -0.6471, -0.6157,  ..., -0.4902, -0.5137, -0.5216],\n",
            "          [-0.6863, -0.6706, -0.6235,  ..., -0.5059, -0.5137, -0.5137],\n",
            "          ...,\n",
            "          [-0.5294, -0.5137, -0.5137,  ..., -0.2078, -0.2235, -0.2471],\n",
            "          [-0.5294, -0.5216, -0.5373,  ..., -0.2235, -0.2549, -0.2706],\n",
            "          [-0.5216, -0.5608, -0.5686,  ..., -0.0745, -0.2314, -0.2863]]]]), tensor([[441,   0,   0],\n",
            "        [497,   0,   2],\n",
            "        [267,   0,   0],\n",
            "        [135,   1,   1],\n",
            "        [389,   0,   0],\n",
            "        [288,   1,   2],\n",
            "        [336,   0,   2],\n",
            "        [315,   0,   2],\n",
            "        [256,   0,   0],\n",
            "        [ 57,   1,   2],\n",
            "        [341,   1,   0],\n",
            "        [104,   0,   2],\n",
            "        [515,   0,   1],\n",
            "        [396,   1,   2],\n",
            "        [201,   1,   0],\n",
            "        [367,   0,   2],\n",
            "        [450,   1,   1],\n",
            "        [564,   0,   1],\n",
            "        [135,   1,   1],\n",
            "        [167,   0,   0],\n",
            "        [423,   0,   2],\n",
            "        [187,   0,   2],\n",
            "        [113,   1,   0],\n",
            "        [ 92,   1,   1],\n",
            "        [301,   1,   1],\n",
            "        [489,   0,   0],\n",
            "        [372,   0,   2],\n",
            "        [234,   1,   1],\n",
            "        [233,   0,   1],\n",
            "        [564,   0,   1],\n",
            "        [232,   0,   2],\n",
            "        [254,   0,   1],\n",
            "        [ 61,   0,   1],\n",
            "        [251,   0,   1],\n",
            "        [454,   1,   0],\n",
            "        [336,   0,   2],\n",
            "        [485,   0,   1],\n",
            "        [ 78,   0,   2],\n",
            "        [220,   1,   1],\n",
            "        [ 85,   1,   1],\n",
            "        [464,   1,   2],\n",
            "        [323,   0,   2],\n",
            "        [354,   0,   1],\n",
            "        [440,   1,   0],\n",
            "        [371,   0,   2],\n",
            "        [ 99,   1,   1],\n",
            "        [141,   0,   0],\n",
            "        [579,   1,   2],\n",
            "        [585,   0,   2],\n",
            "        [191,   1,   1],\n",
            "        [519,   0,   1],\n",
            "        [320,   0,   2],\n",
            "        [430,   1,   2],\n",
            "        [ 64,   1,   0],\n",
            "        [129,   1,   0],\n",
            "        [579,   1,   2],\n",
            "        [284,   0,   0],\n",
            "        [256,   0,   0],\n",
            "        [414,   1,   0],\n",
            "        [ 54,   1,   0],\n",
            "        [448,   1,   1],\n",
            "        [495,   0,   2],\n",
            "        [ 85,   1,   1],\n",
            "        [590,   1,   2]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title subset maker from dataset\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Subset\n",
        "import pandas as pd # Import pandas for value_counts\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "races = ['black', 'white', 'asian']\n",
        "\n",
        "def make_train_subsets_from_dataset(dataset, proportions, subgroup = (0, 'asian')):\n",
        "    \"\"\"\n",
        "    Read a DemogPairs metadata txt file and collect image paths with labels.\n",
        "\n",
        "    Args:\n",
        "      dataset_list (list): A Demogpairs dataset object\n",
        "      proportions (list): A list of proportions for the subgroup.\n",
        "      subgroup (tuple): (gender, race) for the subgroup to vary.\n",
        "\n",
        "    Returns:\n",
        "      Dict of torch.utils.data.Subset: Subsets of the original list, keyed by proportion.\n",
        "    \"\"\"\n",
        "    train_subsets = {}\n",
        "\n",
        "    # Extract identity labels, gender, and race directly from the input list\n",
        "    dataset_identity_labels = [img_path.split(os.sep)[-2] for img_full_path, _, _ in dataset_list] # Assuming identity is the folder name\n",
        "    dataset_gender_labels = [gender for _, gender, _ in dataset]\n",
        "    dataset_race_labels = [race for _, _, race in dataset]\n",
        "\n",
        "    # Calculate base_number based on the minimum samples per identity in this list\n",
        "    identity_counts = pd.Series(dataset_identity_labels).value_counts()\n",
        "    base_number = identity_counts.min() if not identity_counts.empty else 0\n",
        "\n",
        "    # Map original identity names to a numerical label for easier processing\n",
        "    unique_identities = np.unique(dataset_identity_labels)\n",
        "    identity_mapping = {name: i for i, name in enumerate(unique_identities)}\n",
        "    numerical_identity_labels = np.array([identity_mapping[name] for name in dataset_identity_labels])\n",
        "\n",
        "    for prop in proportions:\n",
        "        selected_original_indices_for_prop = [] # Collect original indices for the current proportion\n",
        "\n",
        "        # the indices 'c' here refer to the numerical identity labels\n",
        "        for c_num in np.unique(numerical_identity_labels):\n",
        "            # Get the actual identity name\n",
        "            identity_name = unique_identities[c_num]\n",
        "\n",
        "            # Indices within the *current dataset_list* that correspond to identity 'c_num'\n",
        "            indices_for_identity = np.where(numerical_identity_labels == c_num)[0]\n",
        "\n",
        "            # Separate indices by gender and race *within this identity*\n",
        "            main_sg_indices_for_identity = [\n",
        "                idx for idx in indices_for_identity\n",
        "                if dataset_gender_labels[idx] == subgroup[0] and dataset_race_labels[idx] == subgroup[1]\n",
        "            ]\n",
        "\n",
        "            rng.shuffle(main_sg_indices_for_identity)\n",
        "\n",
        "            # Determine number of samples for the main subgroup\n",
        "            if len(main_sg_indices_for_identity) < base_number:\n",
        "                n_main_sg = int(np.floor(len(main_sg_indices_for_identity) * prop))\n",
        "            else:\n",
        "                n_main_sg = int(np.floor(base_number * prop))\n",
        "\n",
        "            # Collect the original indices for the selected main subgroup samples\n",
        "            selected_original_indices_for_prop.extend(main_sg_indices_for_identity[:n_main_sg])\n",
        "\n",
        "            # selecting for the non-main subgroups *within this identity*\n",
        "            for gender in range(2):\n",
        "                for race in races:\n",
        "                    if race != subgroup[1]:\n",
        "                        subgroup_indices_for_identity = [\n",
        "                            idx for idx in indices_for_identity\n",
        "                            if dataset_gender_labels[idx] == gender and dataset_race_labels[idx] == race\n",
        "                        ]\n",
        "                        rng.shuffle(subgroup_indices_for_identity)\n",
        "\n",
        "                        # Calculate how many samples from this subgroup to select\n",
        "                        available_other_subgroups_count = 0\n",
        "                        for g_other in range(2):\n",
        "                            for r_other in races:\n",
        "                                if r_other != subgroup[1]:\n",
        "                                     if any(dataset_gender_labels[idx] == g_other and dataset_race_labels[idx] == r_other for idx in indices_for_identity):\n",
        "                                         available_other_subgroups_count += 1\n",
        "\n",
        "                        if available_other_subgroups_count > 0:\n",
        "                            target_per_other_subgroup = int(np.floor((base_number * (1-prop)) / available_other_subgroups_count))\n",
        "                        else:\n",
        "                             target_per_other_subgroup = 0\n",
        "\n",
        "                        # Number of samples to select for this specific non-main subgroup\n",
        "                        n_subgroup = min(len(subgroup_indices_for_identity), target_per_other_subgroup)\n",
        "\n",
        "                        # Collect the original indices for the selected non-main subgroup samples\n",
        "                        selected_original_indices_for_prop.extend(subgroup_indices_for_identity[:n_subgroup])\n",
        "\n",
        "        # Shuffle the collected original indices for the current proportion\n",
        "        rng.shuffle(selected_original_indices_for_prop)\n",
        "\n",
        "        # Create the Subset using the original list and the selected original indices\n",
        "        train_subsets[prop] = Subset(dataset_list, selected_original_indices_for_prop)\n",
        "\n",
        "    return train_subsets\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SYuGTOkECXlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title pipeline for wandb\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "id": "5WRIPY5ESbcx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "goy8JCnuN5y8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Various utility functions (not in utils yet)"
      ],
      "metadata": {
        "id": "a2MxhhfvS2CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=8/2550,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ozdMyUbKyike"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Selecting the first column of y (assuming it's the identity label)\n",
        "                loss = nn.CrossEntropyLoss()(output, y[:, 0])\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "P0OJ1YNLy6DT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initializing a wandb run\n",
        "\n",
        "# api key: bd1c08839d0c8c49e7c3efe9aabe2d9c644befb6\n",
        "\n",
        "wandb.init(project=\"face-adv-fairness\", name=\"demogpairs-demo\", config={\"learning_rate\": 0.001, \"epochs\": 20})\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "HIWp5na8TIuD",
        "outputId": "94103ac0-0984-49c4-939c-2b5b8e565019"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33midilks\u001b[0m (\u001b[33midilks-dartmouth\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_164621-7de9ze6j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/7de9ze6j' target=\"_blank\">demogpairs-demo</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/7de9ze6j' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/7de9ze6j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-demo</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/7de9ze6j' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/7de9ze6j</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_164621-7de9ze6j/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title utils: pgd-attack\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                loss = nn.CrossEntropyLoss()(output, y)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "iI3HBGKtzrWA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title utils: eval_test, eval_robust\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "def eval_test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n",
        "\n",
        "def mixup_data(x, y, mixup_alpha=1.0):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=0.003,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Lp-OlBD7pJmB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        labels = targets[:, 0] # the first column is the identity label\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer)\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "id": "fKvyFN-QiJQD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title resnet module\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n"
      ],
      "metadata": {
        "id": "4nLHgl8ekD7z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title in-class statistics (without attack success)\n",
        "\n",
        "# recall that we have all the statistics in metadata_info.items()\n",
        "\n",
        "\n",
        "def in_class_clean(predict, label, classes, metadata_info, epoch):\n",
        "    \"\"\"\n",
        "    Calculate accuracy per gender+race group.\n",
        "\n",
        "    Args:\n",
        "      predict (torch.Tensor): Tensor of predicted class labels.\n",
        "      label (torch.Tensor): Tensor of true labels.\n",
        "                          Assumed to contain identity labels (0th column),\n",
        "                          gender (1st column), and race (2nd column).\n",
        "      classes (int): Total number of identity classes.\n",
        "      metadata_info (dict): Dictionary mapping metadata filenames to (gender, race) tuples.\n",
        "\n",
        "    Returns:\n",
        "      Dict: Accuracy per gender+race group, keyed by '{race}_{gender_str}'.\n",
        "    \"\"\"\n",
        "    group_stats = {}\n",
        "\n",
        "    # Iterate through each gender and race group defined in metadata_info\n",
        "    for _, (gender, race) in metadata_info.items():\n",
        "        gender_str = 'male' if gender == 1 else 'female'\n",
        "        group_key = f'{race}_{gender_str}'\n",
        "        group_stats[group_key] = {'correct': 0, 'total': 0}\n",
        "\n",
        "        # Find samples that belong to this specific gender and race group\n",
        "        # Assuming label tensor has shape (batch_size, 3) -> [identity, gender, race]\n",
        "        # Filter by gender (index 1) and race (index 2)\n",
        "        # Note: Assuming race labels in label tensor match the string keys in metadata_info\n",
        "        is_in_group = (label[:, 1] == gender) & (label[:, 2] == race)\n",
        "\n",
        "        # Select predictions and labels only for samples in this group\n",
        "        group_predict = predict[is_in_group]\n",
        "        group_label = label[is_in_group][:, 0] # Get identity label for this group\n",
        "\n",
        "        # Calculate correct predictions within this group\n",
        "        correct_predictions_in_group = (group_predict == group_label)\n",
        "        num_correct = torch.sum(correct_predictions_in_group).item()\n",
        "        num_total = group_label.size(0) # Number of samples in this group\n",
        "\n",
        "        # Store the counts\n",
        "        group_stats[group_key]['correct'] = num_correct\n",
        "        group_stats[group_key]['total'] = num_total\n",
        "\n",
        "    # Calculate accuracy for each group after iterating through all samples\n",
        "    group_accuracy = {}\n",
        "    for group_key, stats in group_stats.items():\n",
        "        accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0.0\n",
        "        group_accuracy[group_key] = accuracy\n",
        "        wandb.log({f\"accuracy_{group_key}\": accuracy}, step=epoch)\n",
        "\n",
        "    return group_accuracy"
      ],
      "metadata": {
        "id": "YTyT7ZCY1FY3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title in-class statistics\n",
        "\n",
        "# recall that we have all the statistics in metadata_info.items()\n",
        "\n",
        "\n",
        "def in_class(predict, label, classes, metadata_info, epoch, pred_clean):\n",
        "    \"\"\"\n",
        "    Calculate accuracy per gender+race group.\n",
        "\n",
        "    Args:\n",
        "      predict (torch.Tensor): Tensor of predicted class labels (identity).\n",
        "      label (torch.Tensor): Tensor of true labels.\n",
        "                          Assumed to contain identity labels (0th column),\n",
        "                          gender (1st column), and race (2nd column).\n",
        "      classes (int): Total number of identity classes (not directly used here, but good for context).\n",
        "      metadata_info (dict): Dictionary mapping metadata filenames to (gender, race) tuples.\n",
        "\n",
        "    Returns:\n",
        "      Dict: Accuracy per gender+race group, keyed by '{race}_{gender_str}'.\n",
        "            Logs group accuracies to wandb.\n",
        "    \"\"\"\n",
        "    group_stats = {}\n",
        "    group_attack_success = {}\n",
        "    group_accuracy = {}\n",
        "\n",
        "\n",
        "    # Calculate accuracy for each group after iterating through all samples (this should happen after the loop over batches in the eval function)\n",
        "    # The current structure calculates accuracy per batch and logs it, which might not be what you want.\n",
        "    # Let's adjust in_class to aggregate stats over all batches and then calculate/log accuracy once.\n",
        "\n",
        "    # --- Revised in_class to aggregate stats ---\n",
        "    # The evaluation functions (eval_test_celeba, eval_robust_celeba) should collect\n",
        "    # all predictions and all labels, then call in_class once.\n",
        "\n",
        "    # This function will now calculate accuracy given *all* predictions and labels from the epoch\n",
        "    # It should be called *outside* the batch loop in eval functions.\n",
        "\n",
        "    # Reinitialize group_stats for aggregation over the entire dataset passed to this function\n",
        "    group_stats = {}\n",
        "    unique_groups = set(metadata_info.values())\n",
        "    for gender, race_str in unique_groups:\n",
        "        gender_str = 'male' if gender == 1 else 'female'\n",
        "        group_key = f'{race_str}_{gender_str}'\n",
        "        group_stats[group_key] = {'correct': 0, 'total': 0}\n",
        "        group_attack_success[group_key] = 0\n",
        "\n",
        "        # Iterate through each sample in the aggregated label tensor\n",
        "        for i in range(label.size(0)):\n",
        "            sample_identity = label[i, 0].item()\n",
        "            sample_gender = label[i, 1].item()\n",
        "            sample_race = label[i, 2] # Assuming this is the string or index\n",
        "\n",
        "            # keeping track of successful attacks (clean prediction was correct, but adv prediction was wrong)\n",
        "            mask = pred_clean.view_as(sample_identity) == sample_identity # Reshape pred_clean to match labels\n",
        "            succesful_attacks = (predict.view_as(sample_identity) != sample_identity) & mask\n",
        "            success_count += succesful_attacks.sum().item()\n",
        "\n",
        "\n",
        "            # Check if this sample belongs to the current group\n",
        "            # Need to handle potential string comparison or index mapping for race\n",
        "            # Assuming label[:, 2] is a tensor of strings matching 'black', 'white', 'asian'\n",
        "            # If not, adjust the comparison here.\n",
        "            try:\n",
        "                 is_in_group_race = (sample_race == race_str)\n",
        "            except Exception: # Catch potential errors if sample_race is not a string\n",
        "                 # Fallback: if sample_race is an integer index, you need a mapping\n",
        "                 # Example: if race_map_int maps {0: 'black', 1: 'white', 2: 'asian'}\n",
        "                 # is_in_group_race = (race_map_int.get(sample_race.item()) == race_str)\n",
        "                 print(f\"Warning: Could not compare race label {sample_race} (type {type(sample_race)}) with group race string {race_str}. Assuming race is not directly comparable.\")\n",
        "                 is_in_group_race = False # Assume not in group if cannot compare\n",
        "\n",
        "\n",
        "            if sample_gender == gender and is_in_group_race:\n",
        "                 group_stats[group_key]['total'] += 1\n",
        "                 if predict[i].item() == sample_identity:\n",
        "                     group_stats[group_key]['correct'] += 1\n",
        "                     group_attack_success[group_key] += 1\n",
        "\n",
        "\n",
        "    # Calculate accuracy for each group after aggregating all samples\n",
        "    group_accuracy = {}\n",
        "    group_attack_success_rate = {}\n",
        "    for group_key, stats in group_stats.items():\n",
        "        accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0.0\n",
        "        success_rate = group_attack_success[group_key] / stats['total'] if stats['total'] > 0 else 0.0\n",
        "        group_accuracy[group_key] = accuracy\n",
        "        # Log group accuracies *per epoch*\n",
        "        wandb.log({f\"accuracy_{group_key}\": accuracy}, step=epoch)\n",
        "        wandb.log({f\"attack_success_rate_{group_key}\": success_rate}, step=epoch)\n",
        "\n",
        "    # calculate attack success for each group\n",
        "\n",
        "    return group_accuracy, group_attack_success_rate\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4dOb-8X9OKyN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title this works but unsure if it is accurate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import wandb\n",
        "import pandas as pd # Import pandas for value_counts\n",
        "\n",
        "# recall that we have all the statistics in metadata_info.items()\n",
        "# metadata_info is globally available as defined earlier.\n",
        "\n",
        "def in_class(predict_adv, label_full, metadata_info, epoch, predict_clean):\n",
        "    \"\"\"\n",
        "    Calculate accuracy and attack success rate per gender+race group.\n",
        "\n",
        "    Args:\n",
        "      predict_adv (torch.Tensor): Tensor of predicted class labels (identity) on adversarial examples.\n",
        "                                  Shape (N,) where N is the total number of samples.\n",
        "      label_full (torch.Tensor): Tensor of true labels.\n",
        "                                 Shape (N, 3) with columns [identity, gender, race_int].\n",
        "      metadata_info (dict): Dictionary mapping metadata filenames to (gender, race_str) tuples.\n",
        "                            Used to define groups.\n",
        "      epoch (int): The current training epoch for logging purposes.\n",
        "      predict_clean (torch.Tensor): Tensor of predicted class labels (identity) on clean examples.\n",
        "                                   Shape (N,) where N is the total number of samples.\n",
        "\n",
        "    Returns:\n",
        "      Tuple: Dict of robust accuracy per group, Dict of attack success rate per group.\n",
        "            Logs group metrics to wandb.\n",
        "    \"\"\"\n",
        "    group_stats = {}\n",
        "    group_attack_success = {}\n",
        "    group_accuracy = {}\n",
        "    group_attack_success_rate = {}\n",
        "\n",
        "\n",
        "    # create a mapping from integer race label (from label_full) back to race string\n",
        "    # this mapping was created earlier in the notebook: int_to_race\n",
        "    # int_to_race = {0: 'black', 1: 'white', 2: 'asian'}\n",
        "\n",
        "    unique_groups = set(metadata_info.values()) # e.g., {(0, 'black'), (1, 'black'), ...}\n",
        "\n",
        "    for gender, race_str in unique_groups:\n",
        "        gender_str = 'male' if gender == 1 else 'female'\n",
        "        group_key = f'{race_str}_{gender_str}'\n",
        "\n",
        "        # find samples that belong to this specific gender and race group using tensor operations\n",
        "        # label_full[:, 1] is gender (int), label_full[:, 2] is race (int)\n",
        "        # need to convert the race_str from metadata_info to an integer using race_to_int\n",
        "        # assuming race_to_int = {'black': 0, 'white': 1, 'asian': 2}\n",
        "        try:\n",
        "            race_int = race_to_int[race_str]\n",
        "        except KeyError:\n",
        "             print(f\"Warning: Race string '{race_str}' from metadata_info not found in race_to_int mapping. Skipping group {group_key}.\")\n",
        "             continue #skip this group if mapping is missing\n",
        "\n",
        "        is_in_group = (label_full[:, 1] == gender) & (label_full[:, 2] == race_int)\n",
        "\n",
        "        # select predictions and labels only for samples in this group\n",
        "        group_predict_adv = predict_adv[is_in_group]\n",
        "        group_predict_clean = predict_clean[is_in_group]\n",
        "        group_label_identity = label_full[is_in_group][:, 0] # Get identity label for this group\n",
        "\n",
        "        num_total = group_label_identity.size(0) # Number of samples in this group\n",
        "\n",
        "        if num_total > 0:\n",
        "            # Calculate correct predictions within this group for adversarial examples (Robust Accuracy)\n",
        "            correct_predictions_adv = (group_predict_adv == group_label_identity)\n",
        "            num_correct_adv = torch.sum(correct_predictions_adv).item()\n",
        "            group_accuracy[group_key] = num_correct_adv / num_total\n",
        "\n",
        "            # Calculate successful attacks within this group\n",
        "            # Successful attack: Clean prediction was correct, but adversarial prediction was wrong.\n",
        "            clean_was_correct = (group_predict_clean == group_label_identity)\n",
        "            adv_was_wrong = (group_predict_adv != group_label_identity)\n",
        "            successful_attacks_in_group = clean_was_correct & adv_was_wrong\n",
        "            num_successful_attacks = torch.sum(successful_attacks_in_group).item()\n",
        "            group_attack_success_rate[group_key] = num_successful_attacks / num_total\n",
        "\n",
        "            # Log group metrics to wandb\n",
        "            print(f\"Robust accuracy for {group_key}: {group_accuracy[group_key]}\")\n",
        "            print(f\"Attack success rate for {group_key}: {group_attack_success_rate[group_key]}\")\n",
        "            wandb.log({f\"robust_accuracy_{group_key}\": group_accuracy[group_key]}, step=epoch)\n",
        "            wandb.log({f\"attack_success_rate_{group_key}\": group_attack_success_rate[group_key]}, step=epoch)\n",
        "        else:\n",
        "            group_accuracy[group_key] = 0.0\n",
        "            group_attack_success_rate[group_key] = 0.0\n",
        "            # Log 0 if the group is empty to ensure the metric appears in wandb\n",
        "              # Log group metrics to wandb\n",
        "            print(f\"Robust accuracy for {group_key}: {group_accuracy[group_key]}\")\n",
        "            print(f\"Attack success rate for {group_key}: {group_attack_success_rate[group_key]}\")\n",
        "            wandb.log({f\"robust_accuracy_{group_key}\": 0.0}, step=epoch)\n",
        "            wandb.log({f\"attack_success_rate_{group_key}\": 0.0}, step=epoch)\n",
        "\n",
        "\n",
        "    return group_accuracy, group_attack_success_rate\n",
        "\n",
        "\n",
        "def eval_test_demogpairs(model, dataloader, device, name, epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = [] # To store full targets for group evaluation\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # Extract identity label\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, labels, reduction='sum').item() # Use reduction='sum' to sum loss over the batch\n",
        "            pred = outputs.max(1, keepdim=True)[1].squeeze() # Squeeze to get shape (batch_size,)\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "\n",
        "            all_preds.append(pred.cpu())\n",
        "            all_labels.append(targets.cpu()) # Append full targets for group evaluation\n",
        "\n",
        "    test_loss /= total if total > 0 else 1\n",
        "    accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    print(f'Test ({name}): Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)')\n",
        "\n",
        "    # Calculate group accuracies\n",
        "    # Concatenate all batches' predictions and labels\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    # Pass full labels (with gender and race) to in_class_clean\n",
        "    # Note: in_class_clean does not use predict_clean, so we don't pass it.\n",
        "    group_accuracy_clean = in_class_clean(predict=all_preds, label=all_labels, classes=model.linear.out_features, metadata_info=metadata_info, epoch=epoch)\n",
        "\n",
        "    wandb.log({f\"clean_test_loss {name}\": test_loss}, step=epoch)\n",
        "    wandb.log({f\"clean_test_accuracy {name}\": accuracy}, step=epoch)\n",
        "\n",
        "    # Log group accuracies to wandb within in_class_clean function\n",
        "    # Returning only overall loss and accuracy here as in_class_clean logs group stats\n",
        "    return test_loss, accuracy\n",
        "\n",
        "def eval_robust_demogpairs(model, dataloader, pgd_attack, device, name, epoch):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Need to collect both clean and adversarial predictions for the whole dataset\n",
        "    all_preds_clean = []\n",
        "    all_preds_adv = []\n",
        "    all_labels_full = [] # To store full targets for group evaluation\n",
        "\n",
        "    with torch.no_grad(): # Keep this outside the loop if pgd_attack generates adv examples without tracking gradients\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # extract identity label (int)\n",
        "\n",
        "            # Get clean predictions\n",
        "            outputs_clean = model(inputs)\n",
        "            pred_clean = outputs_clean.max(1, keepdim=True)[1].squeeze() # Squeeze to get shape (batch_size,)\n",
        "            all_preds_clean.append(pred_clean.cpu()) # Collect clean predictions\n",
        "\n",
        "\n",
        "            # Generate adversarial examples\n",
        "            # Need to enable gradients temporarily for the attack\n",
        "            with torch.enable_grad():\n",
        "                 adv = pgd_attack(inputs, labels) # Pass identity labels for attack loss\n",
        "\n",
        "            # Evaluate on adversarial examples\n",
        "            outputs_adv = model(adv)\n",
        "            # Use reduction='sum' to sum loss over the batch\n",
        "            robust_loss += F.cross_entropy(outputs_adv, labels, reduction='sum').item()\n",
        "            pred_adv = outputs_adv.max(1, keepdim=True)[1].squeeze() # Squeeze to get shape (batch_size,)\n",
        "            correct += pred_adv.eq(labels.view_as(pred_adv)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "\n",
        "            all_preds_adv.append(pred_adv.cpu())\n",
        "            all_labels_full.append(targets.cpu()) # Store full targets\n",
        "\n",
        "    robust_loss /= total if total > 0 else 1\n",
        "    robust_accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    print(f'Robust Test ({name}): Average loss: {robust_loss:.4f}, Robust Accuracy: {correct}/{total} ({robust_accuracy:.0f}%)')\n",
        "\n",
        "    # Calculate group robust accuracies and attack success rates\n",
        "    all_preds_clean = torch.cat(all_preds_clean)\n",
        "    all_preds_adv = torch.cat(all_preds_adv)\n",
        "    all_labels_full = torch.cat(all_labels_full)\n",
        "\n",
        "    # Pass all aggregated data to in_class\n",
        "    group_accuracy_robust, group_attack_success_rate = in_class(\n",
        "        predict_adv=all_preds_adv,\n",
        "        label_full=all_labels_full,\n",
        "        metadata_info=metadata_info,\n",
        "        epoch=epoch,\n",
        "        predict_clean=all_preds_clean # Pass clean predictions\n",
        "    )\n",
        "\n",
        "    wandb.log({f\"robust_loss_{name}\": robust_loss}, step=epoch)\n",
        "    wandb.log({f\"robust_accuracy_{name}\": robust_accuracy}, step=epoch)\n",
        "    # Overall attack success rate needs clean predictions too\n",
        "    # Overall attack success rate: Number of samples where clean pred was right, but adv pred was wrong / total samples where clean pred was right\n",
        "    total_clean_correct = torch.sum(all_preds_clean == all_labels_full[:, 0]).item()\n",
        "    total_successful_attacks = torch.sum((all_preds_clean == all_labels_full[:, 0]) & (all_preds_adv != all_labels_full[:, 0])).item()\n",
        "    overall_attack_success_rate = 100. * total_successful_attacks / total_clean_correct if total_clean_correct > 0 else 0.0\n",
        "\n",
        "    print(f'Overall Attack success rate ({name}): {overall_attack_success_rate:.2f}%')\n",
        "    wandb.log({f\"overall_attack_success_rate_{name}\": overall_attack_success_rate}, step=epoch)\n",
        "\n",
        "\n",
        "    # Log robust group accuracies and attack success rates within in_class function\n",
        "    # Returning only overall loss and accuracy here as in_class logs group stats\n",
        "    return robust_loss, robust_accuracy\n",
        "\n",
        "\n",
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = targets[:, 0] # Assuming the first column is the identity label\n",
        "\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            # Pass the original multi-dimensional targets to the attack\n",
        "            # The LinfPGDAttack perturbs based on identity label loss, so it only needs identity label for loss calculation.\n",
        "            # Let's update LinfPGDAttack to expect only identity labels as target for loss calculation\n",
        "            # Ensure pgd_attack expects identity labels\n",
        "            adv_x = pgd_attack(inputs, labels) # Pass only identity labels to the attack\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            # TRADES loss function already handles internally evaluation mode for adversarial example generation\n",
        "            # and expects only identity labels for the inner loss calculation (KL divergence).\n",
        "            # It takes the natural targets `y` for the natural loss and KL divergence target.\n",
        "            # Ensure trades_loss function expects identity labels for `y`\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=labels, optimizer=optimizer, step_size=pgd_attack.step_size, epsilon=pgd_attack.epsilon, perturb_steps=pgd_attack.steps)\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader.dataset), # Use len(train_loader.dataset) for total samples\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "            # Ensure train_loader.dataset can be converted to string for logging or use a predefined name\n",
        "            try:\n",
        "                # Try to get a meaningful name if the dataset is a Subset, otherwise use a default\n",
        "                if isinstance(train_loader.dataset, Subset):\n",
        "                    # Attempt to identify the underlying dataset or its source\n",
        "                    dataset_name = \"train_subset\" # Default for Subset\n",
        "                    # More specific name might require inspection of the Subset's dataset attribute\n",
        "                    # e.g., if the underlying dataset has a name or identifier\n",
        "                else:\n",
        "                   dataset_name = type(train_loader.dataset).__name__ # Use class name\n",
        "            except Exception:\n",
        "                dataset_name = \"train_dataset\" # Fallback name\n",
        "            wandb.log({f\"train_loss {dataset_name}\": loss.item()}, step=epoch)\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    # criterion should be CrossEntropyLoss as we are predicting identity\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Use Adam optimizer as specified in the original notebook block\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # Assuming device is globally available ('cuda' or 'cpu')\n",
        "\n",
        "\n",
        "    best_acc = 0.0 # Track best clean validation accuracy to save model\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        # training\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, train_loader.batch_size) # Pass actual batch_size from loader\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        # We need to evaluate both clean and robustly on the same validation set\n",
        "        # Assuming test_loader is used as the validation loader in the sanity check block\n",
        "        # In a real training loop, you would likely have a separate validation_loader\n",
        "\n",
        "        # Evaluate robustly first to get robust metrics\n",
        "        # Pass test_loader as the validation loader here for consistency with the sanity check\n",
        "        robust_loss, robust_accuracy = eval_robust_demogpairs(model, val_loader, pgd_attack, device, name='robust-validation', epoch=epoch)\n",
        "        # Evaluate cleanly second\n",
        "        val_loss, val_acc = eval_test_demogpairs(model, val_loader, device, name='clean-validation', epoch=epoch)\n",
        "\n",
        "\n",
        "        # remember best acc and save checkpoint based on clean validation accuracy\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            print(f\"Saving best model with clean validation accuracy: {best_acc:.2f}%\")\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        else:\n",
        "             print(f\"Validation accuracy did not improve. Best clean accuracy so far: {best_acc:.2f}%\")\n",
        "\n",
        "\n",
        "    print(\"\\nTraining finished.\")\n",
        "    print(f\"Best clean validation accuracy: {best_acc:.2f}%\")\n",
        "    # Optionally load the best model state dict at the end\n",
        "    # model.load_state_dict(torch.load(checkpoint_path))\n",
        "    # print(\"Loaded best model checkpoint.\")"
      ],
      "metadata": {
        "id": "xafR2U-neTRB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#@title small sanity check\n",
        "\n",
        "wandb.init(project=\"face-adv-fairness\", name=\"demogpairs-sanity-check-dataloaders\", config={\"learning_rate\": 0.001, \"epochs\": 1})\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Fix: Create val_loader directly from test_dataset, not from test_loader\n",
        "# Also, model should be initialized with the correct number of classes (number of unique identities)\n",
        "# Assuming 1000 unique identities based on previous code.\n",
        "num_identity_classes = 1000\n",
        "model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "# Create DataLoader directly from the test_dataset list\n",
        "# val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) # Shuffle usually False for validation\n",
        "\n",
        "# label_debug(model, val_loader, device)\n",
        "\n",
        "# # Ensure pgd attack is initialized with the model that has the correct output dimension\n",
        "pgd = LinfPGDAttack(model, epsilon=8/255, step_size = 2/255, steps = 10)\n",
        "\n",
        "# # Call eval_robust_celeba\n",
        "# eval_test_demogpairs(model, test_loader, device, name = 'test_set_clean', epoch = 0)\n",
        "\n",
        "eval_robust_demogpairs(model, test_loader, pgd, device, name='validation', epoch=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5IQILudwAOeX",
        "outputId": "e2d152b2-d0ba-4c4c-9a26-eb1bcd588bd5",
        "cellView": "form"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_asian_female</td><td></td></tr><tr><td>attack_success_rate_asian_male</td><td></td></tr><tr><td>attack_success_rate_black_female</td><td></td></tr><tr><td>attack_success_rate_black_male</td><td></td></tr><tr><td>attack_success_rate_white_female</td><td></td></tr><tr><td>attack_success_rate_white_male</td><td></td></tr><tr><td>overall_attack_success_rate_validation</td><td></td></tr><tr><td>robust_accuracy_asian_female</td><td></td></tr><tr><td>robust_accuracy_asian_male</td><td></td></tr><tr><td>robust_accuracy_black_female</td><td></td></tr><tr><td>robust_accuracy_black_male</td><td></td></tr><tr><td>robust_accuracy_validation</td><td></td></tr><tr><td>robust_accuracy_white_female</td><td></td></tr><tr><td>robust_accuracy_white_male</td><td></td></tr><tr><td>robust_loss_validation</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_asian_female</td><td>0</td></tr><tr><td>attack_success_rate_asian_male</td><td>0</td></tr><tr><td>attack_success_rate_black_female</td><td>0.01389</td></tr><tr><td>attack_success_rate_black_male</td><td>0</td></tr><tr><td>attack_success_rate_white_female</td><td>0</td></tr><tr><td>attack_success_rate_white_male</td><td>0</td></tr><tr><td>overall_attack_success_rate_validation</td><td>100</td></tr><tr><td>robust_accuracy_asian_female</td><td>0</td></tr><tr><td>robust_accuracy_asian_male</td><td>0</td></tr><tr><td>robust_accuracy_black_female</td><td>0</td></tr><tr><td>robust_accuracy_black_male</td><td>0</td></tr><tr><td>robust_accuracy_validation</td><td>0</td></tr><tr><td>robust_accuracy_white_female</td><td>0</td></tr><tr><td>robust_accuracy_white_male</td><td>0</td></tr><tr><td>robust_loss_validation</td><td>6.91844</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-sanity-check-dataloaders</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/t4irw3s4' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/t4irw3s4</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_171123-t4irw3s4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_171621-jgq1xlrf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/jgq1xlrf' target=\"_blank\">demogpairs-sanity-check-dataloaders</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/jgq1xlrf' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/jgq1xlrf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust Test (validation): Average loss: 6.9175, Robust Accuracy: 0/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (validation): 0.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6.917504261158131, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zrvtI7VX_nCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title convenience funtion to log predictions for a batch of test images\n",
        "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):\n",
        "  # obtain confidence scores for all classes\n",
        "  scores = F.softmax(outputs.data, dim=1)\n",
        "  log_scores = scores.cpu().numpy()\n",
        "  log_images = images.cpu().numpy()\n",
        "  log_labels = labels.cpu().numpy()\n",
        "  log_preds = predicted.cpu().numpy()\n",
        "  # adding ids based on the order of the images\n",
        "  _id = 0\n",
        "  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):\n",
        "    # Transpose image dimensions from (C, H, W) to (H, W, C) for wandb.Image\n",
        "    i_transposed = np.transpose(i, (1, 2, 0))\n",
        "\n",
        "    # add required info to data table:\n",
        "    # id, image pixels, model's guess, true label, scores for all classes\n",
        "    img_id = str(_id) + \"_\" + str(log_counter)\n",
        "    # Use the transposed image data\n",
        "    test_table.add_data(img_id, wandb.Image(i_transposed), p, l, *s)\n",
        "    _id += 1\n",
        "    if _id == batch_size:\n",
        "      break"
      ],
      "metadata": {
        "id": "QclN7fi5tVWy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training run: new, with balanced datasets\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "# or trades which was the method used in the original paper\n",
        "batch_size = 64\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=999).to(device) # ResNet for identity classification\n",
        "    # Note: number of classes (1000) should match the number of unique identities\n",
        "    # it gives very few examples on the test set\n",
        "\n",
        "\n",
        "    # make a new run for each example\n",
        "    wandb.init(project=\"face-adv-fairness\", name=f\"demogpairs-asian-women-{proportion}\", config={\"learning_rate\": 0.001, \"epochs\": 30})\n",
        "\n",
        "\n",
        "    num_identity_classes = 999 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    # maybe create these on the go?\n",
        "    train_loader = train_demogpair_loaders[prop]\n",
        "\n",
        "    val_loader = test_loader\n",
        "\n",
        "\n",
        "    # call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader=val_loader,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=20) # Save checkpoints with proportion\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6175MiPJr5en",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47a98485-bfb9-4723-e2b1-3782fed57f9f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-asian-women-0.25</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/27llpo0x' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/27llpo0x</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_171940-27llpo0x/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_172032-4km3l8n3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/4km3l8n3' target=\"_blank\">demogpairs-asian-women-0.25</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/4km3l8n3' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/4km3l8n3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Train Epoch: 0 [00064/1600 (4%)]\t Loss: 7.048247\n",
            "Robust Test (robust-validation): Average loss: 6.7663, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 66.9977, Accuracy: 2/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.09%\n",
            "\n",
            "Epoch 2/20\n",
            "Train Epoch: 1 [00064/1600 (4%)]\t Loss: 6.341015\n",
            "Robust Test (robust-validation): Average loss: 7.8329, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 291.7282, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.09%\n",
            "\n",
            "Epoch 3/20\n",
            "Train Epoch: 2 [00064/1600 (4%)]\t Loss: 6.413117\n",
            "Robust Test (robust-validation): Average loss: 6.9466, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.019444444444444445\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 16.2602, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.09%\n",
            "\n",
            "Epoch 4/20\n",
            "Train Epoch: 3 [00064/1600 (4%)]\t Loss: 6.307942\n",
            "Robust Test (robust-validation): Average loss: 7.4212, Robust Accuracy: 2/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 14.3682, Accuracy: 4/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.19%\n",
            "\n",
            "Epoch 5/20\n",
            "Train Epoch: 4 [00064/1600 (4%)]\t Loss: 6.276914\n",
            "Robust Test (robust-validation): Average loss: 7.5751, Robust Accuracy: 1/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 11.4360, Accuracy: 5/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.23%\n",
            "\n",
            "Epoch 6/20\n",
            "Train Epoch: 5 [00064/1600 (4%)]\t Loss: 6.115273\n",
            "Robust Test (robust-validation): Average loss: 7.7549, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 60.00%\n",
            "Test (clean-validation): Average loss: 9.6848, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.23%\n",
            "\n",
            "Epoch 7/20\n",
            "Train Epoch: 6 [00064/1600 (4%)]\t Loss: 5.716565\n",
            "Robust Test (robust-validation): Average loss: 8.0114, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 14.8378, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.23%\n",
            "\n",
            "Epoch 8/20\n",
            "Train Epoch: 7 [00064/1600 (4%)]\t Loss: 5.615521\n",
            "Robust Test (robust-validation): Average loss: 8.9361, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.011111111111111112\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 15.0611, Accuracy: 9/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.42%\n",
            "\n",
            "Epoch 9/20\n",
            "Train Epoch: 8 [00064/1600 (4%)]\t Loss: 5.375865\n",
            "Robust Test (robust-validation): Average loss: 9.8924, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.008333333333333333\n",
            "Overall Attack success rate (robust-validation): 88.89%\n",
            "Test (clean-validation): Average loss: 12.2397, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.42%\n",
            "\n",
            "Epoch 10/20\n",
            "Train Epoch: 9 [00064/1600 (4%)]\t Loss: 5.260144\n",
            "Robust Test (robust-validation): Average loss: 10.1174, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.008333333333333333\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.008333333333333333\n",
            "Overall Attack success rate (robust-validation): 57.89%\n",
            "Test (clean-validation): Average loss: 10.3562, Accuracy: 19/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.88%\n",
            "\n",
            "Epoch 11/20\n",
            "Train Epoch: 10 [00064/1600 (4%)]\t Loss: 4.859128\n",
            "Robust Test (robust-validation): Average loss: 8.9997, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.008333333333333333\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 43.75%\n",
            "Test (clean-validation): Average loss: 8.8081, Accuracy: 16/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.88%\n",
            "\n",
            "Epoch 12/20\n",
            "Train Epoch: 11 [00064/1600 (4%)]\t Loss: 4.543684\n",
            "Robust Test (robust-validation): Average loss: 11.3959, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.008333333333333333\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 53.33%\n",
            "Test (clean-validation): Average loss: 10.5523, Accuracy: 15/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.88%\n",
            "\n",
            "Epoch 13/20\n",
            "Train Epoch: 12 [00064/1600 (4%)]\t Loss: 4.269510\n",
            "Robust Test (robust-validation): Average loss: 11.8010, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 60.00%\n",
            "Test (clean-validation): Average loss: 10.6767, Accuracy: 15/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.88%\n",
            "\n",
            "Epoch 14/20\n",
            "Train Epoch: 13 [00064/1600 (4%)]\t Loss: 4.473408\n",
            "Robust Test (robust-validation): Average loss: 11.0916, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.008333333333333333\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 90.91%\n",
            "Test (clean-validation): Average loss: 11.5786, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.88%\n",
            "\n",
            "Epoch 15/20\n",
            "Train Epoch: 14 [00064/1600 (4%)]\t Loss: 3.606137\n",
            "Robust Test (robust-validation): Average loss: 12.6670, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 77.78%\n",
            "Test (clean-validation): Average loss: 11.3227, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.88%\n",
            "\n",
            "Epoch 16/20\n",
            "Train Epoch: 15 [00064/1600 (4%)]\t Loss: 3.754239\n",
            "Robust Test (robust-validation): Average loss: 12.9739, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 52.63%\n",
            "Test (clean-validation): Average loss: 11.5653, Accuracy: 19/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.88%\n",
            "\n",
            "Epoch 17/20\n",
            "Train Epoch: 16 [00064/1600 (4%)]\t Loss: 3.338576\n",
            "Robust Test (robust-validation): Average loss: 14.1410, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.013888888888888888\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 92.31%\n",
            "Test (clean-validation): Average loss: 13.5722, Accuracy: 13/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.88%\n",
            "\n",
            "Epoch 18/20\n",
            "Train Epoch: 17 [00064/1600 (4%)]\t Loss: 2.738995\n",
            "Robust Test (robust-validation): Average loss: 12.6513, Robust Accuracy: 2/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 87.50%\n",
            "Test (clean-validation): Average loss: 13.1563, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.88%\n",
            "\n",
            "Epoch 19/20\n",
            "Train Epoch: 18 [00064/1600 (4%)]\t Loss: 3.035269\n",
            "Robust Test (robust-validation): Average loss: 15.2463, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.008333333333333333\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.011111111111111112\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 81.25%\n",
            "Test (clean-validation): Average loss: 14.9667, Accuracy: 16/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.88%\n",
            "\n",
            "Epoch 20/20\n",
            "Train Epoch: 19 [00064/1600 (4%)]\t Loss: 2.595393\n",
            "Robust Test (robust-validation): Average loss: 13.7529, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.013888888888888888\n",
            "Overall Attack success rate (robust-validation): 61.90%\n",
            "Test (clean-validation): Average loss: 12.2734, Accuracy: 21/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.97%\n",
            "\n",
            "Training finished.\n",
            "Best clean validation accuracy: 0.97%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td></td></tr><tr><td>accuracy_asian_male</td><td></td></tr><tr><td>accuracy_black_female</td><td></td></tr><tr><td>accuracy_black_male</td><td></td></tr><tr><td>accuracy_white_female</td><td></td></tr><tr><td>accuracy_white_male</td><td></td></tr><tr><td>attack_success_rate_asian_female</td><td></td></tr><tr><td>attack_success_rate_asian_male</td><td></td></tr><tr><td>attack_success_rate_black_female</td><td></td></tr><tr><td>attack_success_rate_black_male</td><td></td></tr><tr><td>attack_success_rate_white_female</td><td></td></tr><tr><td>attack_success_rate_white_male</td><td></td></tr><tr><td>clean_test_accuracy clean-validation</td><td></td></tr><tr><td>clean_test_loss clean-validation</td><td></td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td></td></tr><tr><td>robust_accuracy_asian_female</td><td></td></tr><tr><td>robust_accuracy_asian_male</td><td></td></tr><tr><td>robust_accuracy_black_female</td><td></td></tr><tr><td>robust_accuracy_black_male</td><td></td></tr><tr><td>robust_accuracy_robust-validation</td><td></td></tr><tr><td>robust_accuracy_white_female</td><td></td></tr><tr><td>robust_accuracy_white_male</td><td></td></tr><tr><td>robust_loss_robust-validation</td><td></td></tr><tr><td>train_loss DemogPairsDataset</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td>0</td></tr><tr><td>accuracy_asian_male</td><td>0</td></tr><tr><td>accuracy_black_female</td><td>0</td></tr><tr><td>accuracy_black_male</td><td>0</td></tr><tr><td>accuracy_white_female</td><td>0</td></tr><tr><td>accuracy_white_male</td><td>0</td></tr><tr><td>attack_success_rate_asian_female</td><td>0.01111</td></tr><tr><td>attack_success_rate_asian_male</td><td>0.00278</td></tr><tr><td>attack_success_rate_black_female</td><td>0.00556</td></tr><tr><td>attack_success_rate_black_male</td><td>0.01389</td></tr><tr><td>attack_success_rate_white_female</td><td>0.00278</td></tr><tr><td>attack_success_rate_white_male</td><td>0</td></tr><tr><td>clean_test_accuracy clean-validation</td><td>0.97222</td></tr><tr><td>clean_test_loss clean-validation</td><td>12.2734</td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td>61.90476</td></tr><tr><td>robust_accuracy_asian_female</td><td>0.01111</td></tr><tr><td>robust_accuracy_asian_male</td><td>0</td></tr><tr><td>robust_accuracy_black_female</td><td>0</td></tr><tr><td>robust_accuracy_black_male</td><td>0.00556</td></tr><tr><td>robust_accuracy_robust-validation</td><td>0.37037</td></tr><tr><td>robust_accuracy_white_female</td><td>0</td></tr><tr><td>robust_accuracy_white_male</td><td>0.00556</td></tr><tr><td>robust_loss_robust-validation</td><td>13.75286</td></tr><tr><td>train_loss DemogPairsDataset</td><td>2.59539</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-asian-women-0.25</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/4km3l8n3' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/4km3l8n3</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_172032-4km3l8n3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_173834-p4xhs3wk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p4xhs3wk' target=\"_blank\">demogpairs-asian-women-0.5</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p4xhs3wk' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p4xhs3wk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Train Epoch: 0 [00064/1600 (4%)]\t Loss: 7.019219\n",
            "Robust Test (robust-validation): Average loss: 6.7645, Robust Accuracy: 2/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 15.7596, Accuracy: 2/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.09%\n",
            "\n",
            "Epoch 2/20\n",
            "Train Epoch: 1 [00064/1600 (4%)]\t Loss: 6.354087\n",
            "Robust Test (robust-validation): Average loss: 6.7775, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 52.0408, Accuracy: 3/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.14%\n",
            "\n",
            "Epoch 3/20\n",
            "Train Epoch: 2 [00064/1600 (4%)]\t Loss: 6.398646\n",
            "Robust Test (robust-validation): Average loss: 8.1440, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 18.2635, Accuracy: 4/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.19%\n",
            "\n",
            "Epoch 4/20\n",
            "Train Epoch: 3 [00064/1600 (4%)]\t Loss: 6.198156\n",
            "Robust Test (robust-validation): Average loss: 8.0951, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 50.00%\n",
            "Test (clean-validation): Average loss: 15.0544, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 5/20\n",
            "Train Epoch: 4 [00064/1600 (4%)]\t Loss: 6.075043\n",
            "Robust Test (robust-validation): Average loss: 8.1529, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 50.00%\n",
            "Test (clean-validation): Average loss: 11.8386, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 6/20\n",
            "Train Epoch: 5 [00064/1600 (4%)]\t Loss: 5.875442\n",
            "Robust Test (robust-validation): Average loss: 9.4067, Robust Accuracy: 2/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 71.43%\n",
            "Test (clean-validation): Average loss: 12.6434, Accuracy: 7/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.32%\n",
            "\n",
            "Epoch 7/20\n",
            "Train Epoch: 6 [00064/1600 (4%)]\t Loss: 5.690179\n",
            "Robust Test (robust-validation): Average loss: 10.4776, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 50.00%\n",
            "Test (clean-validation): Average loss: 13.3559, Accuracy: 8/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.37%\n",
            "\n",
            "Epoch 8/20\n",
            "Train Epoch: 7 [00064/1600 (4%)]\t Loss: 5.325732\n",
            "Robust Test (robust-validation): Average loss: 9.9223, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 50.00%\n",
            "Test (clean-validation): Average loss: 11.6463, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.37%\n",
            "\n",
            "Epoch 9/20\n",
            "Train Epoch: 8 [00064/1600 (4%)]\t Loss: 4.727607\n",
            "Robust Test (robust-validation): Average loss: 12.1737, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 55.56%\n",
            "Test (clean-validation): Average loss: 16.3934, Accuracy: 9/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.42%\n",
            "\n",
            "Epoch 10/20\n",
            "Train Epoch: 9 [00064/1600 (4%)]\t Loss: 4.873722\n",
            "Robust Test (robust-validation): Average loss: 13.7781, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 75.00%\n",
            "Test (clean-validation): Average loss: 23.7201, Accuracy: 4/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.42%\n",
            "\n",
            "Epoch 11/20\n",
            "Train Epoch: 10 [00064/1600 (4%)]\t Loss: 4.749574\n",
            "Robust Test (robust-validation): Average loss: 11.9524, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 17.6319, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.42%\n",
            "\n",
            "Epoch 12/20\n",
            "Train Epoch: 11 [00064/1600 (4%)]\t Loss: 4.558666\n",
            "Robust Test (robust-validation): Average loss: 12.0883, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 18.6910, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.42%\n",
            "\n",
            "Epoch 13/20\n",
            "Train Epoch: 12 [00064/1600 (4%)]\t Loss: 4.255734\n",
            "Robust Test (robust-validation): Average loss: 12.0630, Robust Accuracy: 11/2160 (1%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.022222222222222223\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 90.00%\n",
            "Test (clean-validation): Average loss: 15.1576, Accuracy: 10/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.46%\n",
            "\n",
            "Epoch 14/20\n",
            "Train Epoch: 13 [00064/1600 (4%)]\t Loss: 3.880770\n",
            "Robust Test (robust-validation): Average loss: 14.3578, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.008333333333333333\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 75.00%\n",
            "Test (clean-validation): Average loss: 17.6110, Accuracy: 4/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 15/20\n",
            "Train Epoch: 14 [00064/1600 (4%)]\t Loss: 3.690141\n",
            "Robust Test (robust-validation): Average loss: 14.9356, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 16.6492, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 16/20\n",
            "Train Epoch: 15 [00064/1600 (4%)]\t Loss: 3.185738\n",
            "Robust Test (robust-validation): Average loss: 14.2032, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 80.00%\n",
            "Test (clean-validation): Average loss: 16.4116, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 17/20\n",
            "Train Epoch: 16 [00064/1600 (4%)]\t Loss: 3.075612\n",
            "Robust Test (robust-validation): Average loss: 15.0596, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 83.33%\n",
            "Test (clean-validation): Average loss: 16.0188, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 18/20\n",
            "Train Epoch: 17 [00064/1600 (4%)]\t Loss: 3.270344\n",
            "Robust Test (robust-validation): Average loss: 17.1755, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 87.50%\n",
            "Test (clean-validation): Average loss: 20.6392, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 19/20\n",
            "Train Epoch: 18 [00064/1600 (4%)]\t Loss: 3.036934\n",
            "Robust Test (robust-validation): Average loss: 17.2225, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.008333333333333333\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.005555555555555556\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 90.91%\n",
            "Test (clean-validation): Average loss: 20.2268, Accuracy: 11/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.51%\n",
            "\n",
            "Epoch 20/20\n",
            "Train Epoch: 19 [00064/1600 (4%)]\t Loss: 2.649241\n",
            "Robust Test (robust-validation): Average loss: 16.4195, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.008333333333333333\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 83.33%\n",
            "Test (clean-validation): Average loss: 17.2624, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Training finished.\n",
            "Best clean validation accuracy: 0.51%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td></td></tr><tr><td>accuracy_asian_male</td><td></td></tr><tr><td>accuracy_black_female</td><td></td></tr><tr><td>accuracy_black_male</td><td></td></tr><tr><td>accuracy_white_female</td><td></td></tr><tr><td>accuracy_white_male</td><td></td></tr><tr><td>attack_success_rate_asian_female</td><td></td></tr><tr><td>attack_success_rate_asian_male</td><td></td></tr><tr><td>attack_success_rate_black_female</td><td></td></tr><tr><td>attack_success_rate_black_male</td><td></td></tr><tr><td>attack_success_rate_white_female</td><td></td></tr><tr><td>attack_success_rate_white_male</td><td></td></tr><tr><td>clean_test_accuracy clean-validation</td><td></td></tr><tr><td>clean_test_loss clean-validation</td><td></td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td></td></tr><tr><td>robust_accuracy_asian_female</td><td></td></tr><tr><td>robust_accuracy_asian_male</td><td></td></tr><tr><td>robust_accuracy_black_female</td><td></td></tr><tr><td>robust_accuracy_black_male</td><td></td></tr><tr><td>robust_accuracy_robust-validation</td><td></td></tr><tr><td>robust_accuracy_white_female</td><td></td></tr><tr><td>robust_accuracy_white_male</td><td></td></tr><tr><td>robust_loss_robust-validation</td><td></td></tr><tr><td>train_loss DemogPairsDataset</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td>0</td></tr><tr><td>accuracy_asian_male</td><td>0</td></tr><tr><td>accuracy_black_female</td><td>0</td></tr><tr><td>accuracy_black_male</td><td>0</td></tr><tr><td>accuracy_white_female</td><td>0</td></tr><tr><td>accuracy_white_male</td><td>0</td></tr><tr><td>attack_success_rate_asian_female</td><td>0.00278</td></tr><tr><td>attack_success_rate_asian_male</td><td>0.00833</td></tr><tr><td>attack_success_rate_black_female</td><td>0</td></tr><tr><td>attack_success_rate_black_male</td><td>0.00278</td></tr><tr><td>attack_success_rate_white_female</td><td>0</td></tr><tr><td>attack_success_rate_white_male</td><td>0</td></tr><tr><td>clean_test_accuracy clean-validation</td><td>0.27778</td></tr><tr><td>clean_test_loss clean-validation</td><td>17.26239</td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td>83.33333</td></tr><tr><td>robust_accuracy_asian_female</td><td>0.00833</td></tr><tr><td>robust_accuracy_asian_male</td><td>0</td></tr><tr><td>robust_accuracy_black_female</td><td>0.00278</td></tr><tr><td>robust_accuracy_black_male</td><td>0.01111</td></tr><tr><td>robust_accuracy_robust-validation</td><td>0.37037</td></tr><tr><td>robust_accuracy_white_female</td><td>0</td></tr><tr><td>robust_accuracy_white_male</td><td>0</td></tr><tr><td>robust_loss_robust-validation</td><td>16.41951</td></tr><tr><td>train_loss DemogPairsDataset</td><td>2.64924</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-asian-women-0.5</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p4xhs3wk' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p4xhs3wk</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_173834-p4xhs3wk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_175634-wcs3yo3e</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/wcs3yo3e' target=\"_blank\">demogpairs-asian-women-0.75</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/wcs3yo3e' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/wcs3yo3e</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Train Epoch: 0 [00064/1600 (4%)]\t Loss: 7.078086\n",
            "Robust Test (robust-validation): Average loss: 7.1556, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 66.67%\n",
            "Test (clean-validation): Average loss: 51.3893, Accuracy: 3/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.14%\n",
            "\n",
            "Epoch 2/20\n",
            "Train Epoch: 1 [00064/1600 (4%)]\t Loss: 6.339103\n",
            "Robust Test (robust-validation): Average loss: 7.9589, Robust Accuracy: 1/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 105.5488, Accuracy: 5/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.23%\n",
            "\n",
            "Epoch 3/20\n",
            "Train Epoch: 2 [00064/1600 (4%)]\t Loss: 6.364654\n",
            "Robust Test (robust-validation): Average loss: 7.0867, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 23.1924, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.23%\n",
            "\n",
            "Epoch 4/20\n",
            "Train Epoch: 3 [00064/1600 (4%)]\t Loss: 6.225765\n",
            "Robust Test (robust-validation): Average loss: 7.2706, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 14.8558, Accuracy: 6/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.28%\n",
            "\n",
            "Epoch 5/20\n",
            "Train Epoch: 4 [00064/1600 (4%)]\t Loss: 6.164522\n",
            "Robust Test (robust-validation): Average loss: 7.6974, Robust Accuracy: 0/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 11.6897, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.28%\n",
            "\n",
            "Epoch 6/20\n",
            "Train Epoch: 5 [00064/1600 (4%)]\t Loss: 6.038443\n",
            "Robust Test (robust-validation): Average loss: 8.0534, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 15.1124, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.28%\n",
            "\n",
            "Epoch 7/20\n",
            "Train Epoch: 6 [00064/1600 (4%)]\t Loss: 5.710633\n",
            "Robust Test (robust-validation): Average loss: 10.9420, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 18.4102, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.28%\n",
            "\n",
            "Epoch 8/20\n",
            "Train Epoch: 7 [00064/1600 (4%)]\t Loss: 5.467755\n",
            "Robust Test (robust-validation): Average loss: 11.1574, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.008333333333333333\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 16.9008, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.28%\n",
            "\n",
            "Epoch 9/20\n",
            "Train Epoch: 8 [00064/1600 (4%)]\t Loss: 5.292646\n",
            "Robust Test (robust-validation): Average loss: 9.2882, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 83.33%\n",
            "Test (clean-validation): Average loss: 9.8619, Accuracy: 12/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.56%\n",
            "\n",
            "Epoch 10/20\n",
            "Train Epoch: 9 [00064/1600 (4%)]\t Loss: 4.793685\n",
            "Robust Test (robust-validation): Average loss: 10.9307, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 23.4131, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 11/20\n",
            "Train Epoch: 10 [00064/1600 (4%)]\t Loss: 4.805583\n",
            "Robust Test (robust-validation): Average loss: 10.5502, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.008333333333333333\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.008333333333333333\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 12.6863, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 12/20\n",
            "Train Epoch: 11 [00064/1600 (4%)]\t Loss: 4.533932\n",
            "Robust Test (robust-validation): Average loss: 10.9005, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.013888888888888888\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 85.71%\n",
            "Test (clean-validation): Average loss: 10.8987, Accuracy: 14/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.65%\n",
            "\n",
            "Epoch 13/20\n",
            "Train Epoch: 12 [00064/1600 (4%)]\t Loss: 4.570698\n",
            "Robust Test (robust-validation): Average loss: 11.0047, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.019444444444444445\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 72.22%\n",
            "Test (clean-validation): Average loss: 11.2501, Accuracy: 18/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.83%\n",
            "\n",
            "Epoch 14/20\n",
            "Train Epoch: 13 [00064/1600 (4%)]\t Loss: 3.963629\n",
            "Robust Test (robust-validation): Average loss: 11.7644, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.019444444444444445\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.008333333333333333\n",
            "Overall Attack success rate (robust-validation): 77.27%\n",
            "Test (clean-validation): Average loss: 10.8044, Accuracy: 22/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 1.02%\n",
            "\n",
            "Epoch 15/20\n",
            "Train Epoch: 14 [00064/1600 (4%)]\t Loss: 3.743801\n",
            "Robust Test (robust-validation): Average loss: 13.9260, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.022222222222222223\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 80.00%\n",
            "Test (clean-validation): Average loss: 13.1697, Accuracy: 15/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 1.02%\n",
            "\n",
            "Epoch 16/20\n",
            "Train Epoch: 15 [00064/1600 (4%)]\t Loss: 3.597173\n",
            "Robust Test (robust-validation): Average loss: 17.4629, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.008333333333333333\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.019444444444444445\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 72.73%\n",
            "Test (clean-validation): Average loss: 16.2949, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 1.02%\n",
            "\n",
            "Epoch 17/20\n",
            "Train Epoch: 16 [00064/1600 (4%)]\t Loss: 3.039113\n",
            "Robust Test (robust-validation): Average loss: 16.6250, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 70.59%\n",
            "Test (clean-validation): Average loss: 16.1069, Accuracy: 17/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 1.02%\n",
            "\n",
            "Epoch 18/20\n",
            "Train Epoch: 17 [00064/1600 (4%)]\t Loss: 2.563397\n",
            "Robust Test (robust-validation): Average loss: 13.1887, Robust Accuracy: 12/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.008333333333333333\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.016666666666666666\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 64.29%\n",
            "Test (clean-validation): Average loss: 14.5072, Accuracy: 14/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 1.02%\n",
            "\n",
            "Epoch 19/20\n",
            "Train Epoch: 18 [00064/1600 (4%)]\t Loss: 2.896820\n",
            "Robust Test (robust-validation): Average loss: 14.5252, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.011111111111111112\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 69.23%\n",
            "Test (clean-validation): Average loss: 16.7910, Accuracy: 13/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 1.02%\n",
            "\n",
            "Epoch 20/20\n",
            "Train Epoch: 19 [00064/1600 (4%)]\t Loss: 2.476136\n",
            "Robust Test (robust-validation): Average loss: 16.3357, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.008333333333333333\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.011111111111111112\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 54.55%\n",
            "Test (clean-validation): Average loss: 17.5661, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 1.02%\n",
            "\n",
            "Training finished.\n",
            "Best clean validation accuracy: 1.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training run: new, with balanced datasets\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "# or trades which was the method used in the original paper\n",
        "batch_size = 64\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=999).to(device) # ResNet for identity classification\n",
        "    # Note: number of classes (1000) should match the number of unique identities\n",
        "    # it gives very few examples on the test set\n",
        "\n",
        "\n",
        "    # make a new run for each example\n",
        "    wandb.init(project=\"face-adv-fairness\", name=f\"demogpairs-asian-women-{proportion}\", config={\"learning_rate\": 0.001, \"epochs\": 50, 'steps':5})\n",
        "\n",
        "\n",
        "    num_identity_classes = 999 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/5, steps = 5)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    # maybe create these on the go?\n",
        "    train_loader = train_demogpair_loaders[prop]\n",
        "\n",
        "    val_loader = test_loader\n",
        "\n",
        "\n",
        "    # call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader=val_loader,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=50) # Save checkpoints with proportion\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JxypbLzaZh01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1bb88c1-3ce2-4b32-95dc-d98f1b516604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td></td></tr><tr><td>accuracy_asian_male</td><td></td></tr><tr><td>accuracy_black_female</td><td></td></tr><tr><td>accuracy_black_male</td><td></td></tr><tr><td>accuracy_white_female</td><td></td></tr><tr><td>accuracy_white_male</td><td></td></tr><tr><td>attack_success_rate_asian_female</td><td></td></tr><tr><td>attack_success_rate_asian_male</td><td></td></tr><tr><td>attack_success_rate_black_female</td><td></td></tr><tr><td>attack_success_rate_black_male</td><td></td></tr><tr><td>attack_success_rate_white_female</td><td></td></tr><tr><td>attack_success_rate_white_male</td><td></td></tr><tr><td>clean_test_accuracy clean-validation</td><td></td></tr><tr><td>clean_test_loss clean-validation</td><td></td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td></td></tr><tr><td>robust_accuracy_asian_female</td><td></td></tr><tr><td>robust_accuracy_asian_male</td><td></td></tr><tr><td>robust_accuracy_black_female</td><td></td></tr><tr><td>robust_accuracy_black_male</td><td></td></tr><tr><td>robust_accuracy_robust-validation</td><td></td></tr><tr><td>robust_accuracy_white_female</td><td></td></tr><tr><td>robust_accuracy_white_male</td><td></td></tr><tr><td>robust_loss_robust-validation</td><td></td></tr><tr><td>train_loss DemogPairsDataset</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td>0</td></tr><tr><td>accuracy_asian_male</td><td>0</td></tr><tr><td>accuracy_black_female</td><td>0</td></tr><tr><td>accuracy_black_male</td><td>0</td></tr><tr><td>accuracy_white_female</td><td>0</td></tr><tr><td>accuracy_white_male</td><td>0</td></tr><tr><td>attack_success_rate_asian_female</td><td>0.01389</td></tr><tr><td>attack_success_rate_asian_male</td><td>0</td></tr><tr><td>attack_success_rate_black_female</td><td>0</td></tr><tr><td>attack_success_rate_black_male</td><td>0</td></tr><tr><td>attack_success_rate_white_female</td><td>0</td></tr><tr><td>attack_success_rate_white_male</td><td>0</td></tr><tr><td>clean_test_accuracy clean-validation</td><td>0.23148</td></tr><tr><td>clean_test_loss clean-validation</td><td>8.2814</td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td>100</td></tr><tr><td>robust_accuracy_asian_female</td><td>0.00556</td></tr><tr><td>robust_accuracy_asian_male</td><td>0</td></tr><tr><td>robust_accuracy_black_female</td><td>0</td></tr><tr><td>robust_accuracy_black_male</td><td>0</td></tr><tr><td>robust_accuracy_robust-validation</td><td>0.09259</td></tr><tr><td>robust_accuracy_white_female</td><td>0</td></tr><tr><td>robust_accuracy_white_male</td><td>0</td></tr><tr><td>robust_loss_robust-validation</td><td>7.53894</td></tr><tr><td>train_loss DemogPairsDataset</td><td>6.20856</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-asian-women-0.25</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/5hfjrlv3' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/5hfjrlv3</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_181952-5hfjrlv3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_182227-ja99dhnb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/ja99dhnb' target=\"_blank\">demogpairs-asian-women-0.25</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/ja99dhnb' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/ja99dhnb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "Train Epoch: 0 [00064/1600 (4%)]\t Loss: 7.062039\n",
            "Robust Test (robust-validation): Average loss: 6.7901, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 14.5563, Accuracy: 4/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.19%\n",
            "\n",
            "Epoch 2/50\n",
            "Train Epoch: 1 [00064/1600 (4%)]\t Loss: 6.292163\n",
            "Robust Test (robust-validation): Average loss: 6.6014, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 57.4042, Accuracy: 1/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 3/50\n",
            "Train Epoch: 2 [00064/1600 (4%)]\t Loss: 6.327252\n",
            "Robust Test (robust-validation): Average loss: 7.5232, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 27.0304, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 4/50\n",
            "Train Epoch: 3 [00064/1600 (4%)]\t Loss: 6.185514\n",
            "Robust Test (robust-validation): Average loss: 7.1994, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 22.0725, Accuracy: 1/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 5/50\n",
            "Train Epoch: 4 [00064/1600 (4%)]\t Loss: 6.147258\n",
            "Robust Test (robust-validation): Average loss: 8.2607, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.016666666666666666\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 75.00%\n",
            "Test (clean-validation): Average loss: 19.4039, Accuracy: 4/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 6/50\n",
            "Train Epoch: 5 [00064/1600 (4%)]\t Loss: 5.961950\n",
            "Robust Test (robust-validation): Average loss: 8.8825, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 71.43%\n",
            "Test (clean-validation): Average loss: 11.5593, Accuracy: 7/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.32%\n",
            "\n",
            "Epoch 7/50\n",
            "Train Epoch: 6 [00064/1600 (4%)]\t Loss: 5.809439\n",
            "Robust Test (robust-validation): Average loss: 8.5312, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 87.50%\n",
            "Test (clean-validation): Average loss: 10.8140, Accuracy: 8/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.37%\n",
            "\n",
            "Epoch 8/50\n",
            "Train Epoch: 7 [00064/1600 (4%)]\t Loss: 5.427960\n",
            "Robust Test (robust-validation): Average loss: 11.4820, Robust Accuracy: 2/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 15.8154, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.37%\n",
            "\n",
            "Epoch 9/50\n",
            "Train Epoch: 8 [00064/1600 (4%)]\t Loss: 5.538305\n",
            "Robust Test (robust-validation): Average loss: 14.7128, Robust Accuracy: 2/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 15.6516, Accuracy: 4/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.37%\n",
            "\n",
            "Epoch 10/50\n",
            "Train Epoch: 9 [00064/1600 (4%)]\t Loss: 5.384553\n",
            "Robust Test (robust-validation): Average loss: 19.4057, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 62.50%\n",
            "Test (clean-validation): Average loss: 20.7859, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.37%\n",
            "\n",
            "Epoch 11/50\n",
            "Train Epoch: 10 [00064/1600 (4%)]\t Loss: 5.227950\n",
            "Robust Test (robust-validation): Average loss: 22.4535, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 70.00%\n",
            "Test (clean-validation): Average loss: 22.1766, Accuracy: 10/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.46%\n",
            "\n",
            "Epoch 12/50\n",
            "Train Epoch: 11 [00064/1600 (4%)]\t Loss: 4.815483\n",
            "Robust Test (robust-validation): Average loss: 17.1547, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 40.00%\n",
            "Test (clean-validation): Average loss: 18.3193, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 13/50\n",
            "Train Epoch: 12 [00064/1600 (4%)]\t Loss: 4.520345\n",
            "Robust Test (robust-validation): Average loss: 13.1951, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 75.00%\n",
            "Test (clean-validation): Average loss: 14.9998, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 14/50\n",
            "Train Epoch: 13 [00064/1600 (4%)]\t Loss: 4.754317\n",
            "Robust Test (robust-validation): Average loss: 10.8073, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.005555555555555556\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 11.7280, Accuracy: 4/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 15/50\n",
            "Train Epoch: 14 [00064/1600 (4%)]\t Loss: 4.827094\n",
            "Robust Test (robust-validation): Average loss: 11.6552, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.005555555555555556\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 12.3562, Accuracy: 4/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 16/50\n",
            "Train Epoch: 15 [00064/1600 (4%)]\t Loss: 4.300609\n",
            "Robust Test (robust-validation): Average loss: 12.6445, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 12.1661, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 17/50\n",
            "Train Epoch: 16 [00064/1600 (4%)]\t Loss: 3.782922\n",
            "Robust Test (robust-validation): Average loss: 12.6186, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 12.9029, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 18/50\n",
            "Train Epoch: 17 [00064/1600 (4%)]\t Loss: 3.602814\n",
            "Robust Test (robust-validation): Average loss: 13.5759, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 88.89%\n",
            "Test (clean-validation): Average loss: 13.9850, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 19/50\n",
            "Train Epoch: 18 [00064/1600 (4%)]\t Loss: 3.362559\n",
            "Robust Test (robust-validation): Average loss: 13.5866, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 87.50%\n",
            "Test (clean-validation): Average loss: 14.1525, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 20/50\n",
            "Train Epoch: 19 [00064/1600 (4%)]\t Loss: 3.114572\n",
            "Robust Test (robust-validation): Average loss: 14.3491, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 87.50%\n",
            "Test (clean-validation): Average loss: 15.5164, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 21/50\n",
            "Train Epoch: 20 [00064/1600 (4%)]\t Loss: 2.671081\n",
            "Robust Test (robust-validation): Average loss: 15.2686, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.011111111111111112\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 17.5371, Accuracy: 7/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 22/50\n",
            "Train Epoch: 21 [00064/1600 (4%)]\t Loss: 2.999015\n",
            "Robust Test (robust-validation): Average loss: 15.4094, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 16.9392, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 23/50\n",
            "Train Epoch: 22 [00064/1600 (4%)]\t Loss: 2.405602\n",
            "Robust Test (robust-validation): Average loss: 17.9185, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 87.50%\n",
            "Test (clean-validation): Average loss: 18.4893, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 24/50\n",
            "Train Epoch: 23 [00064/1600 (4%)]\t Loss: 2.040850\n",
            "Robust Test (robust-validation): Average loss: 17.9503, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 20.1032, Accuracy: 7/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 25/50\n",
            "Train Epoch: 24 [00064/1600 (4%)]\t Loss: 1.974979\n",
            "Robust Test (robust-validation): Average loss: 18.2183, Robust Accuracy: 10/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.005555555555555556\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 20.3909, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 26/50\n",
            "Train Epoch: 25 [00064/1600 (4%)]\t Loss: 1.739267\n",
            "Robust Test (robust-validation): Average loss: 18.3589, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 90.91%\n",
            "Test (clean-validation): Average loss: 18.7320, Accuracy: 11/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.51%\n",
            "\n",
            "Epoch 27/50\n",
            "Train Epoch: 26 [00064/1600 (4%)]\t Loss: 1.601884\n",
            "Robust Test (robust-validation): Average loss: 18.4631, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 20.6671, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 28/50\n",
            "Train Epoch: 27 [00064/1600 (4%)]\t Loss: 1.492287\n",
            "Robust Test (robust-validation): Average loss: 18.4639, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.005555555555555556\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 88.89%\n",
            "Test (clean-validation): Average loss: 21.1855, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 29/50\n",
            "Train Epoch: 28 [00064/1600 (4%)]\t Loss: 1.404029\n",
            "Robust Test (robust-validation): Average loss: 17.8167, Robust Accuracy: 10/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.011111111111111112\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 88.89%\n",
            "Test (clean-validation): Average loss: 21.8341, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 30/50\n",
            "Train Epoch: 29 [00064/1600 (4%)]\t Loss: 1.353105\n",
            "Robust Test (robust-validation): Average loss: 19.6933, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.011111111111111112\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 22.4138, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 31/50\n",
            "Train Epoch: 30 [00064/1600 (4%)]\t Loss: 1.145275\n",
            "Robust Test (robust-validation): Average loss: 20.6338, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 85.71%\n",
            "Test (clean-validation): Average loss: 21.6902, Accuracy: 7/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 32/50\n",
            "Train Epoch: 31 [00064/1600 (4%)]\t Loss: 1.249211\n",
            "Robust Test (robust-validation): Average loss: 20.8944, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.011111111111111112\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 22.8736, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 33/50\n",
            "Train Epoch: 32 [00064/1600 (4%)]\t Loss: 1.084411\n",
            "Robust Test (robust-validation): Average loss: 20.6721, Robust Accuracy: 10/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 88.89%\n",
            "Test (clean-validation): Average loss: 23.1535, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 34/50\n",
            "Train Epoch: 33 [00064/1600 (4%)]\t Loss: 0.899246\n",
            "Robust Test (robust-validation): Average loss: 19.8684, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.013888888888888888\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 62.50%\n",
            "Test (clean-validation): Average loss: 22.2902, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 35/50\n",
            "Train Epoch: 34 [00064/1600 (4%)]\t Loss: 0.792411\n",
            "Robust Test (robust-validation): Average loss: 20.0418, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.011111111111111112\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 22.6962, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 36/50\n",
            "Train Epoch: 35 [00064/1600 (4%)]\t Loss: 0.633741\n",
            "Robust Test (robust-validation): Average loss: 21.1740, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 24.2843, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 37/50\n",
            "Train Epoch: 36 [00064/1600 (4%)]\t Loss: 0.440376\n",
            "Robust Test (robust-validation): Average loss: 20.9022, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 23.1888, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 38/50\n",
            "Train Epoch: 37 [00064/1600 (4%)]\t Loss: 0.327829\n",
            "Robust Test (robust-validation): Average loss: 21.4193, Robust Accuracy: 12/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 81.82%\n",
            "Test (clean-validation): Average loss: 23.5289, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 39/50\n",
            "Train Epoch: 38 [00064/1600 (4%)]\t Loss: 0.249254\n",
            "Robust Test (robust-validation): Average loss: 21.9111, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 88.89%\n",
            "Test (clean-validation): Average loss: 24.3250, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 40/50\n",
            "Train Epoch: 39 [00064/1600 (4%)]\t Loss: 0.245739\n",
            "Robust Test (robust-validation): Average loss: 21.8707, Robust Accuracy: 12/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 24.4385, Accuracy: 7/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 41/50\n",
            "Train Epoch: 40 [00064/1600 (4%)]\t Loss: 0.159227\n",
            "Robust Test (robust-validation): Average loss: 22.6378, Robust Accuracy: 12/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 24.5196, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 42/50\n",
            "Train Epoch: 41 [00064/1600 (4%)]\t Loss: 0.149510\n",
            "Robust Test (robust-validation): Average loss: 22.4415, Robust Accuracy: 14/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.013888888888888888\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 24.1838, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 43/50\n",
            "Train Epoch: 42 [00064/1600 (4%)]\t Loss: 0.150551\n",
            "Robust Test (robust-validation): Average loss: 22.2404, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.013888888888888888\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 24.4794, Accuracy: 7/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 44/50\n",
            "Train Epoch: 43 [00064/1600 (4%)]\t Loss: 0.104033\n",
            "Robust Test (robust-validation): Average loss: 22.5681, Robust Accuracy: 12/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 87.50%\n",
            "Test (clean-validation): Average loss: 24.7823, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 45/50\n",
            "Train Epoch: 44 [00064/1600 (4%)]\t Loss: 0.077971\n",
            "Robust Test (robust-validation): Average loss: 22.8139, Robust Accuracy: 15/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.013888888888888888\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 88.89%\n",
            "Test (clean-validation): Average loss: 24.4606, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 46/50\n",
            "Train Epoch: 45 [00064/1600 (4%)]\t Loss: 0.052440\n",
            "Robust Test (robust-validation): Average loss: 23.3707, Robust Accuracy: 14/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 90.00%\n",
            "Test (clean-validation): Average loss: 24.5852, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 47/50\n",
            "Train Epoch: 46 [00064/1600 (4%)]\t Loss: 0.041425\n",
            "Robust Test (robust-validation): Average loss: 23.6747, Robust Accuracy: 15/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 90.00%\n",
            "Test (clean-validation): Average loss: 24.8522, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 48/50\n",
            "Train Epoch: 47 [00064/1600 (4%)]\t Loss: 0.039693\n",
            "Robust Test (robust-validation): Average loss: 23.6104, Robust Accuracy: 17/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.016666666666666666\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 90.00%\n",
            "Test (clean-validation): Average loss: 25.3867, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 49/50\n",
            "Train Epoch: 48 [00064/1600 (4%)]\t Loss: 0.046116\n",
            "Robust Test (robust-validation): Average loss: 23.6989, Robust Accuracy: 15/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.016666666666666666\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 87.50%\n",
            "Test (clean-validation): Average loss: 25.3095, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 50/50\n",
            "Train Epoch: 49 [00064/1600 (4%)]\t Loss: 0.033622\n",
            "Robust Test (robust-validation): Average loss: 23.9563, Robust Accuracy: 15/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.013888888888888888\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 90.00%\n",
            "Test (clean-validation): Average loss: 25.3195, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Training finished.\n",
            "Best clean validation accuracy: 0.51%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td></td></tr><tr><td>accuracy_asian_male</td><td></td></tr><tr><td>accuracy_black_female</td><td></td></tr><tr><td>accuracy_black_male</td><td></td></tr><tr><td>accuracy_white_female</td><td></td></tr><tr><td>accuracy_white_male</td><td></td></tr><tr><td>attack_success_rate_asian_female</td><td></td></tr><tr><td>attack_success_rate_asian_male</td><td></td></tr><tr><td>attack_success_rate_black_female</td><td></td></tr><tr><td>attack_success_rate_black_male</td><td></td></tr><tr><td>attack_success_rate_white_female</td><td></td></tr><tr><td>attack_success_rate_white_male</td><td></td></tr><tr><td>clean_test_accuracy clean-validation</td><td></td></tr><tr><td>clean_test_loss clean-validation</td><td></td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td></td></tr><tr><td>robust_accuracy_asian_female</td><td></td></tr><tr><td>robust_accuracy_asian_male</td><td></td></tr><tr><td>robust_accuracy_black_female</td><td></td></tr><tr><td>robust_accuracy_black_male</td><td></td></tr><tr><td>robust_accuracy_robust-validation</td><td></td></tr><tr><td>robust_accuracy_white_female</td><td></td></tr><tr><td>robust_accuracy_white_male</td><td></td></tr><tr><td>robust_loss_robust-validation</td><td></td></tr><tr><td>train_loss DemogPairsDataset</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td>0</td></tr><tr><td>accuracy_asian_male</td><td>0</td></tr><tr><td>accuracy_black_female</td><td>0</td></tr><tr><td>accuracy_black_male</td><td>0</td></tr><tr><td>accuracy_white_female</td><td>0</td></tr><tr><td>accuracy_white_male</td><td>0</td></tr><tr><td>attack_success_rate_asian_female</td><td>0.01111</td></tr><tr><td>attack_success_rate_asian_male</td><td>0.00278</td></tr><tr><td>attack_success_rate_black_female</td><td>0.00556</td></tr><tr><td>attack_success_rate_black_male</td><td>0</td></tr><tr><td>attack_success_rate_white_female</td><td>0.00278</td></tr><tr><td>attack_success_rate_white_male</td><td>0.00278</td></tr><tr><td>clean_test_accuracy clean-validation</td><td>0.46296</td></tr><tr><td>clean_test_loss clean-validation</td><td>25.31947</td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td>90</td></tr><tr><td>robust_accuracy_asian_female</td><td>0.01111</td></tr><tr><td>robust_accuracy_asian_male</td><td>0.00278</td></tr><tr><td>robust_accuracy_black_female</td><td>0.00556</td></tr><tr><td>robust_accuracy_black_male</td><td>0.01389</td></tr><tr><td>robust_accuracy_robust-validation</td><td>0.69444</td></tr><tr><td>robust_accuracy_white_female</td><td>0</td></tr><tr><td>robust_accuracy_white_male</td><td>0.00833</td></tr><tr><td>robust_loss_robust-validation</td><td>23.95634</td></tr><tr><td>train_loss DemogPairsDataset</td><td>0.03362</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-asian-women-0.25</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/ja99dhnb' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/ja99dhnb</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_182227-ja99dhnb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_185008-why19o56</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/why19o56' target=\"_blank\">demogpairs-asian-women-0.5</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/why19o56' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/why19o56</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "Train Epoch: 0 [00064/1600 (4%)]\t Loss: 7.123359\n",
            "Robust Test (robust-validation): Average loss: 6.6758, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 162.9593, Accuracy: 4/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.19%\n",
            "\n",
            "Epoch 2/50\n",
            "Train Epoch: 1 [00064/1600 (4%)]\t Loss: 6.370084\n",
            "Robust Test (robust-validation): Average loss: 6.5222, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 61.7693, Accuracy: 1/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 3/50\n",
            "Train Epoch: 2 [00064/1600 (4%)]\t Loss: 6.346282\n",
            "Robust Test (robust-validation): Average loss: 6.5221, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.019444444444444445\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 226.3300, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 4/50\n",
            "Train Epoch: 3 [00064/1600 (4%)]\t Loss: 6.290153\n",
            "Robust Test (robust-validation): Average loss: 7.0035, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 89.2242, Accuracy: 4/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 5/50\n",
            "Train Epoch: 4 [00064/1600 (4%)]\t Loss: 6.174785\n",
            "Robust Test (robust-validation): Average loss: 7.1533, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 113.5401, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 6/50\n",
            "Train Epoch: 5 [00064/1600 (4%)]\t Loss: 6.160038\n",
            "Robust Test (robust-validation): Average loss: 7.9797, Robust Accuracy: 1/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 114.0300, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 7/50\n",
            "Train Epoch: 6 [00064/1600 (4%)]\t Loss: 6.126159\n",
            "Robust Test (robust-validation): Average loss: 9.4613, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 110.1007, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 8/50\n",
            "Train Epoch: 7 [00064/1600 (4%)]\t Loss: 5.840864\n",
            "Robust Test (robust-validation): Average loss: 12.3716, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 86.2080, Accuracy: 0/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 9/50\n",
            "Train Epoch: 8 [00064/1600 (4%)]\t Loss: 5.549954\n",
            "Robust Test (robust-validation): Average loss: 14.3509, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 56.9254, Accuracy: 1/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 10/50\n",
            "Train Epoch: 9 [00064/1600 (4%)]\t Loss: 5.761389\n",
            "Robust Test (robust-validation): Average loss: 15.4777, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 97.7709, Accuracy: 0/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 11/50\n",
            "Train Epoch: 10 [00064/1600 (4%)]\t Loss: 5.386624\n",
            "Robust Test (robust-validation): Average loss: 9.9004, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 24.3974, Accuracy: 0/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 12/50\n",
            "Train Epoch: 11 [00064/1600 (4%)]\t Loss: 5.342595\n",
            "Robust Test (robust-validation): Average loss: 11.2596, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 50.00%\n",
            "Test (clean-validation): Average loss: 21.2850, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.19%\n",
            "\n",
            "Epoch 13/50\n",
            "Train Epoch: 12 [00064/1600 (4%)]\t Loss: 5.075869\n",
            "Robust Test (robust-validation): Average loss: 12.0805, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 18.6776, Accuracy: 5/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.23%\n",
            "\n",
            "Epoch 14/50\n",
            "Train Epoch: 13 [00064/1600 (4%)]\t Loss: 4.680899\n",
            "Robust Test (robust-validation): Average loss: 16.1103, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 66.67%\n",
            "Test (clean-validation): Average loss: 25.7908, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.23%\n",
            "\n",
            "Epoch 15/50\n",
            "Train Epoch: 14 [00064/1600 (4%)]\t Loss: 4.822370\n",
            "Robust Test (robust-validation): Average loss: 16.4170, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 71.43%\n",
            "Test (clean-validation): Average loss: 22.0447, Accuracy: 7/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.32%\n",
            "\n",
            "Epoch 16/50\n",
            "Train Epoch: 15 [00064/1600 (4%)]\t Loss: 4.769924\n",
            "Robust Test (robust-validation): Average loss: 13.1024, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.019444444444444445\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 66.67%\n",
            "Test (clean-validation): Average loss: 17.9599, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.32%\n",
            "\n",
            "Epoch 17/50\n",
            "Train Epoch: 16 [00064/1600 (4%)]\t Loss: 4.520263\n",
            "Robust Test (robust-validation): Average loss: 12.2916, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 18.6699, Accuracy: 7/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.32%\n",
            "\n",
            "Epoch 18/50\n",
            "Train Epoch: 17 [00064/1600 (4%)]\t Loss: 4.284671\n",
            "Robust Test (robust-validation): Average loss: 12.1655, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 21.6419, Accuracy: 8/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.37%\n",
            "\n",
            "Epoch 19/50\n",
            "Train Epoch: 18 [00064/1600 (4%)]\t Loss: 4.716044\n",
            "Robust Test (robust-validation): Average loss: 13.1348, Robust Accuracy: 2/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 19.6118, Accuracy: 7/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.37%\n",
            "\n",
            "Epoch 20/50\n",
            "Train Epoch: 19 [00064/1600 (4%)]\t Loss: 4.347261\n",
            "Robust Test (robust-validation): Average loss: 15.6174, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 66.67%\n",
            "Test (clean-validation): Average loss: 15.3598, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.37%\n",
            "\n",
            "Epoch 21/50\n",
            "Train Epoch: 20 [00064/1600 (4%)]\t Loss: 3.853929\n",
            "Robust Test (robust-validation): Average loss: 12.9696, Robust Accuracy: 1/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 14.3246, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.37%\n",
            "\n",
            "Epoch 22/50\n",
            "Train Epoch: 21 [00064/1600 (4%)]\t Loss: 3.597383\n",
            "Robust Test (robust-validation): Average loss: 13.4613, Robust Accuracy: 2/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 83.33%\n",
            "Test (clean-validation): Average loss: 15.4075, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.37%\n",
            "\n",
            "Epoch 23/50\n",
            "Train Epoch: 22 [00064/1600 (4%)]\t Loss: 3.471404\n",
            "Robust Test (robust-validation): Average loss: 13.9733, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.011111111111111112\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.005555555555555556\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 90.91%\n",
            "Test (clean-validation): Average loss: 15.7006, Accuracy: 11/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.51%\n",
            "\n",
            "Epoch 24/50\n",
            "Train Epoch: 23 [00064/1600 (4%)]\t Loss: 2.939196\n",
            "Robust Test (robust-validation): Average loss: 15.0773, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 80.00%\n",
            "Test (clean-validation): Average loss: 16.7268, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 25/50\n",
            "Train Epoch: 24 [00064/1600 (4%)]\t Loss: 2.536772\n",
            "Robust Test (robust-validation): Average loss: 16.6598, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 85.71%\n",
            "Test (clean-validation): Average loss: 18.5655, Accuracy: 7/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 26/50\n",
            "Train Epoch: 25 [00064/1600 (4%)]\t Loss: 2.349311\n",
            "Robust Test (robust-validation): Average loss: 17.5946, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 57.14%\n",
            "Test (clean-validation): Average loss: 18.7792, Accuracy: 7/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.51%\n",
            "\n",
            "Epoch 27/50\n",
            "Train Epoch: 26 [00064/1600 (4%)]\t Loss: 2.277719\n",
            "Robust Test (robust-validation): Average loss: 17.8922, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.005555555555555556\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.011111111111111112\n",
            "Overall Attack success rate (robust-validation): 83.33%\n",
            "Test (clean-validation): Average loss: 16.1748, Accuracy: 12/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.56%\n",
            "\n",
            "Epoch 28/50\n",
            "Train Epoch: 27 [00064/1600 (4%)]\t Loss: 1.997440\n",
            "Robust Test (robust-validation): Average loss: 19.1531, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.008333333333333333\n",
            "Robust accuracy for black_female: 0.011111111111111112\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.008333333333333333\n",
            "Overall Attack success rate (robust-validation): 81.25%\n",
            "Test (clean-validation): Average loss: 17.1791, Accuracy: 16/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.74%\n",
            "\n",
            "Epoch 29/50\n",
            "Train Epoch: 28 [00064/1600 (4%)]\t Loss: 1.638481\n",
            "Robust Test (robust-validation): Average loss: 19.5157, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.005555555555555556\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 81.82%\n",
            "Test (clean-validation): Average loss: 18.7348, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 30/50\n",
            "Train Epoch: 29 [00064/1600 (4%)]\t Loss: 1.538578\n",
            "Robust Test (robust-validation): Average loss: 18.9013, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 80.00%\n",
            "Test (clean-validation): Average loss: 19.2316, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 31/50\n",
            "Train Epoch: 30 [00064/1600 (4%)]\t Loss: 1.872304\n",
            "Robust Test (robust-validation): Average loss: 17.8191, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 90.00%\n",
            "Test (clean-validation): Average loss: 18.9851, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 32/50\n",
            "Train Epoch: 31 [00064/1600 (4%)]\t Loss: 1.737820\n",
            "Robust Test (robust-validation): Average loss: 18.6841, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 20.0131, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 33/50\n",
            "Train Epoch: 32 [00064/1600 (4%)]\t Loss: 1.654260\n",
            "Robust Test (robust-validation): Average loss: 20.1221, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 90.91%\n",
            "Test (clean-validation): Average loss: 20.3461, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 34/50\n",
            "Train Epoch: 33 [00064/1600 (4%)]\t Loss: 1.186933\n",
            "Robust Test (robust-validation): Average loss: 20.6359, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.005555555555555556\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.011111111111111112\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.008333333333333333\n",
            "Overall Attack success rate (robust-validation): 93.75%\n",
            "Test (clean-validation): Average loss: 18.6364, Accuracy: 16/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 35/50\n",
            "Train Epoch: 34 [00064/1600 (4%)]\t Loss: 1.067466\n",
            "Robust Test (robust-validation): Average loss: 19.9938, Robust Accuracy: 11/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.005555555555555556\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 69.23%\n",
            "Test (clean-validation): Average loss: 19.7381, Accuracy: 13/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 36/50\n",
            "Train Epoch: 35 [00064/1600 (4%)]\t Loss: 1.211289\n",
            "Robust Test (robust-validation): Average loss: 20.2005, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.005555555555555556\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 76.92%\n",
            "Test (clean-validation): Average loss: 19.5895, Accuracy: 13/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 37/50\n",
            "Train Epoch: 36 [00064/1600 (4%)]\t Loss: 0.997131\n",
            "Robust Test (robust-validation): Average loss: 20.3282, Robust Accuracy: 10/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 72.73%\n",
            "Test (clean-validation): Average loss: 21.0092, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 38/50\n",
            "Train Epoch: 37 [00064/1600 (4%)]\t Loss: 0.902498\n",
            "Robust Test (robust-validation): Average loss: 20.4002, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 80.00%\n",
            "Test (clean-validation): Average loss: 21.0023, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 39/50\n",
            "Train Epoch: 38 [00064/1600 (4%)]\t Loss: 0.614493\n",
            "Robust Test (robust-validation): Average loss: 20.8752, Robust Accuracy: 10/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 75.00%\n",
            "Test (clean-validation): Average loss: 21.9151, Accuracy: 12/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 40/50\n",
            "Train Epoch: 39 [00064/1600 (4%)]\t Loss: 0.559692\n",
            "Robust Test (robust-validation): Average loss: 21.8079, Robust Accuracy: 15/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.011111111111111112\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.005555555555555556\n",
            "Overall Attack success rate (robust-validation): 53.33%\n",
            "Test (clean-validation): Average loss: 22.2207, Accuracy: 15/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 41/50\n",
            "Train Epoch: 40 [00064/1600 (4%)]\t Loss: 0.483012\n",
            "Robust Test (robust-validation): Average loss: 21.6653, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 73.33%\n",
            "Test (clean-validation): Average loss: 22.2765, Accuracy: 15/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 42/50\n",
            "Train Epoch: 41 [00064/1600 (4%)]\t Loss: 0.391139\n",
            "Robust Test (robust-validation): Average loss: 21.7758, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 90.00%\n",
            "Test (clean-validation): Average loss: 22.0747, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 43/50\n",
            "Train Epoch: 42 [00064/1600 (4%)]\t Loss: 0.286148\n",
            "Robust Test (robust-validation): Average loss: 22.0339, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 66.67%\n",
            "Test (clean-validation): Average loss: 22.2134, Accuracy: 15/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 44/50\n",
            "Train Epoch: 43 [00064/1600 (4%)]\t Loss: 0.287063\n",
            "Robust Test (robust-validation): Average loss: 22.2619, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.005555555555555556\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 73.33%\n",
            "Test (clean-validation): Average loss: 23.1972, Accuracy: 15/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 45/50\n",
            "Train Epoch: 44 [00064/1600 (4%)]\t Loss: 0.179417\n",
            "Robust Test (robust-validation): Average loss: 22.9879, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 60.00%\n",
            "Test (clean-validation): Average loss: 23.2556, Accuracy: 15/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 46/50\n",
            "Train Epoch: 45 [00064/1600 (4%)]\t Loss: 0.257992\n",
            "Robust Test (robust-validation): Average loss: 22.8015, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.008333333333333333\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 64.29%\n",
            "Test (clean-validation): Average loss: 23.1818, Accuracy: 14/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 47/50\n",
            "Train Epoch: 46 [00064/1600 (4%)]\t Loss: 0.159572\n",
            "Robust Test (robust-validation): Average loss: 22.9547, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 64.29%\n",
            "Test (clean-validation): Average loss: 22.9050, Accuracy: 14/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 48/50\n",
            "Train Epoch: 47 [00064/1600 (4%)]\t Loss: 0.131960\n",
            "Robust Test (robust-validation): Average loss: 23.4789, Robust Accuracy: 12/2160 (1%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.016666666666666666\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 68.75%\n",
            "Test (clean-validation): Average loss: 23.3866, Accuracy: 16/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 49/50\n",
            "Train Epoch: 48 [00064/1600 (4%)]\t Loss: 0.117896\n",
            "Robust Test (robust-validation): Average loss: 23.9082, Robust Accuracy: 10/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 80.00%\n",
            "Test (clean-validation): Average loss: 23.8662, Accuracy: 15/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Epoch 50/50\n",
            "Train Epoch: 49 [00064/1600 (4%)]\t Loss: 0.146606\n",
            "Robust Test (robust-validation): Average loss: 23.4771, Robust Accuracy: 11/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 71.43%\n",
            "Test (clean-validation): Average loss: 23.2497, Accuracy: 14/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.74%\n",
            "\n",
            "Training finished.\n",
            "Best clean validation accuracy: 0.74%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td></td></tr><tr><td>accuracy_asian_male</td><td></td></tr><tr><td>accuracy_black_female</td><td></td></tr><tr><td>accuracy_black_male</td><td></td></tr><tr><td>accuracy_white_female</td><td></td></tr><tr><td>accuracy_white_male</td><td></td></tr><tr><td>attack_success_rate_asian_female</td><td></td></tr><tr><td>attack_success_rate_asian_male</td><td></td></tr><tr><td>attack_success_rate_black_female</td><td></td></tr><tr><td>attack_success_rate_black_male</td><td></td></tr><tr><td>attack_success_rate_white_female</td><td></td></tr><tr><td>attack_success_rate_white_male</td><td></td></tr><tr><td>clean_test_accuracy clean-validation</td><td></td></tr><tr><td>clean_test_loss clean-validation</td><td></td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td></td></tr><tr><td>robust_accuracy_asian_female</td><td></td></tr><tr><td>robust_accuracy_asian_male</td><td></td></tr><tr><td>robust_accuracy_black_female</td><td></td></tr><tr><td>robust_accuracy_black_male</td><td></td></tr><tr><td>robust_accuracy_robust-validation</td><td></td></tr><tr><td>robust_accuracy_white_female</td><td></td></tr><tr><td>robust_accuracy_white_male</td><td></td></tr><tr><td>robust_loss_robust-validation</td><td></td></tr><tr><td>train_loss DemogPairsDataset</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy_asian_female</td><td>0</td></tr><tr><td>accuracy_asian_male</td><td>0</td></tr><tr><td>accuracy_black_female</td><td>0</td></tr><tr><td>accuracy_black_male</td><td>0</td></tr><tr><td>accuracy_white_female</td><td>0</td></tr><tr><td>accuracy_white_male</td><td>0</td></tr><tr><td>attack_success_rate_asian_female</td><td>0.01389</td></tr><tr><td>attack_success_rate_asian_male</td><td>0.00556</td></tr><tr><td>attack_success_rate_black_female</td><td>0.00556</td></tr><tr><td>attack_success_rate_black_male</td><td>0</td></tr><tr><td>attack_success_rate_white_female</td><td>0.00278</td></tr><tr><td>attack_success_rate_white_male</td><td>0</td></tr><tr><td>clean_test_accuracy clean-validation</td><td>0.64815</td></tr><tr><td>clean_test_loss clean-validation</td><td>23.24966</td></tr><tr><td>overall_attack_success_rate_robust-validation</td><td>71.42857</td></tr><tr><td>robust_accuracy_asian_female</td><td>0.00833</td></tr><tr><td>robust_accuracy_asian_male</td><td>0.00278</td></tr><tr><td>robust_accuracy_black_female</td><td>0.00556</td></tr><tr><td>robust_accuracy_black_male</td><td>0.00556</td></tr><tr><td>robust_accuracy_robust-validation</td><td>0.50926</td></tr><tr><td>robust_accuracy_white_female</td><td>0</td></tr><tr><td>robust_accuracy_white_male</td><td>0.00833</td></tr><tr><td>robust_loss_robust-validation</td><td>23.47711</td></tr><tr><td>train_loss DemogPairsDataset</td><td>0.14661</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">demogpairs-asian-women-0.5</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/why19o56' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/why19o56</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250601_185008-why19o56/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250601_191750-hzw681bl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/hzw681bl' target=\"_blank\">demogpairs-asian-women-0.75</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/hzw681bl' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/hzw681bl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "Train Epoch: 0 [00064/1600 (4%)]\t Loss: 7.086213\n",
            "Robust Test (robust-validation): Average loss: 17.6768, Robust Accuracy: 1/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 601.0899, Accuracy: 0/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.00%\n",
            "\n",
            "Epoch 2/50\n",
            "Train Epoch: 1 [00064/1600 (4%)]\t Loss: 6.431274\n",
            "Robust Test (robust-validation): Average loss: 6.5770, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 142.1895, Accuracy: 3/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.14%\n",
            "\n",
            "Epoch 3/50\n",
            "Train Epoch: 2 [00064/1600 (4%)]\t Loss: 6.291145\n",
            "Robust Test (robust-validation): Average loss: 7.3713, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 66.4250, Accuracy: 1/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.14%\n",
            "\n",
            "Epoch 4/50\n",
            "Train Epoch: 3 [00064/1600 (4%)]\t Loss: 6.118584\n",
            "Robust Test (robust-validation): Average loss: 7.6904, Robust Accuracy: 3/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 45.7153, Accuracy: 0/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.14%\n",
            "\n",
            "Epoch 5/50\n",
            "Train Epoch: 4 [00064/1600 (4%)]\t Loss: 5.800523\n",
            "Robust Test (robust-validation): Average loss: 8.7340, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.019444444444444445\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 51.0440, Accuracy: 1/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.14%\n",
            "\n",
            "Epoch 6/50\n",
            "Train Epoch: 5 [00064/1600 (4%)]\t Loss: 5.922818\n",
            "Robust Test (robust-validation): Average loss: 10.9415, Robust Accuracy: 2/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 51.5840, Accuracy: 1/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.14%\n",
            "\n",
            "Epoch 7/50\n",
            "Train Epoch: 6 [00064/1600 (4%)]\t Loss: 5.623131\n",
            "Robust Test (robust-validation): Average loss: 11.2770, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.016666666666666666\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 66.67%\n",
            "Test (clean-validation): Average loss: 39.0584, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.14%\n",
            "\n",
            "Epoch 8/50\n",
            "Train Epoch: 7 [00064/1600 (4%)]\t Loss: 5.382514\n",
            "Robust Test (robust-validation): Average loss: 10.5370, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.0\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 50.00%\n",
            "Test (clean-validation): Average loss: 38.7925, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.14%\n",
            "\n",
            "Epoch 9/50\n",
            "Train Epoch: 8 [00064/1600 (4%)]\t Loss: 5.166572\n",
            "Robust Test (robust-validation): Average loss: 11.3887, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 0.00%\n",
            "Test (clean-validation): Average loss: 43.4248, Accuracy: 0/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.14%\n",
            "\n",
            "Epoch 10/50\n",
            "Train Epoch: 9 [00064/1600 (4%)]\t Loss: 5.111467\n",
            "Robust Test (robust-validation): Average loss: 10.0876, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.008333333333333333\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 32.4735, Accuracy: 1/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.14%\n",
            "\n",
            "Epoch 11/50\n",
            "Train Epoch: 10 [00064/1600 (4%)]\t Loss: 5.076672\n",
            "Robust Test (robust-validation): Average loss: 10.1944, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 27.3878, Accuracy: 4/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.19%\n",
            "\n",
            "Epoch 12/50\n",
            "Train Epoch: 11 [00064/1600 (4%)]\t Loss: 4.568967\n",
            "Robust Test (robust-validation): Average loss: 11.3832, Robust Accuracy: 4/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.005555555555555556\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 85.71%\n",
            "Test (clean-validation): Average loss: 27.0103, Accuracy: 7/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.32%\n",
            "\n",
            "Epoch 13/50\n",
            "Train Epoch: 12 [00064/1600 (4%)]\t Loss: 4.579034\n",
            "Robust Test (robust-validation): Average loss: 11.5505, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 17.5938, Accuracy: 4/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.32%\n",
            "\n",
            "Epoch 14/50\n",
            "Train Epoch: 13 [00064/1600 (4%)]\t Loss: 4.334668\n",
            "Robust Test (robust-validation): Average loss: 12.1528, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.025\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 16.2857, Accuracy: 10/2160 (0%)\n",
            "Saving best model with clean validation accuracy: 0.46%\n",
            "\n",
            "Epoch 15/50\n",
            "Train Epoch: 14 [00064/1600 (4%)]\t Loss: 3.748591\n",
            "Robust Test (robust-validation): Average loss: 12.0296, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 17.9495, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 16/50\n",
            "Train Epoch: 15 [00064/1600 (4%)]\t Loss: 3.740175\n",
            "Robust Test (robust-validation): Average loss: 13.3678, Robust Accuracy: 7/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 20.6793, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 17/50\n",
            "Train Epoch: 16 [00064/1600 (4%)]\t Loss: 3.350631\n",
            "Robust Test (robust-validation): Average loss: 13.3495, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.005555555555555556\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 19.3241, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.46%\n",
            "\n",
            "Epoch 18/50\n",
            "Train Epoch: 17 [00064/1600 (4%)]\t Loss: 3.131065\n",
            "Robust Test (robust-validation): Average loss: 14.9159, Robust Accuracy: 6/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.0\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.019444444444444445\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 91.67%\n",
            "Test (clean-validation): Average loss: 19.9183, Accuracy: 12/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.56%\n",
            "\n",
            "Epoch 19/50\n",
            "Train Epoch: 18 [00064/1600 (4%)]\t Loss: 2.948740\n",
            "Robust Test (robust-validation): Average loss: 15.6346, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 83.33%\n",
            "Test (clean-validation): Average loss: 21.1309, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 20/50\n",
            "Train Epoch: 19 [00064/1600 (4%)]\t Loss: 2.670842\n",
            "Robust Test (robust-validation): Average loss: 17.1757, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.0\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.002777777777777778\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 22.7714, Accuracy: 2/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 21/50\n",
            "Train Epoch: 20 [00064/1600 (4%)]\t Loss: 2.481567\n",
            "Robust Test (robust-validation): Average loss: 16.6740, Robust Accuracy: 5/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.0\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.005555555555555556\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 20.8764, Accuracy: 3/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 22/50\n",
            "Train Epoch: 21 [00064/1600 (4%)]\t Loss: 2.510987\n",
            "Robust Test (robust-validation): Average loss: 18.2870, Robust Accuracy: 8/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.002777777777777778\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.002777777777777778\n",
            "Robust accuracy for black_male: 0.002777777777777778\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 75.00%\n",
            "Test (clean-validation): Average loss: 24.5992, Accuracy: 4/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 23/50\n",
            "Train Epoch: 22 [00064/1600 (4%)]\t Loss: 2.130809\n",
            "Robust Test (robust-validation): Average loss: 15.5620, Robust Accuracy: 12/2160 (1%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.016666666666666666\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 80.00%\n",
            "Test (clean-validation): Average loss: 18.0406, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 24/50\n",
            "Train Epoch: 23 [00064/1600 (4%)]\t Loss: 1.998781\n",
            "Robust Test (robust-validation): Average loss: 17.1069, Robust Accuracy: 10/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 80.00%\n",
            "Test (clean-validation): Average loss: 21.3615, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 25/50\n",
            "Train Epoch: 24 [00064/1600 (4%)]\t Loss: 1.583727\n",
            "Robust Test (robust-validation): Average loss: 17.5765, Robust Accuracy: 10/2160 (0%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.0\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 87.50%\n",
            "Test (clean-validation): Average loss: 19.4290, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 26/50\n",
            "Train Epoch: 25 [00064/1600 (4%)]\t Loss: 1.404989\n",
            "Robust Test (robust-validation): Average loss: 17.9699, Robust Accuracy: 10/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.0\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.005555555555555556\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 20.5427, Accuracy: 5/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 27/50\n",
            "Train Epoch: 26 [00064/1600 (4%)]\t Loss: 1.026871\n",
            "Robust Test (robust-validation): Average loss: 17.1936, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.005555555555555556\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 22.0422, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 28/50\n",
            "Train Epoch: 27 [00064/1600 (4%)]\t Loss: 1.179817\n",
            "Robust Test (robust-validation): Average loss: 17.7634, Robust Accuracy: 9/2160 (0%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.0\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.0\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.002777777777777778\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 23.3343, Accuracy: 6/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 29/50\n",
            "Train Epoch: 28 [00064/1600 (4%)]\t Loss: 1.145089\n",
            "Robust Test (robust-validation): Average loss: 18.4912, Robust Accuracy: 17/2160 (1%)\n",
            "Robust accuracy for white_female: 0.005555555555555556\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 88.89%\n",
            "Test (clean-validation): Average loss: 23.8215, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.56%\n",
            "\n",
            "Epoch 30/50\n",
            "Train Epoch: 29 [00064/1600 (4%)]\t Loss: 0.726793\n",
            "Robust Test (robust-validation): Average loss: 18.5354, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.019444444444444445\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.013888888888888888\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 92.31%\n",
            "Test (clean-validation): Average loss: 23.1744, Accuracy: 13/2160 (1%)\n",
            "Saving best model with clean validation accuracy: 0.60%\n",
            "\n",
            "Epoch 31/50\n",
            "Train Epoch: 30 [00064/1600 (4%)]\t Loss: 0.650959\n",
            "Robust Test (robust-validation): Average loss: 19.4399, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 90.00%\n",
            "Test (clean-validation): Average loss: 24.5582, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 32/50\n",
            "Train Epoch: 31 [00064/1600 (4%)]\t Loss: 0.515191\n",
            "Robust Test (robust-validation): Average loss: 20.2039, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.002777777777777778\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.002777777777777778\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.002777777777777778\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.016666666666666666\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 25.2971, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 33/50\n",
            "Train Epoch: 32 [00064/1600 (4%)]\t Loss: 0.394544\n",
            "Robust Test (robust-validation): Average loss: 20.6203, Robust Accuracy: 12/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 91.67%\n",
            "Test (clean-validation): Average loss: 25.8929, Accuracy: 12/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 34/50\n",
            "Train Epoch: 33 [00064/1600 (4%)]\t Loss: 0.311613\n",
            "Robust Test (robust-validation): Average loss: 20.3540, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 24.8816, Accuracy: 9/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 35/50\n",
            "Train Epoch: 34 [00064/1600 (4%)]\t Loss: 0.242562\n",
            "Robust Test (robust-validation): Average loss: 20.9027, Robust Accuracy: 13/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 26.1018, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 36/50\n",
            "Train Epoch: 35 [00064/1600 (4%)]\t Loss: 0.227939\n",
            "Robust Test (robust-validation): Average loss: 21.4774, Robust Accuracy: 17/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.013888888888888888\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 26.0661, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 37/50\n",
            "Train Epoch: 36 [00064/1600 (4%)]\t Loss: 0.157697\n",
            "Robust Test (robust-validation): Average loss: 21.8872, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 26.1754, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 38/50\n",
            "Train Epoch: 37 [00064/1600 (4%)]\t Loss: 0.122888\n",
            "Robust Test (robust-validation): Average loss: 22.1098, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.0\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.008333333333333333\n",
            "Robust accuracy for asian_female: 0.016666666666666666\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 25.7081, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 39/50\n",
            "Train Epoch: 38 [00064/1600 (4%)]\t Loss: 0.144383\n",
            "Robust Test (robust-validation): Average loss: 22.3667, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.016666666666666666\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 26.9302, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 40/50\n",
            "Train Epoch: 39 [00064/1600 (4%)]\t Loss: 0.141951\n",
            "Robust Test (robust-validation): Average loss: 22.5368, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.0\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.008333333333333333\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 27.0294, Accuracy: 8/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 41/50\n",
            "Train Epoch: 40 [00064/1600 (4%)]\t Loss: 0.119847\n",
            "Robust Test (robust-validation): Average loss: 22.7913, Robust Accuracy: 14/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.002777777777777778\n",
            "Robust accuracy for asian_male: 0.005555555555555556\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.008333333333333333\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 26.2942, Accuracy: 10/2160 (0%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 42/50\n",
            "Train Epoch: 41 [00064/1600 (4%)]\t Loss: 0.051228\n",
            "Robust Test (robust-validation): Average loss: 22.6880, Robust Accuracy: 17/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.016666666666666666\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 26.0741, Accuracy: 12/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 43/50\n",
            "Train Epoch: 42 [00064/1600 (4%)]\t Loss: 0.039541\n",
            "Robust Test (robust-validation): Average loss: 22.9440, Robust Accuracy: 15/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 26.4336, Accuracy: 12/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 44/50\n",
            "Train Epoch: 43 [00064/1600 (4%)]\t Loss: 0.027561\n",
            "Robust Test (robust-validation): Average loss: 23.1663, Robust Accuracy: 14/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.005555555555555556\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 26.8600, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 45/50\n",
            "Train Epoch: 44 [00064/1600 (4%)]\t Loss: 0.019764\n",
            "Robust Test (robust-validation): Average loss: 23.1835, Robust Accuracy: 15/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.008333333333333333\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.013888888888888888\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.008333333333333333\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 26.8022, Accuracy: 12/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 46/50\n",
            "Train Epoch: 45 [00064/1600 (4%)]\t Loss: 0.018560\n",
            "Robust Test (robust-validation): Average loss: 23.5911, Robust Accuracy: 14/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.002777777777777778\n",
            "Robust accuracy for white_male: 0.008333333333333333\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.011111111111111112\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 27.4006, Accuracy: 11/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 47/50\n",
            "Train Epoch: 46 [00064/1600 (4%)]\t Loss: 0.016559\n",
            "Robust Test (robust-validation): Average loss: 23.7575, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.005555555555555556\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.013888888888888888\n",
            "Robust accuracy for black_male: 0.013888888888888888\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 100.00%\n",
            "Test (clean-validation): Average loss: 27.1814, Accuracy: 12/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 48/50\n",
            "Train Epoch: 47 [00064/1600 (4%)]\t Loss: 0.014436\n",
            "Robust Test (robust-validation): Average loss: 24.0616, Robust Accuracy: 16/2160 (1%)\n",
            "Robust accuracy for white_female: 0.0\n",
            "Attack success rate for white_female: 0.008333333333333333\n",
            "Robust accuracy for asian_male: 0.002777777777777778\n",
            "Attack success rate for asian_male: 0.005555555555555556\n",
            "Robust accuracy for white_male: 0.011111111111111112\n",
            "Attack success rate for white_male: 0.002777777777777778\n",
            "Robust accuracy for black_female: 0.005555555555555556\n",
            "Attack success rate for black_female: 0.005555555555555556\n",
            "Robust accuracy for asian_female: 0.011111111111111112\n",
            "Attack success rate for asian_female: 0.011111111111111112\n",
            "Robust accuracy for black_male: 0.013888888888888888\n",
            "Attack success rate for black_male: 0.0\n",
            "Overall Attack success rate (robust-validation): 92.31%\n",
            "Test (clean-validation): Average loss: 27.5234, Accuracy: 13/2160 (1%)\n",
            "Validation accuracy did not improve. Best clean accuracy so far: 0.60%\n",
            "\n",
            "Epoch 49/50\n",
            "Train Epoch: 48 [00064/1600 (4%)]\t Loss: 0.012038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "08SLjiH4uLdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}