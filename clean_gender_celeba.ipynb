{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taweener11/darkSideUnmasked/blob/main/clean_gender_celeba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYi8w2zpnKKE",
        "outputId": "f6b0ac63-10bf-47c1-bb13-f54ee2811b12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cores = os.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrb_gzkT-CCv",
        "outputId": "95aa1463-17cd-4b1c-851f-c1f871368186"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8TM7bpCk3kNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title shell pipeline for unzipping! this needs to run every time\n",
        "\n",
        "!unzip -q \"/content/drive/My Drive/Datasets/celeba/img_align_celeba.zip\" -d \"/content/celeba/\""
      ],
      "metadata": {
        "id": "SS8uNZYunxqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content' # setting it to the local environment"
      ],
      "metadata": {
        "id": "4uA1b3RswNS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "cn679gCpx6pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a transform that is smaller per suggestion of rasmus\n",
        "\n",
        "image_size = 64\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                          std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "oLYdzNAHyveC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfering files from gdrive to here so that they would work without us uploading manually all the time\n",
        "# import module\n",
        "import shutil\n",
        "\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/identity_CelebA.txt', '/content/celeba/identity_CelebA.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_attr_celeba.txt', '/content/celeba/list_attr_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_bbox_celeba.txt', '/content/celeba/list_bbox_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_landmarks_align_celeba.txt', '/content/celeba/list_landmarks_align_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_eval_partition.txt', '/content/celeba/list_eval_partition.txt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6CEeXlqPztTx",
        "outputId": "6665c925-3901-4837-ad68-e4887f95c375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/celeba/list_eval_partition.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CelebA\n",
        "\n",
        "\n",
        "# it creates a folder on the go!\n",
        "\n",
        "try:\n",
        "    dataset = CelebA(\n",
        "        root='/content',\n",
        "        split='train',\n",
        "        target_type='attr',\n",
        "        transform=transform,\n",
        "        download=False # this works now!!!! its just important that it is in the root folder\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"CelebA error:\", e)"
      ],
      "metadata": {
        "id": "iyQji5uMx2yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = '/content/celeba'\n",
        "\n",
        "print(\"Root contents:\", os.listdir(data_dir))\n",
        "print(\"Images folder exists:\", os.path.isdir(os.path.join(data_dir, 'img_align_celeba')))\n",
        "print(\"Sample images:\", os.listdir(os.path.join(data_dir, 'img_align_celeba'))[:3])\n",
        "print(\"Has attribute file:\", os.path.isfile(os.path.join(data_dir, 'list_attr_celeba.txt')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jz1k1xJybHu",
        "outputId": "82549f4c-0c60-48db-b5a1-1f89825a4510",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root contents: ['identity_CelebA.txt', 'list_bbox_celeba.txt', 'list_attr_celeba.txt', 'list_eval_partition.txt', 'img_align_celeba', 'list_landmarks_align_celeba.txt']\n",
            "Images folder exists: True\n",
            "Sample images: ['085474.jpg', '129511.jpg', '100524.jpg']\n",
            "Has attribute file: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check 2 & the moment of truth!!\n",
        "\n",
        "# adding a dataloader and a basic model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "tw8aIHtsyj0Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title pipeline for wandb\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "id": "5WRIPY5ESbcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting the training data, different distributions\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "test_dataset = CelebA(\n",
        "    root='/content',\n",
        "    split='test',\n",
        "    target_type='attr',\n",
        "    transform=transform,\n",
        "    download=False # i set it to true in case there is some secret metadata?? it is looking for\n",
        ")\n"
      ],
      "metadata": {
        "id": "ABpA6NKeQYRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Get the identity information from the training dataset\n",
        "identity_labels = dataset.identity\n",
        "# Convert to a pandas Series for easier counting\n",
        "identity_series = pd.Series(identity_labels.squeeze().numpy())\n",
        "identity_counts = identity_series.value_counts()\n",
        "top_1000_identities = identity_counts.nlargest(1000)\n",
        "# Get the indices corresponding to the top 1000 identities\n",
        "top_1000_indices = identity_series[identity_series.isin(top_1000_identities.index)].index\n",
        "# Create a subset of the dataset containing only the top 1000 identities\n",
        "dataset_top_1000 = Subset(dataset, top_1000_indices)\n",
        "\n",
        "\n",
        "min_samples = top_1000_identities.min()\n",
        "max_samples = top_1000_identities.max()\n",
        "\n",
        "print(f\"Minimum samples per identity: {min_samples}\")\n",
        "print(f\"Maximum samples per identity: {max_samples}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIYrBM44Fd_0",
        "outputId": "40ed9c1b-c354-4894-c62d-84c7c45187bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum samples per identity: 30\n",
            "Maximum samples per identity: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "gender_labels_test_subset = []\n",
        "for i in top_1000_indices:\n",
        "  # Note: As discussed before, using training indices on the test dataset\n",
        "  # might lead to issues or misalignment. Assuming this is intended for now.\n",
        "  if i < len(test_dataset):\n",
        "    gender_labels_test_subset.append(test_dataset.attr[i, male_idx])\n",
        "\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "gender_labels_test_subset_np = np.array(gender_labels_test_subset)\n",
        "\n",
        "\n",
        "# Now use np.where on the NumPy array\n",
        "# This is the part that fixes the DeprecationWarning\n",
        "female_test_subset_indices = np.where(gender_labels_test_subset_np == 0)[0]\n",
        "male_test_subset_indices   = np.where(gender_labels_test_subset_np ==  1)[0]\n",
        "\n",
        "\n",
        "print(len(female_test_subset_indices))\n",
        "print(len(male_test_subset_indices))\n",
        "\n",
        "\n",
        "N_test = min(len(female_test_subset_indices), len(male_test_subset_indices))\n",
        "\n",
        "rng_test = np.random.default_rng(seed=42)\n",
        "shuffled_female_test_subset_indices = np.copy(female_test_subset_indices)\n",
        "shuffled_male_test_subset_indices   = np.copy(male_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_female_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_male_test_subset_indices)\n",
        "\n",
        "\n",
        "test_subsets = {}\n",
        "\n",
        "# Create training subsets\n",
        "test_subsets_f = {}\n",
        "test_subsets_m = {}\n",
        "# even split for all examples. we can change this later but we want to be able to generalize... we want there to be the same number of examples for men and women and for these to be in the same set...\n",
        "# we will put this to the loop.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkTrbz2Az2Zy",
        "outputId": "a0414b7c-2c99-4490-fb46-9420be93e9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300\n",
            "1510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "# choose smallest n\n",
        "# proportions = [0, 0.1, 0.25, 0.5, 0.75, 1.0] # changed this bc it doesn't make sense\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "\n",
        "male_idx_train = dataset.attr_names.index('Male')\n",
        "gender_labels_train_subset = dataset.attr[top_1000_indices, male_idx_train] # gender from training dataset\n",
        "female_train_subset_indices = np.where(gender_labels_train_subset == 0)[0]\n",
        "male_train_subset_indices   = np.where(gender_labels_train_subset ==  1)[0]\n",
        "\n",
        "N_train = min(len(female_train_subset_indices), len(male_train_subset_indices))\n",
        "\n",
        "rng_train = np.random.default_rng(seed=42)\n",
        "shuffled_female_train_subset_indices = np.copy(female_train_subset_indices)\n",
        "shuffled_male_train_subset_indices   = np.copy(male_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_female_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_male_train_subset_indices)\n",
        "\n",
        "\n",
        "# training subsets\n",
        "train_subsets = {}\n",
        "for p in proportions:\n",
        "    num_females_train = int(N_train * p)\n",
        "    num_males_train = N_train - num_females_train\n",
        "\n",
        "    q = min(p, 1-p)\n",
        "    num_females_test = int(N_test * q) # even split for testing\n",
        "    num_males_test = num_females_test\n",
        "\n",
        "    chosen_female_train = shuffled_female_train_subset_indices[:num_females_train] if num_females_train > 0 else np.array([], dtype=int)\n",
        "    chosen_male_train   = shuffled_male_train_subset_indices[:num_males_train]   if num_males_train > 0   else np.array([], dtype=int)\n",
        "\n",
        "    chosen_female_test = shuffled_female_test_subset_indices[:num_females_test]\n",
        "    chosen_male_test   = shuffled_male_test_subset_indices[:num_males_test]\n",
        "\n",
        "    # these indices are relative to the 'dataset_top_1000' subset,\n",
        "    # so we need to map them back to the original 'dataset' indices if Subset requires it.\n",
        "    # since top_1000_indices is the mapping, we can directly use that:\n",
        "    original_indices_train = np.concatenate([\n",
        "        top_1000_indices[chosen_female_train],\n",
        "        top_1000_indices[chosen_male_train]\n",
        "    ]).astype(int)\n",
        "    rng_train.shuffle(original_indices_train)\n",
        "    train_subsets[p] = Subset(dataset, original_indices_train)\n",
        "    test_subsets_f[p] = Subset(test_dataset, chosen_female_test)\n",
        "    test_subsets_m[p] = Subset(test_dataset, chosen_male_test)\n",
        "\n",
        "\n",
        "\n",
        "# Verification as before\n",
        "for p in proportions:\n",
        "    # Verification for the training subset\n",
        "    indices_train = train_subsets[p].indices\n",
        "    # Need to get genders for these original training indices from the *full* training dataset\n",
        "    genders_train = dataset.attr[indices_train, male_idx_train]\n",
        "    percent_female_train = (genders_train == 0).sum()/len(indices_train) if len(indices_train) > 0 else 0\n",
        "    print(f\"Train Subset (Prop {int(p*100)}%): Target {int(p*100)}% -- Actual {percent_female_train*100:.2f}% females, {(genders_train == 0).sum()} samples\")\n",
        "\n",
        "\n",
        "    number_female_test = len(test_subsets_f[p].indices)\n",
        "    number_male_test = len(test_subsets_m[p].indices)\n",
        "    print(f\"Number of female test samples: {number_female_test}\")\n",
        "    print(f\"Number of male test samples: {number_male_test}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2x4LIJezqg1",
        "outputId": "8e36292a-2e94-4e77-f2bf-49d16a8d391e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Subset (Prop 25%): Target 25% -- Actual 24.99% females, 2480 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n",
            "Train Subset (Prop 50%): Target 50% -- Actual 50.00% females, 4961 samples\n",
            "Number of female test samples: 755\n",
            "Number of male test samples: 755\n",
            "Train Subset (Prop 75%): Target 75% -- Actual 74.99% females, 7441 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# we know all classes have around 30 examples each\n",
        "# this could give us a split of 10/30, 20/20, 30/10\n",
        "# for the test examples we dont care bc all the class indices are going to be there anyways\n",
        "proportions = [0.1, 0.5, 0.9]\n",
        "\n",
        "# Get the index for the 'Male' attribute in the dataset\n",
        "male_idx_train = dataset.attr_names.index('Male')\n",
        "male_idx_test = test_dataset.attr_names.index('Male') # Assuming the index is the same, but good practice to get from the relevant dataset\n",
        "\n",
        "train_females = {}\n",
        "train_males = {}\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "for prop in proportions:\n",
        "    selected_male_indices = []\n",
        "    selected_female_indices = []\n",
        "    for c in range(num_classes):\n",
        "        # Indices for this class & female\n",
        "        female_class_indices = np.where((labels == c) & (gender_labels == 0))[0]\n",
        "        rng.shuffle(female_class_indices)\n",
        "        n_female = int(np.floor(len(female_class_indices) * prop))\n",
        "        selected_female_indices.extend(female_class_indices[:n_female])\n",
        "\n",
        "        # Indices for this class & male\n",
        "        male_class_indices = np.where((labels == c) & (gender_labels == 1))[0]\n",
        "        rng.shuffle(male_class_indices)\n",
        "        n_male = int(np.floor(len(male_class_indices) * prop))\n",
        "        selected_male_indices.extend(male_class_indices[:n_male])\n",
        "\n",
        "    # Make subsets (using original dataset and selected indices)\n",
        "    train_females[prop] = Subset(dataset, selected_female_indices)\n",
        "    train_males[prop] = Subset(dataset, selected_male_indices)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ayZ3vIgx9xqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_subsets[0.5], batch_size=batch_size, shuffle=True)\n",
        "# val_loader = DataLoader(test_subsets[0.5], batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "V4HgisJbFAtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "goy8JCnuN5y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## module for resnet-18"
      ],
      "metadata": {
        "id": "a2MxhhfvS2CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title putting in the utils here for easier dev\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "V5RyGgI-xDUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=8/2550,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ozdMyUbKyike"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Selecting the first column of y (assuming it's the identity label)\n",
        "                loss = nn.CrossEntropyLoss()(output, y[:, 0])\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "P0OJ1YNLy6DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initializing a run\n",
        "\n",
        "# api key: bd1c08839d0c8c49e7c3efe9aabe2d9c644befb6\n",
        "\n",
        "wandb.init(project=\"face-adv-fairness\", name=\"celeba-gender-with-robustness\", config={\"learning_rate\": 0.001, \"epochs\": 30})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "HIWp5na8TIuD",
        "outputId": "430d876d-79fd-454c-e597-b573bc806a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33midilks\u001b[0m (\u001b[33midilks-dartmouth\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250529_220112-p5sxpxs2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p5sxpxs2' target=\"_blank\">celeba-gender-with-robustness</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p5sxpxs2' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p5sxpxs2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p5sxpxs2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c86771dd250>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Selecting the first column of targets, assuming it represents the identity label\n",
        "        labels = targets[:, 0]\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train':  # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train_trades':  # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "            wandb.log({\"training_loss\": loss.item(), })"
      ],
      "metadata": {
        "id": "BPenGmbpy_C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### resnet 18"
      ],
      "metadata": {
        "id": "ID0e5sr4T8ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting this with a simpler model\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                loss = nn.CrossEntropyLoss()(output, y)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "iI3HBGKtzrWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "def make_dataloader(data_path, batch_size):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform_train)\n",
        "    val_dataset = datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def eval_test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n",
        "\n",
        "def mixup_data(x, y, mixup_alpha=1.0):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=0.003,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Lp-OlBD7pJmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        labels = targets[:, 0] # the first column is the identity label\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer)\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "id": "fKvyFN-QiJQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title resnet module\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n"
      ],
      "metadata": {
        "id": "4nLHgl8ekD7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title implementing pgd attacks\n",
        "\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = y[:, 0] # Assuming the first column is the identity label\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "                loss = nn.CrossEntropyLoss()(output, labels)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "fLKbosPFkqTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title modified train and test functions for celeba\n",
        "\n",
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = targets[:, 0] # Assuming the first column is the identity label\n",
        "\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            # Pass the original multi-dimensional targets to the attack\n",
        "            adv_x = pgd_attack(inputs, targets) # The attack will extract labels internally\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=labels, optimizer=optimizer)\n",
        "\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels.\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, labels)\n",
        "        #     adv_x = pgd_attack(inputs, targets) # Pass original targets to attack\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels from adv_x?\n",
        "        #     # This part of mixup with adversarial training might need careful consideration of how targets are handled.\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, labels) # Using extracted labels\n",
        "\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     # Use the extracted 1D labels for criterion\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "            wandb.log({f\"train_loss {train_loader.dataset}\": loss.item()}, step=epoch)\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Using Adam as in your failing block, but only for model\n",
        "\n",
        "    best_acc = 0.0 # Keep track of best average accuracy across genders\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        # Pass the extracted labels in train_ep as modified above\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "        val_acc_f = 0.0\n",
        "        val_acc_m = 0.0\n",
        "        val_loss_f = 0.0\n",
        "        val_loss_m = 0.0\n",
        "\n",
        "        if val_loader_f and len(val_loader_f.dataset) > 0:\n",
        "            val_loss_f, val_acc_f = eval_test_celeba(model, val_loader_f, device, name = 'female')\n",
        "            robust_loss_f, robust_accuracy_f = eval_robust_celeba(model, val_loader_f, pgd, device, name='female', epoch = epoch)\n",
        "\n",
        "\n",
        "        if val_loader_m and len(val_loader_m.dataset) > 0:\n",
        "            val_loss_m, val_acc_m = eval_test_celeba(model, val_loader_m, device, name = 'male')\n",
        "            robust_loss_m, robust_accuracy_m = eval_robust_celeba(model, val_loader_m, pgd, device, name = 'male', epoch = epoch)\n",
        "\n",
        "\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Average accuracy: {val_acc:.2f}, female: {val_acc_f:.2f}, male: {val_acc_m:.2f}')\n",
        "\n",
        "        wandb.log({\"val_loss_female\": val_loss_f, \"val_accuracy_female\": val_acc_f,\n",
        "               \"val_loss_male\": val_loss_m, \"val_accuracy_male\": val_acc_m,\n",
        "               \"average_val_accuracy\": val_acc}, step=epoch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval_test_celeba(model, dataloader, device, name):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # Extract identity label\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, labels).item() * inputs.size(0)\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "    test_loss /= total if total > 0 else 1\n",
        "    accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    # print(f'Test: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)')\n",
        "    # wandb.log(f\"clean_test_loss {name}: {test_loss}\", step=epoch)\n",
        "    # wandb.log(f\"clean_test_accuracy {name}: {accuracy}\", step=epoch)\n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "def eval_robust_celeba(model, dataloader, pgd_attack, device, name, epoch):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "\n",
        "    success_count = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # extract identity label\n",
        "\n",
        "            outputs_clean = model(inputs)\n",
        "            pred_clean = outputs_clean.max(1, keepdim=True)[1]\n",
        "\n",
        "\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs_adv = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs_adv, labels).item()\n",
        "            pred_adv = outputs_adv.max(1, keepdim=True)[1]\n",
        "            correct += pred_adv.eq(labels.view_as(pred_adv)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "\n",
        "            # keeping track of successful attacks\n",
        "            mask = pred_clean == labels\n",
        "            succesful_attacks = (pred_adv != labels) & mask\n",
        "\n",
        "            success_count += succesful_attacks.sum().item()\n",
        "\n",
        "\n",
        "    attack_success_rate = success_count / correct if correct > 0 else 0\n",
        "    print(f'Attack success rate: {attack_success_rate:.2f}%')\n",
        "    robust_loss /= len(dataloader.dataset) if total > 0 else 1\n",
        "    robust_accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    print(f'LinfPGD Attack: Average loss: {robust_loss:.4f}, Robust Accuracy: {robust_accuracy:.0f}%)')\n",
        "\n",
        "    wandb.log({f\"robust_loss_{name}\": robust_loss}, step=epoch)\n",
        "    wandb.log({f\"robust_accuracy_{name}\": robust_accuracy}, step=epoch)\n",
        "    wandb.log({f\"attack_success_rate_{name}\": attack_success_rate}, step=epoch)\n",
        "    return robust_loss, robust_accuracy\n"
      ],
      "metadata": {
        "id": "aZbU70I7gShk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title small sanity check\n",
        "\n",
        "wandb.init(project=\"face-adv-fairness\", name=\"celeba-sanity-check\", config={\"learning_rate\": 0.001, \"epochs\": 1})\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = ResNet18(num_classes=1000).to(device) # ResNet for identity classification\n",
        "val_loader_f = DataLoader(test_subsets_f[0.25], batch_size=64, shuffle=False) # Shuffle usually False for validation\n",
        "val_loader_m = DataLoader(test_subsets_m[0.25], batch_size=64, shuffle=False) # Shuffle usually False for validation\n",
        "pgd = LinfPGDAttack(model, epsilon=8/255, step_size = 2/255, steps = 10)\n",
        "\n",
        "robust_loss, robust_accuracy = eval_robust_celeba(model, val_loader_f, pgd, device, name = 'female', epoch = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "pqwPuQ6xvxYA",
        "outputId": "5e8af0ae-50cd-4245-e7d2-7d49e32ec853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_val_accuracy</td><td>█▁█████████████████████████</td></tr><tr><td>robust_accuracy female</td><td>█▁████▅▂█▇█████████████████</td></tr><tr><td>robust_accuracy male</td><td>█▁████▅▂█▆█████████████████</td></tr><tr><td>robust_loss female</td><td>▂█▃▂▂▂▆█▄▅▄▂▁▂▄▃▂▃▁▃▂▃▂▂▁▁▁</td></tr><tr><td>robust_loss male</td><td>▂█▃▁▁▂▆█▄▅▄▂▂▂▄▃▂▃▂▂▁▃▂▂▁▁▁</td></tr><tr><td>train_loss <torch.utils.data.dataset.Subset object at 0x7c867756f4d0></td><td>▄▆▄▃▄▄▂▅▅▅▄▂▃▄▅▄▃█▅▁▆▆▁▃▂▂▃▂</td></tr><tr><td>val_accuracy_female</td><td>█▁█████████████████████████</td></tr><tr><td>val_accuracy_male</td><td>█▁█████████████████████████</td></tr><tr><td>val_loss_female</td><td>▃█▃▂▂▂▄▅▃▄▃▂▁▂▄▄▂▃▂▃▂▃▂▂▁▁▁</td></tr><tr><td>val_loss_male</td><td>▃█▃▂▂▂▄▅▃▄▃▂▂▂▃▃▂▃▂▃▂▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_val_accuracy</td><td>91.64456</td></tr><tr><td>robust_accuracy female</td><td>90.45093</td></tr><tr><td>robust_accuracy male</td><td>88.85942</td></tr><tr><td>robust_loss female</td><td>0.00533</td></tr><tr><td>robust_loss male</td><td>0.00589</td></tr><tr><td>train_loss <torch.utils.data.dataset.Subset object at 0x7c867756f4d0></td><td>0.54087</td></tr><tr><td>val_accuracy_female</td><td>92.30769</td></tr><tr><td>val_accuracy_male</td><td>90.98143</td></tr><tr><td>val_loss_female</td><td>0.29766</td></tr><tr><td>val_loss_male</td><td>0.32516</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">celeba-gender-with-robustness</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p5sxpxs2' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/p5sxpxs2</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250529_220112-p5sxpxs2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250529_222835-3ua1ebnj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/3ua1ebnj' target=\"_blank\">celeba-sanity-check</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/3ua1ebnj' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/3ua1ebnj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.1099, Robust Accuracy: 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "sZhjgFrvIJyA",
        "outputId": "5c5e3dcf-fcba-4ba7-c70a-66e3e70cb2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_female</td><td>▁</td></tr><tr><td>robust_accuracy_female</td><td>▁</td></tr><tr><td>robust_loss_female</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_female</td><td>0</td></tr><tr><td>robust_accuracy_female</td><td>0</td></tr><tr><td>robust_loss_female</td><td>0.10988</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">celeba-sanity-check</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/3ua1ebnj' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/3ua1ebnj</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250529_222835-3ua1ebnj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250529_222949-yhch932m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/yhch932m' target=\"_blank\">celeba-gender-with-robustness_attack_success</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/yhch932m' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/yhch932m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/yhch932m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c86755ca2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "batch_size = 64\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=1000).to(device) # ResNet for identity classification\n",
        "    # Note: number of classes (1000) should match the number of unique identities\n",
        "    # we filtered initially by top 1000 identitites but this might be limiting perhaps?\n",
        "    # it gives very few examples on the test set\n",
        "    # make a new run for each example\n",
        "    wandb.init(project=\"face-adv-fairness\", name=f\"celeba-gender-{proportion}\", config={\"learning_rate\": 0.001, \"epochs\": 30})\n",
        "\n",
        "\n",
        "    num_identity_classes = 1000 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    if proportion in test_subsets_f and len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "    if proportion in test_subsets_m and len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "\n",
        "\n",
        "    # call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader_f=val_loader_f, val_loader_m=val_loader_m,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=20) # Save checkpoints with proportion\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-THEtjHpv_T_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4cfdd398-a2b6-4f2b-a065-117c3005160e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss <torch.utils.data.dataset.Subset object at 0x7c867756f4d0></td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss <torch.utils.data.dataset.Subset object at 0x7c867756f4d0></td><td>0.58438</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">celeba-gender-with-robustness_attack_success</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/yhch932m' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/yhch932m</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250529_222949-yhch932m/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250529_223129-771msuo9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/771msuo9' target=\"_blank\">celeba-gender-0.25</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/771msuo9' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/771msuo9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 7.046680\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.827129\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.635453\n",
            "Train Epoch: 0 [09664/39936 (97%)]\t Loss: 0.791710\n",
            "Attack success rate: 2.69%\n",
            "LinfPGD Attack: Average loss: 0.0090, Robust Accuracy: 87%)\n",
            "Attack success rate: 4.74%\n",
            "LinfPGD Attack: Average loss: 0.0092, Robust Accuracy: 84%)\n",
            "Average accuracy: 87.93, female: 88.33, male: 87.53\n",
            "Train Epoch: 1 [00064/39936 (1%)]\t Loss: 0.674365\n",
            "Train Epoch: 1 [03264/39936 (33%)]\t Loss: 0.649970\n",
            "Train Epoch: 1 [06464/39936 (65%)]\t Loss: 0.645569\n",
            "Train Epoch: 1 [09664/39936 (97%)]\t Loss: 0.642040\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0046, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0052, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 2 [00064/39936 (1%)]\t Loss: 0.755392\n",
            "Train Epoch: 2 [03264/39936 (33%)]\t Loss: 0.674201\n",
            "Train Epoch: 2 [06464/39936 (65%)]\t Loss: 0.651369\n",
            "Train Epoch: 2 [09664/39936 (97%)]\t Loss: 0.653498\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0054, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0058, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 3 [00064/39936 (1%)]\t Loss: 0.669203\n",
            "Train Epoch: 3 [03264/39936 (33%)]\t Loss: 0.546864\n",
            "Train Epoch: 3 [06464/39936 (65%)]\t Loss: 0.647743\n",
            "Train Epoch: 3 [09664/39936 (97%)]\t Loss: 0.594953\n",
            "Attack success rate: 0.17%\n",
            "LinfPGD Attack: Average loss: 0.0078, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.16%\n",
            "LinfPGD Attack: Average loss: 0.0080, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 4 [00064/39936 (1%)]\t Loss: 0.631649\n",
            "Train Epoch: 4 [03264/39936 (33%)]\t Loss: 0.553092\n",
            "Train Epoch: 4 [06464/39936 (65%)]\t Loss: 0.647012\n",
            "Train Epoch: 4 [09664/39936 (97%)]\t Loss: 0.590961\n",
            "Attack success rate: 3.23%\n",
            "LinfPGD Attack: Average loss: 0.0074, Robust Accuracy: 90%)\n",
            "Attack success rate: 3.98%\n",
            "LinfPGD Attack: Average loss: 0.0078, Robust Accuracy: 87%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 5 [00064/39936 (1%)]\t Loss: 0.596091\n",
            "Train Epoch: 5 [03264/39936 (33%)]\t Loss: 0.561079\n",
            "Train Epoch: 5 [06464/39936 (65%)]\t Loss: 0.607286\n",
            "Train Epoch: 5 [09664/39936 (97%)]\t Loss: 0.610278\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0050, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0055, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 6 [00064/39936 (1%)]\t Loss: 0.522910\n",
            "Train Epoch: 6 [03264/39936 (33%)]\t Loss: 0.678194\n",
            "Train Epoch: 6 [06464/39936 (65%)]\t Loss: 0.665867\n",
            "Train Epoch: 6 [09664/39936 (97%)]\t Loss: 0.618360\n",
            "Attack success rate: 3.58%\n",
            "LinfPGD Attack: Average loss: 0.0077, Robust Accuracy: 90%)\n",
            "Attack success rate: 5.20%\n",
            "LinfPGD Attack: Average loss: 0.0081, Robust Accuracy: 86%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 7 [00064/39936 (1%)]\t Loss: 0.637037\n",
            "Train Epoch: 7 [03264/39936 (33%)]\t Loss: 0.586153\n",
            "Train Epoch: 7 [06464/39936 (65%)]\t Loss: 0.580705\n",
            "Train Epoch: 7 [09664/39936 (97%)]\t Loss: 0.648394\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0050, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0056, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 8 [00064/39936 (1%)]\t Loss: 0.565276\n",
            "Train Epoch: 8 [03264/39936 (33%)]\t Loss: 0.663077\n",
            "Train Epoch: 8 [06464/39936 (65%)]\t Loss: 0.645509\n",
            "Train Epoch: 8 [09664/39936 (97%)]\t Loss: 0.589114\n",
            "Attack success rate: 0.18%\n",
            "LinfPGD Attack: Average loss: 0.0055, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0061, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 9 [00064/39936 (1%)]\t Loss: 0.602389\n",
            "Train Epoch: 9 [03264/39936 (33%)]\t Loss: 0.540578\n",
            "Train Epoch: 9 [06464/39936 (65%)]\t Loss: 0.671680\n",
            "Train Epoch: 9 [09664/39936 (97%)]\t Loss: 0.593940\n",
            "Attack success rate: 4.50%\n",
            "LinfPGD Attack: Average loss: 0.0075, Robust Accuracy: 89%)\n",
            "Attack success rate: 7.10%\n",
            "LinfPGD Attack: Average loss: 0.0080, Robust Accuracy: 85%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 10 [00064/39936 (1%)]\t Loss: 0.537013\n",
            "Train Epoch: 10 [03264/39936 (33%)]\t Loss: 0.583266\n",
            "Train Epoch: 10 [06464/39936 (65%)]\t Loss: 0.515215\n",
            "Train Epoch: 10 [09664/39936 (97%)]\t Loss: 0.560188\n",
            "Attack success rate: 1.66%\n",
            "LinfPGD Attack: Average loss: 0.0055, Robust Accuracy: 92%)\n",
            "Attack success rate: 1.85%\n",
            "LinfPGD Attack: Average loss: 0.0059, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 11 [00064/39936 (1%)]\t Loss: 0.621410\n",
            "Train Epoch: 11 [03264/39936 (33%)]\t Loss: 0.562132\n",
            "Train Epoch: 11 [06464/39936 (65%)]\t Loss: 0.510418\n",
            "Train Epoch: 11 [09664/39936 (97%)]\t Loss: 0.557198\n",
            "Attack success rate: 0.34%\n",
            "LinfPGD Attack: Average loss: 0.0054, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.50%\n",
            "LinfPGD Attack: Average loss: 0.0059, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 12 [00064/39936 (1%)]\t Loss: 0.534307\n",
            "Train Epoch: 12 [03264/39936 (33%)]\t Loss: 0.593113\n",
            "Train Epoch: 12 [06464/39936 (65%)]\t Loss: 0.562443\n",
            "Train Epoch: 12 [09664/39936 (97%)]\t Loss: 0.548766\n",
            "Attack success rate: 8.35%\n",
            "LinfPGD Attack: Average loss: 0.0069, Robust Accuracy: 86%)\n",
            "Attack success rate: 8.99%\n",
            "LinfPGD Attack: Average loss: 0.0073, Robust Accuracy: 85%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 13 [00064/39936 (1%)]\t Loss: 0.514834\n",
            "Train Epoch: 13 [03264/39936 (33%)]\t Loss: 0.516662\n",
            "Train Epoch: 13 [06464/39936 (65%)]\t Loss: 0.598023\n",
            "Train Epoch: 13 [09664/39936 (97%)]\t Loss: 0.570229\n",
            "Attack success rate: 1.07%\n",
            "LinfPGD Attack: Average loss: 0.0061, Robust Accuracy: 89%)\n",
            "Attack success rate: 2.97%\n",
            "LinfPGD Attack: Average loss: 0.0066, Robust Accuracy: 84%)\n",
            "Average accuracy: 89.52, female: 90.45, male: 88.59\n",
            "Train Epoch: 14 [00064/39936 (1%)]\t Loss: 0.591787\n",
            "Train Epoch: 14 [03264/39936 (33%)]\t Loss: 0.505498\n",
            "Train Epoch: 14 [06464/39936 (65%)]\t Loss: 0.607856\n",
            "Train Epoch: 14 [09664/39936 (97%)]\t Loss: 0.567799\n",
            "Attack success rate: 3.24%\n",
            "LinfPGD Attack: Average loss: 0.0056, Robust Accuracy: 90%)\n",
            "Attack success rate: 3.94%\n",
            "LinfPGD Attack: Average loss: 0.0060, Robust Accuracy: 88%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 15 [00064/39936 (1%)]\t Loss: 0.538987\n",
            "Train Epoch: 15 [03264/39936 (33%)]\t Loss: 0.512368\n",
            "Train Epoch: 15 [06464/39936 (65%)]\t Loss: 0.560654\n",
            "Train Epoch: 15 [09664/39936 (97%)]\t Loss: 0.596372\n",
            "Attack success rate: 0.17%\n",
            "LinfPGD Attack: Average loss: 0.0046, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.35%\n",
            "LinfPGD Attack: Average loss: 0.0049, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 16 [00064/39936 (1%)]\t Loss: 0.673801\n",
            "Train Epoch: 16 [03264/39936 (33%)]\t Loss: 0.565968\n",
            "Train Epoch: 16 [06464/39936 (65%)]\t Loss: 0.503398\n",
            "Train Epoch: 16 [09664/39936 (97%)]\t Loss: 0.567583\n",
            "Attack success rate: 1.49%\n",
            "LinfPGD Attack: Average loss: 0.0051, Robust Accuracy: 92%)\n",
            "Attack success rate: 1.53%\n",
            "LinfPGD Attack: Average loss: 0.0052, Robust Accuracy: 91%)\n",
            "Average accuracy: 92.18, female: 92.31, male: 92.04\n",
            "Train Epoch: 17 [00064/39936 (1%)]\t Loss: 0.450871\n",
            "Train Epoch: 17 [03264/39936 (33%)]\t Loss: 0.536220\n",
            "Train Epoch: 17 [06464/39936 (65%)]\t Loss: 0.635165\n",
            "Train Epoch: 17 [09664/39936 (97%)]\t Loss: 0.527311\n",
            "Attack success rate: 7.18%\n",
            "LinfPGD Attack: Average loss: 0.0057, Robust Accuracy: 84%)\n",
            "Attack success rate: 6.50%\n",
            "LinfPGD Attack: Average loss: 0.0056, Robust Accuracy: 84%)\n",
            "Average accuracy: 91.91, female: 92.31, male: 91.51\n",
            "Train Epoch: 18 [00064/39936 (1%)]\t Loss: 0.558090\n",
            "Train Epoch: 18 [03264/39936 (33%)]\t Loss: 0.481057\n",
            "Train Epoch: 18 [06464/39936 (65%)]\t Loss: 0.532177\n",
            "Train Epoch: 18 [09664/39936 (97%)]\t Loss: 0.621087\n",
            "Attack success rate: 2.79%\n",
            "LinfPGD Attack: Average loss: 0.0050, Robust Accuracy: 89%)\n",
            "Attack success rate: 5.30%\n",
            "LinfPGD Attack: Average loss: 0.0052, Robust Accuracy: 85%)\n",
            "Average accuracy: 90.98, female: 91.25, male: 90.72\n",
            "Train Epoch: 19 [00064/39936 (1%)]\t Loss: 0.598164\n",
            "Train Epoch: 19 [03264/39936 (33%)]\t Loss: 0.490106\n",
            "Train Epoch: 19 [06464/39936 (65%)]\t Loss: 0.494423\n",
            "Train Epoch: 19 [09664/39936 (97%)]\t Loss: 0.519069\n",
            "Attack success rate: 1.87%\n",
            "LinfPGD Attack: Average loss: 0.0044, Robust Accuracy: 92%)\n",
            "Attack success rate: 1.64%\n",
            "LinfPGD Attack: Average loss: 0.0047, Robust Accuracy: 91%)\n",
            "Average accuracy: 91.64, female: 92.04, male: 91.25\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_female</td><td>▃▁▁▁▄▁▄▁▁▅▂▁█▂▄▁▂▇▃▃</td></tr><tr><td>attack_success_rate_male</td><td>▅▁▁▁▄▁▅▁▁▇▂▁█▃▄▁▂▆▅▂</td></tr><tr><td>average_val_accuracy</td><td>▁▆▆▆▆▆▆▆▆▆▆▆▆▄▆▆██▆▇</td></tr><tr><td>robust_accuracy_female</td><td>▄███▇█▆██▅██▃▅▆██▁▅█</td></tr><tr><td>robust_accuracy_male</td><td>▁██▇▄█▃██▃▇▇▂▁▅▇█▁▃█</td></tr><tr><td>robust_loss_female</td><td>█▁▂▆▅▂▆▂▃▆▃▃▅▄▃▁▂▃▂▁</td></tr><tr><td>robust_loss_male</td><td>█▂▃▆▆▂▆▂▃▆▃▃▅▄▃▁▂▂▂▁</td></tr><tr><td>train_loss <torch.utils.data.dataset.Subset object at 0x7c867756f4d0></td><td>█▄▄▃▃▃▄▄▃▃▂▂▂▂▂▃▂▁▄▁</td></tr><tr><td>val_accuracy_female</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▅▇▇██▆█</td></tr><tr><td>val_accuracy_male</td><td>▁▆▆▆▆▆▆▆▆▆▆▆▆▃▆▆█▇▆▇</td></tr><tr><td>val_loss_female</td><td>█▂▂▄▄▂▅▂▂▄▂▂▃▃▃▁▂▂▂▁</td></tr><tr><td>val_loss_male</td><td>█▂▂▄▄▂▅▂▃▅▃▂▃▃▃▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_female</td><td>1.86744</td></tr><tr><td>attack_success_rate_male</td><td>1.63743</td></tr><tr><td>average_val_accuracy</td><td>91.64456</td></tr><tr><td>robust_accuracy_female</td><td>92.04244</td></tr><tr><td>robust_accuracy_male</td><td>90.71618</td></tr><tr><td>robust_loss_female</td><td>0.00441</td></tr><tr><td>robust_loss_male</td><td>0.00465</td></tr><tr><td>train_loss <torch.utils.data.dataset.Subset object at 0x7c867756f4d0></td><td>0.51907</td></tr><tr><td>val_accuracy_female</td><td>92.04244</td></tr><tr><td>val_accuracy_male</td><td>91.24668</td></tr><tr><td>val_loss_female</td><td>0.25663</td></tr><tr><td>val_loss_male</td><td>0.27023</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">celeba-gender-0.25</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/771msuo9' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/771msuo9</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250529_223129-771msuo9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250529_224245-gxw6cm4i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/gxw6cm4i' target=\"_blank\">celeba-gender-0.5</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/gxw6cm4i' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/gxw6cm4i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 7.082790\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.459345\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.639854\n",
            "Train Epoch: 0 [09664/39936 (97%)]\t Loss: 0.610312\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0049, Robust Accuracy: 91%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0052, Robust Accuracy: 90%)\n",
            "Average accuracy: 90.53, female: 91.26, male: 89.80\n",
            "Train Epoch: 1 [00064/39936 (1%)]\t Loss: 0.687378\n",
            "Train Epoch: 1 [03264/39936 (33%)]\t Loss: 0.550997\n",
            "Train Epoch: 1 [06464/39936 (65%)]\t Loss: 0.317281\n",
            "Train Epoch: 1 [09664/39936 (97%)]\t Loss: 0.568782\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0046, Robust Accuracy: 91%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0051, Robust Accuracy: 90%)\n",
            "Average accuracy: 90.53, female: 91.26, male: 89.80\n",
            "Train Epoch: 2 [00064/39936 (1%)]\t Loss: 0.511059\n",
            "Train Epoch: 2 [03264/39936 (33%)]\t Loss: 0.576831\n",
            "Train Epoch: 2 [06464/39936 (65%)]\t Loss: 0.586397\n",
            "Train Epoch: 2 [09664/39936 (97%)]\t Loss: 0.526365\n",
            "Attack success rate: 1.92%\n",
            "LinfPGD Attack: Average loss: 0.0061, Robust Accuracy: 89%)\n",
            "Attack success rate: 1.31%\n",
            "LinfPGD Attack: Average loss: 0.0062, Robust Accuracy: 88%)\n",
            "Average accuracy: 90.46, female: 91.26, male: 89.67\n",
            "Train Epoch: 3 [00064/39936 (1%)]\t Loss: 0.426896\n",
            "Train Epoch: 3 [03264/39936 (33%)]\t Loss: 0.406764\n",
            "Train Epoch: 3 [06464/39936 (65%)]\t Loss: 0.513689\n",
            "Train Epoch: 3 [09664/39936 (97%)]\t Loss: 0.490635\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0046, Robust Accuracy: 91%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0049, Robust Accuracy: 90%)\n",
            "Average accuracy: 90.53, female: 91.26, male: 89.80\n",
            "Train Epoch: 4 [00064/39936 (1%)]\t Loss: 0.484777\n",
            "Train Epoch: 4 [03264/39936 (33%)]\t Loss: 0.426444\n",
            "Train Epoch: 4 [06464/39936 (65%)]\t Loss: 0.570575\n",
            "Train Epoch: 4 [09664/39936 (97%)]\t Loss: 0.481195\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0051, Robust Accuracy: 91%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0056, Robust Accuracy: 90%)\n",
            "Average accuracy: 90.53, female: 91.26, male: 89.80\n",
            "Train Epoch: 5 [00064/39936 (1%)]\t Loss: 0.539366\n",
            "Train Epoch: 5 [03264/39936 (33%)]\t Loss: 0.516631\n",
            "Train Epoch: 5 [06464/39936 (65%)]\t Loss: 0.515733\n",
            "Train Epoch: 5 [09664/39936 (97%)]\t Loss: 0.500075\n",
            "Attack success rate: 0.43%\n",
            "LinfPGD Attack: Average loss: 0.0053, Robust Accuracy: 91%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0055, Robust Accuracy: 90%)\n",
            "Average accuracy: 90.53, female: 91.26, male: 89.80\n",
            "Train Epoch: 6 [00064/39936 (1%)]\t Loss: 0.551894\n",
            "Train Epoch: 6 [03264/39936 (33%)]\t Loss: 0.380653\n",
            "Train Epoch: 6 [06464/39936 (65%)]\t Loss: 0.344846\n",
            "Train Epoch: 6 [09664/39936 (97%)]\t Loss: 0.535407\n",
            "Attack success rate: 1.54%\n",
            "LinfPGD Attack: Average loss: 0.0052, Robust Accuracy: 90%)\n",
            "Attack success rate: 1.26%\n",
            "LinfPGD Attack: Average loss: 0.0054, Robust Accuracy: 89%)\n",
            "Average accuracy: 90.53, female: 91.26, male: 89.80\n",
            "Train Epoch: 7 [00064/39936 (1%)]\t Loss: 0.432554\n",
            "Train Epoch: 7 [03264/39936 (33%)]\t Loss: 0.452812\n",
            "Train Epoch: 7 [06464/39936 (65%)]\t Loss: 0.332265\n",
            "Train Epoch: 7 [09664/39936 (97%)]\t Loss: 0.404204\n",
            "Attack success rate: 10.60%\n",
            "LinfPGD Attack: Average loss: 0.0075, Robust Accuracy: 79%)\n",
            "Attack success rate: 10.48%\n",
            "LinfPGD Attack: Average loss: 0.0074, Robust Accuracy: 80%)\n",
            "Average accuracy: 86.69, female: 87.68, male: 85.70\n",
            "Train Epoch: 8 [00064/39936 (1%)]\t Loss: 0.461636\n",
            "Train Epoch: 8 [03264/39936 (33%)]\t Loss: 0.518849\n",
            "Train Epoch: 8 [06464/39936 (65%)]\t Loss: 0.506945\n",
            "Train Epoch: 8 [09664/39936 (97%)]\t Loss: 0.496254\n",
            "Attack success rate: 0.08%\n",
            "LinfPGD Attack: Average loss: 0.0046, Robust Accuracy: 91%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0050, Robust Accuracy: 90%)\n",
            "Average accuracy: 90.53, female: 91.26, male: 89.80\n",
            "Train Epoch: 9 [00064/39936 (1%)]\t Loss: 0.377047\n",
            "Train Epoch: 9 [03264/39936 (33%)]\t Loss: 0.362764\n",
            "Train Epoch: 9 [06464/39936 (65%)]\t Loss: 0.488666\n",
            "Train Epoch: 9 [09664/39936 (97%)]\t Loss: 0.432672\n",
            "Attack success rate: 1.30%\n",
            "LinfPGD Attack: Average loss: 0.0045, Robust Accuracy: 90%)\n",
            "Attack success rate: 0.68%\n",
            "LinfPGD Attack: Average loss: 0.0046, Robust Accuracy: 89%)\n",
            "Average accuracy: 90.53, female: 91.26, male: 89.80\n",
            "Train Epoch: 10 [00064/39936 (1%)]\t Loss: 0.433383\n",
            "Train Epoch: 10 [03264/39936 (33%)]\t Loss: 0.548681\n",
            "Train Epoch: 10 [06464/39936 (65%)]\t Loss: 0.379500\n",
            "Train Epoch: 10 [09664/39936 (97%)]\t Loss: 0.456043\n",
            "Attack success rate: 0.26%\n",
            "LinfPGD Attack: Average loss: 0.0043, Robust Accuracy: 91%)\n",
            "Attack success rate: 0.26%\n",
            "LinfPGD Attack: Average loss: 0.0044, Robust Accuracy: 89%)\n",
            "Average accuracy: 90.60, female: 91.39, male: 89.80\n",
            "Train Epoch: 11 [00064/39936 (1%)]\t Loss: 0.531344\n",
            "Train Epoch: 11 [03264/39936 (33%)]\t Loss: 0.459409\n",
            "Train Epoch: 11 [06464/39936 (65%)]\t Loss: 0.385976\n",
            "Train Epoch: 11 [09664/39936 (97%)]\t Loss: 0.524559\n",
            "Attack success rate: 1.22%\n",
            "LinfPGD Attack: Average loss: 0.0048, Robust Accuracy: 88%)\n",
            "Attack success rate: 1.47%\n",
            "LinfPGD Attack: Average loss: 0.0048, Robust Accuracy: 88%)\n",
            "Average accuracy: 90.79, female: 90.60, male: 90.99\n",
            "Train Epoch: 12 [00064/39936 (1%)]\t Loss: 0.462292\n",
            "Train Epoch: 12 [03264/39936 (33%)]\t Loss: 0.399802\n",
            "Train Epoch: 12 [06464/39936 (65%)]\t Loss: 0.321049\n",
            "Train Epoch: 12 [09664/39936 (97%)]\t Loss: 0.336355\n",
            "Attack success rate: 1.63%\n",
            "LinfPGD Attack: Average loss: 0.0049, Robust Accuracy: 90%)\n",
            "Attack success rate: 0.94%\n",
            "LinfPGD Attack: Average loss: 0.0050, Robust Accuracy: 89%)\n",
            "Average accuracy: 90.53, female: 91.26, male: 89.80\n",
            "Train Epoch: 13 [00064/39936 (1%)]\t Loss: 0.554624\n",
            "Train Epoch: 13 [03264/39936 (33%)]\t Loss: 0.446860\n",
            "Train Epoch: 13 [06464/39936 (65%)]\t Loss: 0.458340\n",
            "Train Epoch: 13 [09664/39936 (97%)]\t Loss: 0.499801\n",
            "Attack success rate: 2.46%\n",
            "LinfPGD Attack: Average loss: 0.0053, Robust Accuracy: 86%)\n",
            "Attack success rate: 2.62%\n",
            "LinfPGD Attack: Average loss: 0.0052, Robust Accuracy: 85%)\n",
            "Average accuracy: 89.93, female: 90.20, male: 89.67\n",
            "Train Epoch: 14 [00064/39936 (1%)]\t Loss: 0.526317\n",
            "Train Epoch: 14 [03264/39936 (33%)]\t Loss: 0.443014\n",
            "Train Epoch: 14 [06464/39936 (65%)]\t Loss: 0.401507\n",
            "Train Epoch: 14 [09664/39936 (97%)]\t Loss: 0.490440\n",
            "Attack success rate: 1.14%\n",
            "LinfPGD Attack: Average loss: 0.0052, Robust Accuracy: 88%)\n",
            "Attack success rate: 1.18%\n",
            "LinfPGD Attack: Average loss: 0.0052, Robust Accuracy: 88%)\n",
            "Average accuracy: 90.53, female: 90.46, male: 90.60\n",
            "Train Epoch: 15 [00064/39936 (1%)]\t Loss: 0.423744\n",
            "Train Epoch: 15 [03264/39936 (33%)]\t Loss: 0.482639\n",
            "Train Epoch: 15 [06464/39936 (65%)]\t Loss: 0.419453\n",
            "Train Epoch: 15 [09664/39936 (97%)]\t Loss: 0.493091\n",
            "Attack success rate: 2.88%\n",
            "LinfPGD Attack: Average loss: 0.0054, Robust Accuracy: 86%)\n",
            "Attack success rate: 1.99%\n",
            "LinfPGD Attack: Average loss: 0.0053, Robust Accuracy: 86%)\n",
            "Average accuracy: 90.00, female: 90.33, male: 89.67\n",
            "Train Epoch: 16 [00064/39936 (1%)]\t Loss: 0.459872\n",
            "Train Epoch: 16 [03264/39936 (33%)]\t Loss: 0.355994\n",
            "Train Epoch: 16 [06464/39936 (65%)]\t Loss: 0.398772\n",
            "Train Epoch: 16 [09664/39936 (97%)]\t Loss: 0.449743\n",
            "Attack success rate: 1.53%\n",
            "LinfPGD Attack: Average loss: 0.0049, Robust Accuracy: 88%)\n",
            "Attack success rate: 1.31%\n",
            "LinfPGD Attack: Average loss: 0.0048, Robust Accuracy: 88%)\n",
            "Average accuracy: 90.73, female: 90.86, male: 90.60\n",
            "Train Epoch: 17 [00064/39936 (1%)]\t Loss: 0.310229\n",
            "Train Epoch: 17 [03264/39936 (33%)]\t Loss: 0.450854\n",
            "Train Epoch: 17 [06464/39936 (65%)]\t Loss: 0.340207\n",
            "Train Epoch: 17 [09664/39936 (97%)]\t Loss: 0.432191\n",
            "Attack success rate: 2.39%\n",
            "LinfPGD Attack: Average loss: 0.0048, Robust Accuracy: 87%)\n",
            "Attack success rate: 2.34%\n",
            "LinfPGD Attack: Average loss: 0.0047, Robust Accuracy: 86%)\n",
            "Average accuracy: 91.13, female: 91.79, male: 90.46\n",
            "Train Epoch: 18 [00064/39936 (1%)]\t Loss: 0.502215\n",
            "Train Epoch: 18 [03264/39936 (33%)]\t Loss: 0.341071\n",
            "Train Epoch: 18 [06464/39936 (65%)]\t Loss: 0.349554\n",
            "Train Epoch: 18 [09664/39936 (97%)]\t Loss: 0.378935\n",
            "Attack success rate: 1.53%\n",
            "LinfPGD Attack: Average loss: 0.0044, Robust Accuracy: 90%)\n",
            "Attack success rate: 1.12%\n",
            "LinfPGD Attack: Average loss: 0.0042, Robust Accuracy: 89%)\n",
            "Average accuracy: 90.73, female: 91.52, male: 89.93\n",
            "Train Epoch: 19 [00064/39936 (1%)]\t Loss: 0.383713\n",
            "Train Epoch: 19 [03264/39936 (33%)]\t Loss: 0.471329\n",
            "Train Epoch: 19 [06464/39936 (65%)]\t Loss: 0.346446\n",
            "Train Epoch: 19 [09664/39936 (97%)]\t Loss: 0.451616\n",
            "Attack success rate: 0.91%\n",
            "LinfPGD Attack: Average loss: 0.0048, Robust Accuracy: 89%)\n",
            "Attack success rate: 0.74%\n",
            "LinfPGD Attack: Average loss: 0.0044, Robust Accuracy: 88%)\n",
            "Average accuracy: 90.46, female: 91.26, male: 89.67\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_female</td><td>▁▁▂▁▁▁▂█▁▂▁▂▂▃▂▃▂▃▂▂</td></tr><tr><td>attack_success_rate_male</td><td>▁▁▂▁▁▁▂█▁▁▁▂▂▃▂▂▂▃▂▁</td></tr><tr><td>average_val_accuracy</td><td>▇▇▇▇▇▇▇▁▇▇▇▇▇▆▇▆▇█▇▇</td></tr><tr><td>robust_accuracy_female</td><td>██▇███▇▁█▇█▆▇▅▆▅▆▆▇▇</td></tr><tr><td>robust_accuracy_male</td><td>██▇███▇▁█▇█▇▇▅▇▅▇▅▇▇</td></tr><tr><td>robust_loss_female</td><td>▂▂▅▂▃▃▃█▂▂▁▂▂▃▃▄▂▂▁▂</td></tr><tr><td>robust_loss_male</td><td>▃▃▅▃▄▄▄█▃▂▁▂▃▃▃▃▂▂▁▁</td></tr><tr><td>train_loss <torch.utils.data.dataset.Subset object at 0x7c8677760250></td><td>█▇▆▅▅▅▆▃▅▃▄▆▁▅▅▅▄▃▂▄</td></tr><tr><td>val_accuracy_female</td><td>▇▇▇▇▇▇▇▁▇▇▇▆▇▅▆▆▆██▇</td></tr><tr><td>val_accuracy_male</td><td>▆▆▆▆▆▆▆▁▆▆▆█▆▆▇▆▇▇▇▆</td></tr><tr><td>val_loss_female</td><td>▃▃▅▃▃▄▄█▂▂▂▂▃▂▃▃▂▂▁▁</td></tr><tr><td>val_loss_male</td><td>▄▄▆▄▄▄▅█▃▂▂▂▃▂▃▃▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>attack_success_rate_female</td><td>0.90815</td></tr><tr><td>attack_success_rate_male</td><td>0.74398</td></tr><tr><td>average_val_accuracy</td><td>90.46358</td></tr><tr><td>robust_accuracy_female</td><td>89.40397</td></tr><tr><td>robust_accuracy_male</td><td>87.94702</td></tr><tr><td>robust_loss_female</td><td>0.00477</td></tr><tr><td>robust_loss_male</td><td>0.00438</td></tr><tr><td>train_loss <torch.utils.data.dataset.Subset object at 0x7c8677760250></td><td>0.45162</td></tr><tr><td>val_accuracy_female</td><td>91.25828</td></tr><tr><td>val_accuracy_male</td><td>89.66887</td></tr><tr><td>val_loss_female</td><td>0.21658</td></tr><tr><td>val_loss_male</td><td>0.21069</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">celeba-gender-0.5</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/gxw6cm4i' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/gxw6cm4i</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250529_224245-gxw6cm4i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250529_225515-79ltefmv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/79ltefmv' target=\"_blank\">celeba-gender-0.75</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/79ltefmv' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/79ltefmv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 7.466568\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.378760\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.402743\n",
            "Train Epoch: 0 [09664/39936 (97%)]\t Loss: 0.283337\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0047, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0054, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 1 [00064/39936 (1%)]\t Loss: 0.328532\n",
            "Train Epoch: 1 [03264/39936 (33%)]\t Loss: 0.307150\n",
            "Train Epoch: 1 [06464/39936 (65%)]\t Loss: 0.431284\n",
            "Train Epoch: 1 [09664/39936 (97%)]\t Loss: 0.657547\n",
            "Attack success rate: 1.64%\n",
            "LinfPGD Attack: Average loss: 0.0069, Robust Accuracy: 86%)\n",
            "Attack success rate: 1.06%\n",
            "LinfPGD Attack: Average loss: 0.0073, Robust Accuracy: 85%)\n",
            "Average accuracy: 87.40, female: 88.33, male: 86.47\n",
            "Train Epoch: 2 [00064/39936 (1%)]\t Loss: 0.406748\n",
            "Train Epoch: 2 [03264/39936 (33%)]\t Loss: 0.260706\n",
            "Train Epoch: 2 [06464/39936 (65%)]\t Loss: 0.276284\n",
            "Train Epoch: 2 [09664/39936 (97%)]\t Loss: 0.160800\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0045, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0050, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 3 [00064/39936 (1%)]\t Loss: 0.153122\n",
            "Train Epoch: 3 [03264/39936 (33%)]\t Loss: 0.412893\n",
            "Train Epoch: 3 [06464/39936 (65%)]\t Loss: 0.396223\n",
            "Train Epoch: 3 [09664/39936 (97%)]\t Loss: 0.403559\n",
            "Attack success rate: 10.36%\n",
            "LinfPGD Attack: Average loss: 0.0072, Robust Accuracy: 81%)\n",
            "Attack success rate: 8.44%\n",
            "LinfPGD Attack: Average loss: 0.0072, Robust Accuracy: 81%)\n",
            "Average accuracy: 86.60, female: 85.68, male: 87.53\n",
            "Train Epoch: 4 [00064/39936 (1%)]\t Loss: 0.432465\n",
            "Train Epoch: 4 [03264/39936 (33%)]\t Loss: 0.130527\n",
            "Train Epoch: 4 [06464/39936 (65%)]\t Loss: 0.353971\n",
            "Train Epoch: 4 [09664/39936 (97%)]\t Loss: 0.339740\n",
            "Attack success rate: 0.01%\n",
            "LinfPGD Attack: Average loss: 0.0044, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.18%\n",
            "LinfPGD Attack: Average loss: 0.0048, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.25, female: 92.04, male: 90.45\n",
            "Train Epoch: 5 [00064/39936 (1%)]\t Loss: 0.303997\n",
            "Train Epoch: 5 [03264/39936 (33%)]\t Loss: 0.335714\n",
            "Train Epoch: 5 [06464/39936 (65%)]\t Loss: 0.286682\n",
            "Train Epoch: 5 [09664/39936 (97%)]\t Loss: 0.256992\n",
            "Attack success rate: 10.23%\n",
            "LinfPGD Attack: Average loss: 0.0125, Robust Accuracy: 45%)\n",
            "Attack success rate: 9.53%\n",
            "LinfPGD Attack: Average loss: 0.0120, Robust Accuracy: 48%)\n",
            "Average accuracy: 55.31, female: 53.85, male: 56.76\n",
            "Train Epoch: 6 [00064/39936 (1%)]\t Loss: 0.602920\n",
            "Train Epoch: 6 [03264/39936 (33%)]\t Loss: 0.194536\n",
            "Train Epoch: 6 [06464/39936 (65%)]\t Loss: 0.242505\n",
            "Train Epoch: 6 [09664/39936 (97%)]\t Loss: 0.204260\n",
            "Attack success rate: 0.15%\n",
            "LinfPGD Attack: Average loss: 0.0042, Robust Accuracy: 92%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0046, Robust Accuracy: 90%)\n",
            "Average accuracy: 91.25, female: 92.04, male: 90.45\n",
            "Train Epoch: 7 [00064/39936 (1%)]\t Loss: 0.385504\n",
            "Train Epoch: 7 [03264/39936 (33%)]\t Loss: 0.316798\n",
            "Train Epoch: 7 [06464/39936 (65%)]\t Loss: 0.474367\n",
            "Train Epoch: 7 [09664/39936 (97%)]\t Loss: 0.112740\n",
            "Attack success rate: 0.63%\n",
            "LinfPGD Attack: Average loss: 0.0045, Robust Accuracy: 90%)\n",
            "Attack success rate: 1.49%\n",
            "LinfPGD Attack: Average loss: 0.0048, Robust Accuracy: 88%)\n",
            "Average accuracy: 91.91, female: 92.04, male: 91.78\n",
            "Train Epoch: 8 [00064/39936 (1%)]\t Loss: 0.234571\n",
            "Train Epoch: 8 [03264/39936 (33%)]\t Loss: 0.230972\n",
            "Train Epoch: 8 [06464/39936 (65%)]\t Loss: 0.339554\n",
            "Train Epoch: 8 [09664/39936 (97%)]\t Loss: 0.350323\n",
            "Attack success rate: 0.73%\n",
            "LinfPGD Attack: Average loss: 0.0046, Robust Accuracy: 90%)\n",
            "Attack success rate: 0.79%\n",
            "LinfPGD Attack: Average loss: 0.0049, Robust Accuracy: 89%)\n",
            "Average accuracy: 91.25, female: 91.51, male: 90.98\n",
            "Train Epoch: 9 [00064/39936 (1%)]\t Loss: 0.222063\n",
            "Train Epoch: 9 [03264/39936 (33%)]\t Loss: 0.290335\n",
            "Train Epoch: 9 [06464/39936 (65%)]\t Loss: 0.308283\n",
            "Train Epoch: 9 [09664/39936 (97%)]\t Loss: 0.212692\n",
            "Attack success rate: 0.36%\n",
            "LinfPGD Attack: Average loss: 0.0041, Robust Accuracy: 91%)\n",
            "Attack success rate: 0.56%\n",
            "LinfPGD Attack: Average loss: 0.0044, Robust Accuracy: 89%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 10 [00064/39936 (1%)]\t Loss: 0.226738\n",
            "Train Epoch: 10 [03264/39936 (33%)]\t Loss: 0.239126\n",
            "Train Epoch: 10 [06464/39936 (65%)]\t Loss: 0.219294\n",
            "Train Epoch: 10 [09664/39936 (97%)]\t Loss: 0.349948\n",
            "Attack success rate: 0.17%\n",
            "LinfPGD Attack: Average loss: 0.0043, Robust Accuracy: 91%)\n",
            "Attack success rate: 0.00%\n",
            "LinfPGD Attack: Average loss: 0.0049, Robust Accuracy: 90%)\n",
            "Average accuracy: 90.85, female: 91.51, male: 90.19\n",
            "Train Epoch: 11 [00064/39936 (1%)]\t Loss: 0.155347\n",
            "Train Epoch: 11 [03264/39936 (33%)]\t Loss: 0.275168\n",
            "Train Epoch: 11 [06464/39936 (65%)]\t Loss: 0.177769\n",
            "Train Epoch: 11 [09664/39936 (97%)]\t Loss: 0.185613\n",
            "Attack success rate: 5.09%\n",
            "LinfPGD Attack: Average loss: 0.0060, Robust Accuracy: 80%)\n",
            "Attack success rate: 4.64%\n",
            "LinfPGD Attack: Average loss: 0.0058, Robust Accuracy: 81%)\n",
            "Average accuracy: 88.20, female: 87.80, male: 88.59\n",
            "Train Epoch: 12 [00064/39936 (1%)]\t Loss: 0.193704\n",
            "Train Epoch: 12 [03264/39936 (33%)]\t Loss: 0.228928\n",
            "Train Epoch: 12 [06464/39936 (65%)]\t Loss: 0.232327\n",
            "Train Epoch: 12 [09664/39936 (97%)]\t Loss: 0.345048\n",
            "Attack success rate: 1.60%\n",
            "LinfPGD Attack: Average loss: 0.0049, Robust Accuracy: 89%)\n",
            "Attack success rate: 1.94%\n",
            "LinfPGD Attack: Average loss: 0.0051, Robust Accuracy: 86%)\n",
            "Average accuracy: 90.05, female: 90.72, male: 89.39\n",
            "Train Epoch: 13 [00064/39936 (1%)]\t Loss: 0.195164\n",
            "Train Epoch: 13 [03264/39936 (33%)]\t Loss: 0.142509\n",
            "Train Epoch: 13 [06464/39936 (65%)]\t Loss: 0.136234\n",
            "Train Epoch: 13 [09664/39936 (97%)]\t Loss: 0.205475\n",
            "Attack success rate: 0.86%\n",
            "LinfPGD Attack: Average loss: 0.0041, Robust Accuracy: 91%)\n",
            "Attack success rate: 2.07%\n",
            "LinfPGD Attack: Average loss: 0.0043, Robust Accuracy: 88%)\n",
            "Average accuracy: 90.98, female: 91.78, male: 90.19\n",
            "Train Epoch: 14 [00064/39936 (1%)]\t Loss: 0.189480\n",
            "Train Epoch: 14 [03264/39936 (33%)]\t Loss: 0.221955\n",
            "Train Epoch: 14 [06464/39936 (65%)]\t Loss: 0.272635\n",
            "Train Epoch: 14 [09664/39936 (97%)]\t Loss: 0.189505\n",
            "Attack success rate: 5.83%\n",
            "LinfPGD Attack: Average loss: 0.0064, Robust Accuracy: 78%)\n",
            "Attack success rate: 6.11%\n",
            "LinfPGD Attack: Average loss: 0.0068, Robust Accuracy: 76%)\n",
            "Average accuracy: 86.60, female: 87.80, male: 85.41\n",
            "Train Epoch: 15 [00064/39936 (1%)]\t Loss: 0.321904\n",
            "Train Epoch: 15 [03264/39936 (33%)]\t Loss: 0.474369\n",
            "Train Epoch: 15 [06464/39936 (65%)]\t Loss: 0.197424\n",
            "Train Epoch: 15 [09664/39936 (97%)]\t Loss: 0.223393\n",
            "Attack success rate: 0.87%\n",
            "LinfPGD Attack: Average loss: 0.0039, Robust Accuracy: 90%)\n",
            "Attack success rate: 1.87%\n",
            "LinfPGD Attack: Average loss: 0.0043, Robust Accuracy: 86%)\n",
            "Average accuracy: 91.51, female: 92.31, male: 90.72\n",
            "Train Epoch: 16 [00064/39936 (1%)]\t Loss: 0.409468\n",
            "Train Epoch: 16 [03264/39936 (33%)]\t Loss: 0.127078\n",
            "Train Epoch: 16 [06464/39936 (65%)]\t Loss: 0.175666\n",
            "Train Epoch: 16 [09664/39936 (97%)]\t Loss: 0.172300\n",
            "Attack success rate: 1.05%\n",
            "LinfPGD Attack: Average loss: 0.0042, Robust Accuracy: 90%)\n",
            "Attack success rate: 1.43%\n",
            "LinfPGD Attack: Average loss: 0.0043, Robust Accuracy: 89%)\n",
            "Average accuracy: 91.64, female: 91.78, male: 91.51\n",
            "Train Epoch: 17 [00064/39936 (1%)]\t Loss: 0.148896\n",
            "Train Epoch: 17 [03264/39936 (33%)]\t Loss: 0.303545\n",
            "Train Epoch: 17 [06464/39936 (65%)]\t Loss: 0.183412\n",
            "Train Epoch: 17 [09664/39936 (97%)]\t Loss: 0.306450\n",
            "Attack success rate: 1.24%\n",
            "LinfPGD Attack: Average loss: 0.0043, Robust Accuracy: 90%)\n",
            "Attack success rate: 1.05%\n",
            "LinfPGD Attack: Average loss: 0.0046, Robust Accuracy: 88%)\n",
            "Average accuracy: 91.25, female: 92.31, male: 90.19\n",
            "Train Epoch: 18 [00064/39936 (1%)]\t Loss: 0.193627\n",
            "Train Epoch: 18 [03264/39936 (33%)]\t Loss: 0.186474\n",
            "Train Epoch: 18 [06464/39936 (65%)]\t Loss: 0.245968\n",
            "Train Epoch: 18 [09664/39936 (97%)]\t Loss: 0.180790\n",
            "Attack success rate: 5.10%\n",
            "LinfPGD Attack: Average loss: 0.0048, Robust Accuracy: 86%)\n",
            "Attack success rate: 3.92%\n",
            "LinfPGD Attack: Average loss: 0.0051, Robust Accuracy: 86%)\n",
            "Average accuracy: 92.04, female: 93.90, male: 90.19\n",
            "Train Epoch: 19 [00064/39936 (1%)]\t Loss: 0.148824\n",
            "Train Epoch: 19 [03264/39936 (33%)]\t Loss: 0.228155\n",
            "Train Epoch: 19 [06464/39936 (65%)]\t Loss: 0.213147\n",
            "Train Epoch: 19 [09664/39936 (97%)]\t Loss: 0.153583\n",
            "Attack success rate: 2.33%\n",
            "LinfPGD Attack: Average loss: 0.0050, Robust Accuracy: 87%)\n",
            "Attack success rate: 3.65%\n",
            "LinfPGD Attack: Average loss: 0.0052, Robust Accuracy: 84%)\n",
            "Average accuracy: 90.45, female: 90.98, male: 89.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6175MiPJr5en"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}