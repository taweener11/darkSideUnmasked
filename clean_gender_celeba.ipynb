{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taweener11/darkSideUnmasked/blob/main/clean_gender_celeba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYi8w2zpnKKE",
        "outputId": "a62e9c5b-8428-4399-fc4b-16c8b9eea14c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cores = os.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrb_gzkT-CCv",
        "outputId": "db260847-6537-4252-e18d-eab34486d5f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8TM7bpCk3kNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title shell pipeline for unzipping! this needs to run every time\n",
        "\n",
        "!unzip -q \"/content/drive/My Drive/Datasets/celeba/img_align_celeba.zip\" -d \"/content/celeba/\""
      ],
      "metadata": {
        "id": "SS8uNZYunxqx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content' # setting it to the local environment"
      ],
      "metadata": {
        "id": "4uA1b3RswNS5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "cn679gCpx6pD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a transform that is smaller per suggestion of rasmus\n",
        "\n",
        "image_size = 64\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                          std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "oLYdzNAHyveC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfering files from gdrive to here so that they would work without us uploading manually all the time\n",
        "# import module\n",
        "import shutil\n",
        "\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/identity_CelebA.txt', '/content/celeba/identity_CelebA.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_attr_celeba.txt', '/content/celeba/list_attr_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_bbox_celeba.txt', '/content/celeba/list_bbox_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_landmarks_align_celeba.txt', '/content/celeba/list_landmarks_align_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_eval_partition.txt', '/content/celeba/list_eval_partition.txt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6CEeXlqPztTx",
        "outputId": "41399dda-9531-4212-fe0c-84c8241186e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/celeba/list_eval_partition.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CelebA\n",
        "\n",
        "\n",
        "# it creates a folder on the go!\n",
        "\n",
        "try:\n",
        "    dataset = CelebA(\n",
        "        root='/content',\n",
        "        split='train',\n",
        "        target_type='attr',\n",
        "        transform=transform,\n",
        "        download=False # this works now!!!! its just important that it is in the root folder\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"CelebA error:\", e)"
      ],
      "metadata": {
        "id": "iyQji5uMx2yu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = '/content/celeba'\n",
        "\n",
        "print(\"Root contents:\", os.listdir(data_dir))\n",
        "print(\"Images folder exists:\", os.path.isdir(os.path.join(data_dir, 'img_align_celeba')))\n",
        "print(\"Sample images:\", os.listdir(os.path.join(data_dir, 'img_align_celeba'))[:3])\n",
        "print(\"Has attribute file:\", os.path.isfile(os.path.join(data_dir, 'list_attr_celeba.txt')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jz1k1xJybHu",
        "outputId": "cc86bd6a-ea48-40ee-d7e8-ca58336a8dac",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root contents: ['img_align_celeba', 'list_bbox_celeba.txt', 'identity_CelebA.txt', 'list_landmarks_align_celeba.txt', 'list_attr_celeba.txt', 'list_eval_partition.txt']\n",
            "Images folder exists: True\n",
            "Sample images: ['053361.jpg', '189109.jpg', '130880.jpg']\n",
            "Has attribute file: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check 2 & the moment of truth!!\n",
        "\n",
        "# adding a dataloader and a basic model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "tw8aIHtsyj0Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title pipeline for wandb\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "id": "5WRIPY5ESbcx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting the training data, different distributions\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "test_dataset = CelebA(\n",
        "    root='/content',\n",
        "    split='test',\n",
        "    target_type='attr',\n",
        "    transform=transform,\n",
        "    download=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "ABpA6NKeQYRI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "# Get the identity information from the training dataset\n",
        "identity_labels = dataset.identity\n",
        "# Convert to a pandas Series for easier counting\n",
        "identity_series = pd.Series(identity_labels.squeeze().numpy())\n",
        "identity_counts = identity_series.value_counts()\n",
        "top_1000_identities = identity_counts.nlargest(1000)\n",
        "# Get the indices corresponding to the top 1000 identities\n",
        "top_1000_indices = identity_series[identity_series.isin(top_1000_identities.index)].index\n",
        "# Create a subset of the dataset containing only the top 1000 identities\n",
        "dataset_top_1000 = Subset(dataset, top_1000_indices)\n",
        "\n",
        "\n",
        "min_samples = top_1000_identities.min()\n",
        "max_samples = top_1000_identities.max()\n",
        "\n",
        "print(f\"Minimum samples per identity: {min_samples}\")\n",
        "print(f\"Maximum samples per identity: {max_samples}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIYrBM44Fd_0",
        "outputId": "f0f52979-4856-4809-b026-ddb76523b719"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum samples per identity: 30\n",
            "Maximum samples per identity: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "gender_labels_test_subset = []\n",
        "for i in top_1000_indices:\n",
        "  # Note: As discussed before, using training indices on the test dataset\n",
        "  # might lead to issues or misalignment. Assuming this is intended for now.\n",
        "  if i < len(test_dataset):\n",
        "    gender_labels_test_subset.append(test_dataset.attr[i, male_idx])\n",
        "\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "gender_labels_test_subset_np = np.array(gender_labels_test_subset)\n",
        "\n",
        "\n",
        "# Now use np.where on the NumPy array\n",
        "# This is the part that fixes the DeprecationWarning\n",
        "female_test_subset_indices = np.where(gender_labels_test_subset_np == 0)[0]\n",
        "male_test_subset_indices   = np.where(gender_labels_test_subset_np ==  1)[0]\n",
        "\n",
        "\n",
        "print(len(female_test_subset_indices))\n",
        "print(len(male_test_subset_indices))\n",
        "\n",
        "\n",
        "N_test = min(len(female_test_subset_indices), len(male_test_subset_indices))\n",
        "\n",
        "rng_test = np.random.default_rng(seed=42)\n",
        "shuffled_female_test_subset_indices = np.copy(female_test_subset_indices)\n",
        "shuffled_male_test_subset_indices   = np.copy(male_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_female_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_male_test_subset_indices)\n",
        "\n",
        "\n",
        "test_subsets = {}\n",
        "\n",
        "# Create training subsets\n",
        "test_subsets_f = {}\n",
        "test_subsets_m = {}\n",
        "# even split for all examples. we can change this later but we want to be able to generalize... we want there to be the same number of examples for men and women and for these to be in the same set...\n",
        "# we will put this to the loop.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkTrbz2Az2Zy",
        "outputId": "e65cc17a-8237-4b57-fea3-80cee4c1d658"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300\n",
            "1510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title pipeline for # of classes subsetting\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "# choose smallest n\n",
        "# proportions = [0, 0.1, 0.25, 0.5, 0.75, 1.0] # changed this bc it doesn't make sense\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "\n",
        "male_idx_train = dataset.attr_names.index('Male')\n",
        "gender_labels_train_subset = dataset.attr[top_1000_indices, male_idx_train] # gender from training dataset\n",
        "female_train_subset_indices = np.where(gender_labels_train_subset == 0)[0]\n",
        "male_train_subset_indices   = np.where(gender_labels_train_subset ==  1)[0]\n",
        "\n",
        "N_train = min(len(female_train_subset_indices), len(male_train_subset_indices))\n",
        "\n",
        "rng_train = np.random.default_rng(seed=42)\n",
        "shuffled_female_train_subset_indices = np.copy(female_train_subset_indices)\n",
        "shuffled_male_train_subset_indices   = np.copy(male_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_female_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_male_train_subset_indices)\n",
        "\n",
        "\n",
        "# training subsets\n",
        "train_subsets = {}\n",
        "for p in proportions:\n",
        "    num_females_train = int(N_train * p)\n",
        "    num_males_train = N_train - num_females_train\n",
        "\n",
        "    q = min(p, 1-p)\n",
        "    num_females_test = int(N_test * q) # even split for testing\n",
        "    num_males_test = num_females_test\n",
        "\n",
        "    chosen_female_train = shuffled_female_train_subset_indices[:num_females_train] if num_females_train > 0 else np.array([], dtype=int)\n",
        "    chosen_male_train   = shuffled_male_train_subset_indices[:num_males_train]   if num_males_train > 0   else np.array([], dtype=int)\n",
        "\n",
        "    chosen_female_test = shuffled_female_test_subset_indices[:num_females_test]\n",
        "    chosen_male_test   = shuffled_male_test_subset_indices[:num_males_test]\n",
        "\n",
        "    # these indices are relative to the 'dataset_top_1000' subset,\n",
        "    # so we need to map them back to the original 'dataset' indices if Subset requires it.\n",
        "    # since top_1000_indices is the mapping, we can directly use that:\n",
        "    original_indices_train = np.concatenate([\n",
        "        top_1000_indices[chosen_female_train],\n",
        "        top_1000_indices[chosen_male_train]\n",
        "    ]).astype(int)\n",
        "    rng_train.shuffle(original_indices_train)\n",
        "    train_subsets[p] = Subset(dataset, original_indices_train)\n",
        "    test_subsets_f[p] = Subset(test_dataset, chosen_female_test)\n",
        "    test_subsets_m[p] = Subset(test_dataset, chosen_male_test)\n",
        "\n",
        "\n",
        "\n",
        "# Verification as before\n",
        "for p in proportions:\n",
        "    # Verification for the training subsets\n",
        "    indices_train = train_subsets[p].indices\n",
        "    # Need to get genders for these original training indices from the *full* training dataset\n",
        "    genders_train = dataset.attr[indices_train, male_idx_train]\n",
        "    percent_female_train = (genders_train == 0).sum()/len(indices_train) if len(indices_train) > 0 else 0\n",
        "    print(f\"Train Subset (Prop {int(p*100)}%): Target {int(p*100)}% -- Actual {percent_female_train*100:.2f}% females, {(genders_train == 0).sum()} samples\")\n",
        "\n",
        "\n",
        "    number_female_test = len(test_subsets_f[p].indices)\n",
        "    number_male_test = len(test_subsets_m[p].indices)\n",
        "    print(f\"Number of female test samples: {number_female_test}\")\n",
        "    print(f\"Number of male test samples: {number_male_test}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2x4LIJezqg1",
        "outputId": "bcc5dea6-74f5-40b5-f56f-d2622ae98cd6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Subset (Prop 25%): Target 25% -- Actual 24.99% females, 2480 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n",
            "Train Subset (Prop 50%): Target 50% -- Actual 50.00% females, 4961 samples\n",
            "Number of female test samples: 755\n",
            "Number of male test samples: 755\n",
            "Train Subset (Prop 75%): Target 75% -- Actual 74.99% females, 7441 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title pipeline for class-based subsetting\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# we know all classes have around 30 examples each\n",
        "# this could give us a split of 10/30, 20/20, 30/10\n",
        "# for the test examples we dont care bc all the class indices are going to be there anyways\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "# index by the even split dataset\n",
        "\n",
        "indices_train = train_subsets[0.5].indices\n",
        "# Need to get genders for these original training indices from the *full* training dataset\n",
        "genders_train = dataset.attr[indices_train, male_idx_train]\n",
        "percent_female_train = (genders_train == 0).sum()/len(indices_train) if len(indices_train) > 0 else 0\n",
        "print(f\"Train Subset (Prop {int(0.5*100)}%): Target {int(0.5*100)}% -- Actual {percent_female_train*100:.2f}% females, {(genders_train == 0).sum()} samples\")\n",
        "\n",
        "# Assumptions for your dataset (update if needed)\n",
        "labels = np.array(dataset.identity[train_subsets[0.5].indices]).squeeze()\n",
        "\n",
        "\n",
        "num_classes = len(np.unique(labels))\n",
        "print(num_classes)\n",
        "\n",
        "# Get the index for the 'Male' attribute in the dataset\n",
        "male_idx_train = dataset.attr_names.index('Male')\n",
        "# we also need the gender labels for the instances within dataset_top_1000,\n",
        "# accessed from the original dataset's attributes using the subset indices.\n",
        "gender_labels_top_1000 = dataset.attr[train_subsets[0.5].indices, male_idx_train].squeeze().numpy()\n",
        "\n",
        "\n",
        "train_females = {}\n",
        "train_males = {}\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "base_number = 30  # everything has more than 30 examples\n",
        "\n",
        "\n",
        "\n",
        "train_subsets_new={}\n",
        "\n",
        "for prop in proportions:\n",
        "    selected_male_indices = []\n",
        "    selected_female_indices = []\n",
        "    # the indices 'c' here refer to the unique identity classes within the top 1000.\n",
        "    for c in np.unique(labels):\n",
        "        # indices within the `train_subsets[0.5]` even split by gender array\n",
        "        # that correspond to class 'c' AND are female\n",
        "        female_class_subset_indices = np.where((labels == c) & (gender_labels_top_1000 == 0))[0]\n",
        "\n",
        "        rng.shuffle(female_class_subset_indices)\n",
        "\n",
        "        if len(female_class_subset_indices) < base_number:\n",
        "            n_female = int(np.floor(len(female_class_subset_indices) * prop))\n",
        "        else:\n",
        "            n_female = int(np.floor(base_number * prop))\n",
        "\n",
        "        # n_female = int(np.floor(len(female_class_subset_indices) * prop))\n",
        "\n",
        "        # get the original dataset indices for these selected items\n",
        "        original_female_indices = train_subsets[0.5].indices[female_class_subset_indices[:n_female]]\n",
        "        selected_female_indices.extend(original_female_indices)\n",
        "\n",
        "        # indices within the `train_subsets[0.5]` even split by gender array\n",
        "        # that correspond to class 'c' AND are male\n",
        "        male_class_subset_indices = np.where((labels == c) & (gender_labels_top_1000 == 1))[0]\n",
        "        rng.shuffle(male_class_subset_indices)\n",
        "        n_male = int(np.floor(len(male_class_subset_indices) * (1-prop)))\n",
        "\n",
        "        if len(male_class_subset_indices) < base_number:\n",
        "            n_male = int(np.floor(len(male_class_subset_indices) * (1-prop)))\n",
        "        else:\n",
        "            n_male = int(np.floor(base_number * (1-prop)))\n",
        "\n",
        "\n",
        "        # Get the original dataset indices for these selected items\n",
        "        original_male_indices = train_subsets[0.5].indices[male_class_subset_indices[:n_male]]\n",
        "        selected_male_indices.extend(original_male_indices)\n",
        "\n",
        "\n",
        "    # make subsets (using original dataset and selected original indices)\n",
        "\n",
        "    original_indices_train = np.concatenate([\n",
        "        top_1000_indices[chosen_female_train],\n",
        "        top_1000_indices[chosen_male_train]\n",
        "    ]).astype(int)\n",
        "    rng_train.shuffle(original_indices_train)\n",
        "    train_subsets_new[prop] = Subset(dataset, original_indices_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayZ3vIgx9xqI",
        "outputId": "8780e0d8-82e4-4495-b65f-20892a4e8e2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Subset (Prop 50%): Target 50% -- Actual 50.00% females, 4961 samples\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title test datasets!!! now since we have all the classes we can just use the regular dataset\n",
        "\n",
        "\n",
        "# print(len(shuffled_female_test_subset_indices))\n",
        "# print(len(shuffled_male_test_subset_indices))\n",
        "\n",
        "N_test = min(len(female_test_subset_indices), len(male_test_subset_indices))\n",
        "\n",
        "test_subsets_f = {}\n",
        "test_subsets_m = {}\n",
        "\n",
        "chosen_female_test = shuffled_female_test_subset_indices[:N_test]\n",
        "chosen_male_test   = shuffled_male_test_subset_indices[:N_test]\n",
        "\n",
        "\n",
        "for p in proportions:\n",
        "    test_subsets_f[p] = Subset(test_dataset, chosen_female_test)\n",
        "    test_subsets_m[p] = Subset(test_dataset, chosen_male_test)\n"
      ],
      "metadata": {
        "id": "KumEL3yoXY_f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# train_loader = DataLoader(train_subsets[0.5], batch_size=batch_size, shuffle=True)\n",
        "# val_loader = DataLoader(test_subsets[0.5], batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "V4HgisJbFAtR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "goy8JCnuN5y8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## module for resnet-18"
      ],
      "metadata": {
        "id": "a2MxhhfvS2CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title putting in the utils here for easier dev\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "V5RyGgI-xDUG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=8/2550,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ozdMyUbKyike"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Selecting the first column of y (assuming it's the identity label)\n",
        "                loss = nn.CrossEntropyLoss()(output, y[:, 0])\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "P0OJ1YNLy6DT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initializing a run\n",
        "\n",
        "# api key: bd1c08839d0c8c49e7c3efe9aabe2d9c644befb6\n",
        "\n",
        "wandb.init(project=\"face-adv-fairness\", name=\"celeba-balanced-datasets\", config={\"learning_rate\": 0.001, \"epochs\": 30})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "HIWp5na8TIuD",
        "outputId": "f6846f56-9294-4206-dc61-5ad66d410c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33midilks\u001b[0m (\u001b[33midilks-dartmouth\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250531_221532-t0lcemcw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/t0lcemcw' target=\"_blank\">celeba-balanced-datasets</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/t0lcemcw' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/t0lcemcw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/t0lcemcw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7cffb5eab510>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Selecting the first column of targets, assuming it represents the identity label\n",
        "        labels = targets[:, 0]\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train':  # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train_trades':  # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "            wandb.log({\"training_loss\": loss.item(), })"
      ],
      "metadata": {
        "id": "BPenGmbpy_C0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### resnet 18"
      ],
      "metadata": {
        "id": "ID0e5sr4T8ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting this with a simpler model\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                loss = nn.CrossEntropyLoss()(output, y)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "iI3HBGKtzrWA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "def eval_test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n",
        "\n",
        "def mixup_data(x, y, mixup_alpha=1.0):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=0.003,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Lp-OlBD7pJmB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        labels = targets[:, 0] # the first column is the identity label\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer)\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "id": "fKvyFN-QiJQD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title resnet module\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n"
      ],
      "metadata": {
        "id": "4nLHgl8ekD7z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title implementing pgd attacks\n",
        "\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = y[:, 0] # Assuming the first column is the identity label\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "                loss = nn.CrossEntropyLoss()(output, labels)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "fLKbosPFkqTN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title modified train and test functions for celeba\n",
        "\n",
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = targets[:, 0] # Assuming the first column is the identity label\n",
        "\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            # Pass the original multi-dimensional targets to the attack\n",
        "            adv_x = pgd_attack(inputs, targets) # The attack will extract labels internally\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=labels, optimizer=optimizer)\n",
        "\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels.\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, labels)\n",
        "        #     adv_x = pgd_attack(inputs, targets) # Pass original targets to attack\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels from adv_x?\n",
        "        #     # This part of mixup with adversarial training might need careful consideration of how targets are handled.\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, labels) # Using extracted labels\n",
        "\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     # Use the extracted 1D labels for criterion\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "            wandb.log({f\"train_loss {train_loader.dataset}\": loss.item()}, step=epoch)\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Using Adam as in your failing block, but only for model\n",
        "\n",
        "    best_acc = 0.0 # Keep track of best average accuracy across genders\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        # Pass the extracted labels in train_ep as modified above\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "        val_acc_f = 0.0\n",
        "        val_acc_m = 0.0\n",
        "        val_loss_f = 0.0\n",
        "        val_loss_m = 0.0\n",
        "\n",
        "        if val_loader_f and len(val_loader_f.dataset) > 0:\n",
        "            val_loss_f, val_acc_f = eval_test_celeba(model, val_loader_f, device, name = 'female')\n",
        "            robust_loss_f, robust_accuracy_f = eval_robust_celeba(model, val_loader_f, pgd, device, name='female', epoch = epoch)\n",
        "\n",
        "\n",
        "        if val_loader_m and len(val_loader_m.dataset) > 0:\n",
        "            val_loss_m, val_acc_m = eval_test_celeba(model, val_loader_m, device, name = 'male')\n",
        "            robust_loss_m, robust_accuracy_m = eval_robust_celeba(model, val_loader_m, pgd, device, name = 'male', epoch = epoch)\n",
        "\n",
        "\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Average accuracy: {val_acc:.2f}, female: {val_acc_f:.2f}, male: {val_acc_m:.2f}')\n",
        "\n",
        "        wandb.log({\"val_loss_female\": val_loss_f, \"val_accuracy_female\": val_acc_f,\n",
        "               \"val_loss_male\": val_loss_m, \"val_accuracy_male\": val_acc_m,\n",
        "               \"average_val_accuracy\": val_acc}, step=epoch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval_test_celeba(model, dataloader, device, name):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # Extract identity label\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, labels).item() * inputs.size(0)\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "    test_loss /= total if total > 0 else 1\n",
        "    accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    # print(f'Test: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)')\n",
        "    # wandb.log(f\"clean_test_loss {name}: {test_loss}\", step=epoch)\n",
        "    # wandb.log(f\"clean_test_accuracy {name}: {accuracy}\", step=epoch)\n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "# convenience funtion to log predictions for a batch of test images\n",
        "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):\n",
        "  # obtain confidence scores for all classes\n",
        "  scores = F.softmax(outputs.data, dim=1)\n",
        "  log_scores = scores.cpu().numpy()\n",
        "  log_images = images.cpu().numpy()\n",
        "  log_labels = labels.cpu().numpy()\n",
        "  log_preds = predicted.cpu().numpy()\n",
        "  # adding ids based on the order of the images\n",
        "  _id = 0\n",
        "  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):\n",
        "    # add required info to data table:\n",
        "    # id, image pixels, model's guess, true label, scores for all classes\n",
        "    img_id = str(_id) + \"_\" + str(log_counter)\n",
        "    test_table.add_data(img_id, wandb.Image(i), p, l, *s)\n",
        "    _id += 1\n",
        "    if _id == batch_size:\n",
        "      break\n",
        "\n",
        "\n",
        "NUM_BATCHES_TO_LOG = 10\n",
        "\n",
        "def eval_robust_celeba(model, dataloader, pgd_attack, device, name, epoch):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    success_count = 0\n",
        "    log_counter = 0\n",
        "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
        "    for image_id in dataloader.dataset.indices:\n",
        "      columns.append(\"score_\" + str(image_id))\n",
        "    test_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # extract identity label\n",
        "\n",
        "            outputs_clean = model(inputs)\n",
        "            pred_clean = outputs_clean.max(1, keepdim=True)[1]\n",
        "\n",
        "\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs_adv = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs_adv, labels).item()\n",
        "            pred_adv = outputs_adv.max(1, keepdim=True)[1]\n",
        "            correct += pred_adv.eq(labels.view_as(pred_adv)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "\n",
        "            if log_counter < NUM_BATCHES_TO_LOG:\n",
        "              log_test_predictions(inputs, labels, outputs_adv, pred_adv, test_table, log_counter)\n",
        "              log_counter += 1\n",
        "\n",
        "            # keeping track of successful attacks\n",
        "            mask = pred_clean == labels\n",
        "            succesful_attacks = (pred_adv != labels) & mask\n",
        "            success_count += succesful_attacks.sum().item()\n",
        "\n",
        "\n",
        "\n",
        "    attack_success_rate = success_count / correct if correct > 0 else 0\n",
        "    print(f'Attack success rate: {attack_success_rate:.2f}%')\n",
        "    robust_loss /= len(dataloader.dataset) if total > 0 else 1\n",
        "    robust_accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    print(f'LinfPGD Attack: Average loss: {robust_loss:.4f}, Robust Accuracy: {robust_accuracy:.0f}%)')\n",
        "\n",
        "    wandb.log({f\"robust_loss_{name}\": robust_loss}, step=epoch)\n",
        "    wandb.log({f\"robust_accuracy_{name}\": robust_accuracy}, step=epoch)\n",
        "    wandb.log({f\"attack_success_rate_{name}\": attack_success_rate}, step=epoch)\n",
        "\n",
        "\n",
        "    # ✨ W&B: Log predictions table to wandb\n",
        "    wandb.log({\"test_predictions\" : test_table})\n",
        "\n",
        "    return robust_loss, robust_accuracy\n"
      ],
      "metadata": {
        "id": "aZbU70I7gShk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title small sanity check\n",
        "\n",
        "wandb.init(project=\"face-adv-fairness\", name=\"celeba-sanity-check\", config={\"learning_rate\": 0.001, \"epochs\": 1})\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = ResNet18(num_classes=1000).to(device) # ResNet for identity classification\n",
        "val_loader_f = DataLoader(test_subsets_f[0.25], batch_size=64, shuffle=False) # Shuffle usually False for validation\n",
        "val_loader_m = DataLoader(test_subsets_m[0.25], batch_size=64, shuffle=False) # Shuffle usually False for validation\n",
        "pgd = LinfPGDAttack(model, epsilon=8/255, step_size = 2/255, steps = 10)\n",
        "\n",
        "robust_loss, robust_accuracy = eval_robust_celeba(model, val_loader_f, pgd, device, name = 'female', epoch = 0)"
      ],
      "metadata": {
        "id": "pqwPuQ6xvxYA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training run: old\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "batch_size = 64\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=1000).to(device) # ResNet for identity classification\n",
        "    # Note: number of classes (1000) should match the number of unique identities\n",
        "    # we filtered initially by top 1000 identitites but this might be limiting perhaps?\n",
        "    # it gives very few examples on the test set\n",
        "    # make a new run for each example\n",
        "    wandb.init(project=\"face-adv-fairness\", name=f\"celeba-gender-{proportion}\", config={\"learning_rate\": 0.001, \"epochs\": 30})\n",
        "\n",
        "\n",
        "    num_identity_classes = 1000 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    if proportion in test_subsets_f and len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "    if proportion in test_subsets_m and len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "\n",
        "\n",
        "    # call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader_f=val_loader_f, val_loader_m=val_loader_m,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=20) # Save checkpoints with proportion\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-THEtjHpv_T_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convenience funtion to log predictions for a batch of test images\n",
        "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):\n",
        "  # obtain confidence scores for all classes\n",
        "  scores = F.softmax(outputs.data, dim=1)\n",
        "  log_scores = scores.cpu().numpy()\n",
        "  log_images = images.cpu().numpy()\n",
        "  log_labels = labels.cpu().numpy()\n",
        "  log_preds = predicted.cpu().numpy()\n",
        "  # adding ids based on the order of the images\n",
        "  _id = 0\n",
        "  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):\n",
        "    # Transpose image dimensions from (C, H, W) to (H, W, C) for wandb.Image\n",
        "    i_transposed = np.transpose(i, (1, 2, 0))\n",
        "\n",
        "    # add required info to data table:\n",
        "    # id, image pixels, model's guess, true label, scores for all classes\n",
        "    img_id = str(_id) + \"_\" + str(log_counter)\n",
        "    # Use the transposed image data\n",
        "    test_table.add_data(img_id, wandb.Image(i_transposed), p, l, *s)\n",
        "    _id += 1\n",
        "    if _id == batch_size:\n",
        "      break"
      ],
      "metadata": {
        "id": "QclN7fi5tVWy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title modified train and test functions for celeba\n",
        "\n",
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = targets[:, 0] # Assuming the first column is the identity label\n",
        "\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            # Pass the original multi-dimensional targets to the attack\n",
        "            # The attack will extract the identity label internally using targets[:, 0]\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            # For trades_loss, you need to pass the identity labels as it's used directly in the loss calculation.\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=labels, optimizer=optimizer)\n",
        "\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels.\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, labels)\n",
        "        #     # Pass original targets to attack\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels from adv_x?\n",
        "        #     # This part of mixup with adversarial training might need careful consideration of how targets are handled.\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, labels) # Using extracted labels\n",
        "\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     # Use the extracted 1D labels for criterion\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "            # It seems like train_loader.dataset is a Subset, not a string name.\n",
        "            # Using the proportion from the loop might be better.\n",
        "            # You might need to pass the proportion to train_ep or handle logging outside this loop.\n",
        "            # For now, removing the dataset name from log key to avoid issues.\n",
        "            wandb.log({\"train_loss\": loss.item()}, step=epoch)\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Using Adam as in your failing block, but only for model\n",
        "\n",
        "    best_acc = 0.0 # Keep track of best average accuracy across genders\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        # Pass the extracted labels in train_ep as modified above\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "        val_acc_f = 0.0\n",
        "        val_acc_m = 0.0\n",
        "        val_loss_f = 0.0\n",
        "        val_loss_m = 0.0\n",
        "\n",
        "        # Get the number of output classes from the model's linear layer\n",
        "        num_classes = model.linear.out_features\n",
        "\n",
        "        if val_loader_f and len(val_loader_f.dataset) > 0:\n",
        "            val_loss_f, val_acc_f = eval_test_celeba(model, val_loader_f, device, name = 'female', epoch = epoch)\n",
        "            # Pass num_classes to eval_robust_celeba\n",
        "            robust_loss_f, robust_accuracy_f = eval_robust_celeba(model, val_loader_f, pgd_attack, device, name='female', epoch = epoch, num_classes=num_classes)\n",
        "\n",
        "\n",
        "        if val_loader_m and len(val_loader_m.dataset) > 0:\n",
        "            val_loss_m, val_acc_m = eval_test_celeba(model, val_loader_m, device, name = 'male', epoch = epoch)\n",
        "            # Pass num_classes to eval_robust_celeba\n",
        "            robust_loss_m, robust_accuracy_m = eval_robust_celeba(model, val_loader_m, pgd_attack, device, name = 'male', epoch = epoch, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Average accuracy: {val_acc:.2f}, female: {val_acc_f:.2f}, male: {val_acc_m:.2f}')\n",
        "\n",
        "        wandb.log({\"val_loss_female\": val_loss_f, \"val_accuracy_female\": val_acc_f,\n",
        "               \"val_loss_male\": val_loss_m, \"val_accuracy_male\": val_acc_m,\n",
        "               \"average_val_accuracy\": val_acc}, step=epoch)\n",
        "\n",
        "\n",
        "def eval_test_celeba(model, dataloader, device, name, epoch): # Added epoch parameter for logging\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # Extract identity label\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, labels).item() * inputs.size(0)\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "    test_loss /= total if total > 0 else 1\n",
        "    accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "    print(f'Test {name}: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)')\n",
        "    # Log clean test loss and accuracy\n",
        "    wandb.log({f\"clean_test_loss_{name}\": test_loss}, step=epoch)\n",
        "    wandb.log({f\"clean_test_accuracy_{name}\": accuracy}, step=epoch)\n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "# convenience funtion to log predictions for a batch of test images\n",
        "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter, batch_size_for_log):\n",
        "  # obtain confidence scores for all classes\n",
        "  scores = F.softmax(outputs.data, dim=1)\n",
        "  log_scores = scores.cpu().numpy()\n",
        "  log_images = images.cpu().numpy()\n",
        "  log_labels = labels.cpu().numpy()\n",
        "  log_preds = predicted.cpu().numpy()\n",
        "  # adding ids based on the order of the images\n",
        "  _id = 0\n",
        "  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):\n",
        "    # Transpose image dimensions from (C, H, W) to (H, W, C) for wandb.Image\n",
        "    i_transposed = np.transpose(i, (1, 2, 0))\n",
        "\n",
        "    # add required info to data table:\n",
        "    # id, image pixels, model's guess, true label, scores for all classes\n",
        "    img_id = str(_id) + \"_\" + str(log_counter)\n",
        "    # Use the transposed image data\n",
        "    test_table.add_data(img_id, wandb.Image(i_transposed), p, l, *s)\n",
        "    _id += 1\n",
        "    # Use the provided batch_size_for_log for comparison\n",
        "    if _id == batch_size_for_log:\n",
        "      break\n",
        "\n",
        "\n",
        "NUM_BATCHES_TO_LOG = 10\n",
        "\n",
        "# Added num_classes parameter\n",
        "def eval_robust_celeba(model, dataloader, pgd_attack, device, name, epoch, num_classes):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    success_count = 0\n",
        "    log_counter = 0\n",
        "    # Initialize columns based on the number of classes\n",
        "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
        "    for i in range(num_classes):\n",
        "        columns.append(f\"score_{i}\")\n",
        "    test_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            labels = targets[:, 0] # extract identity label\n",
        "\n",
        "            outputs_clean = model(inputs)\n",
        "            pred_clean = outputs_clean.max(1, keepdim=True)[1]\n",
        "\n",
        "            # Pass the original targets to the attack\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs_adv = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs_adv, labels).item() * inputs.size(0)\n",
        "            pred_adv = outputs_adv.max(1, keepdim=True)[1]\n",
        "            correct += pred_adv.eq(labels.view_as(pred_adv)).sum().item()\n",
        "            total += inputs.size(0)\n",
        "\n",
        "            if log_counter < NUM_BATCHES_TO_LOG:\n",
        "                # Pass the actual batch size of the current inputs to log_test_predictions\n",
        "                log_test_predictions(inputs, labels, outputs_adv, pred_adv, test_table, log_counter, inputs.size(0))\n",
        "                log_counter += 1\n",
        "\n",
        "\n",
        "            # keeping track of successful attacks\n",
        "            # Ensure mask uses the correct comparison (pred_clean vs labels)\n",
        "            mask = pred_clean.view_as(labels) == labels\n",
        "            succesful_attacks = (pred_adv.view_as(labels) != labels) & mask\n",
        "            success_count += succesful_attacks.sum().item()\n",
        "\n",
        "    robust_loss /= total if total > 0 else 1\n",
        "    robust_accuracy = 100. * correct / total if total > 0 else 0\n",
        "    attack_success_rate = success_count / correct if correct > 0 else 0 # Calculate attack success rate based on correct predictions\n",
        "\n",
        "\n",
        "    print(f'LinfPGD Attack {name}: Average loss: {robust_loss:.4f}, Robust Accuracy: {robust_accuracy:.0f}%)')\n",
        "    print(f'Attack success rate {name}: {attack_success_rate:.2f}%')\n",
        "\n",
        "    wandb.log({f\"robust_loss_{name}\": robust_loss}, step=epoch)\n",
        "    wandb.log({f\"robust_accuracy_{name}\": robust_accuracy}, step=epoch)\n",
        "    wandb.log({f\"attack_success_rate_{name}\": attack_success_rate}, step=epoch)\n",
        "\n",
        "\n",
        "    # ✨ W&B: Log predictions table to wandb\n",
        "    wandb.log({\"test_predictions\" : test_table}, step=epoch) # Log table at each epoch step\n",
        "\n",
        "    return robust_loss, robust_accuracy"
      ],
      "metadata": {
        "id": "XwjcH741u6uy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training run: new, with balanced datasets\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "batch_size = 64\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=999).to(device) # ResNet for identity classification\n",
        "    # Note: number of classes (1000) should match the number of unique identities\n",
        "    # it gives very few examples on the test set\n",
        "\n",
        "\n",
        "    # make a new run for each example\n",
        "    wandb.init(project=\"face-adv-fairness\", name=f\"celeba-gender-new-{proportion}\", config={\"learning_rate\": 0.001, \"epochs\": 30})\n",
        "\n",
        "\n",
        "    num_identity_classes = 999 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/20, steps = 20)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    train_loader = DataLoader(train_subsets_new[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    if proportion in test_subsets_f and len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "    if proportion in test_subsets_m and len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "\n",
        "\n",
        "    # call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader_f=val_loader_f, val_loader_m=val_loader_m,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=20) # Save checkpoints with proportion\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6175MiPJr5en",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "0bd43c08-4852-4fd3-a3c3-68b02530eb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">celeba-gender-new-0.25</strong> at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/xbj8qmty' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/xbj8qmty</a><br> View project at: <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250531_234327-xbj8qmty/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250531_234446-749l5lvy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/749l5lvy' target=\"_blank\">celeba-gender-new-0.25</a></strong> to <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/749l5lvy' target=\"_blank\">https://wandb.ai/idilks-dartmouth/face-adv-fairness/runs/749l5lvy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 6.422616\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.330793\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.324751\n",
            "Train Epoch: 0 [09664/39936 (97%)]\t Loss: 0.368291\n",
            "Test female: Average loss: 0.4151, Accuracy: 1344/1510 (89%)\n",
            "LinfPGD Attack female: Average loss: 0.4487, Robust Accuracy: 87%)\n",
            "Attack success rate female: 0.02%\n",
            "Test male: Average loss: 0.4274, Accuracy: 1327/1510 (88%)\n",
            "LinfPGD Attack male: Average loss: 0.4626, Robust Accuracy: 85%)\n",
            "Attack success rate male: 0.03%\n",
            "Average accuracy: 88.44, female: 89.01, male: 87.88\n",
            "Train Epoch: 1 [00064/39936 (1%)]\t Loss: 0.445464\n",
            "Train Epoch: 1 [03264/39936 (33%)]\t Loss: 0.383636\n",
            "Train Epoch: 1 [06464/39936 (65%)]\t Loss: 0.384255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxypbLzaZh01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}