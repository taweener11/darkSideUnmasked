{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taweener11/darkSideUnmasked/blob/main/dataset_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYi8w2zpnKKE",
        "outputId": "90db6a7d-cb91-44b3-d12d-eefa96bb283f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title defining filenames for downloading fairface\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "ROOT_DIR = '/content/drive/My Drive/Datasets/fairface'\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "\n",
        "# Filenames\n",
        "LABEL_ZIP = os.path.join(ROOT_DIR, 'fairface_label_train.zip')\n",
        "IMG_ZIP = os.path.join(ROOT_DIR, 'fairface-img-margin050-trainval.zip')\n",
        "LABEL_CSV = os.path.join(ROOT_DIR, 'fairface_label_train.csv')\n",
        "IMG_FOLDER = os.path.join(ROOT_DIR, 'train')"
      ],
      "metadata": {
        "id": "C8td5koAnUZg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title this downloads fairface! but you can also edit for other datasets\n",
        "\n",
        "!pip install -q gdown\n",
        "\n",
        "# extract file IDs from your provided links:\n",
        "# labels zip: https://drive.google.com/file/d/1i1L3Yqwaio7YSOCj7ftgk8ZZchPG7dmH/view\n",
        "# dataset zip: https://drive.google.com/file/d/1Z1RqRo0_JiavaZw2yzZG6WETdZQ8qX86/view\n",
        "\n",
        "IMG_FILE_ID   = '1Z1RqRo0_JiavaZw2yzZG6WETdZQ8qX86'\n",
        "\n",
        "# download if not already there:\n",
        "if not os.path.exists(LABEL_ZIP):\n",
        "    !gdown --id $LABEL_FILE_ID -O \"$LABEL_ZIP\"\n",
        "if not os.path.exists(IMG_ZIP):\n",
        "    !gdown --id $IMG_FILE_ID -O \"$IMG_ZIP\""
      ],
      "metadata": {
        "id": "wSFp7yLq5f1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee1ef0f-caf2-4243-9d2d-3a325a010872",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: gdown [-h] [-V] [-O OUTPUT] [-q] [--fuzzy] [--id] [--proxy PROXY]\n",
            "             [--speed SPEED] [--no-cookies] [--no-check-certificate]\n",
            "             [--continue] [--folder] [--remaining-ok] [--format FORMAT]\n",
            "             [--user-agent USER_AGENT]\n",
            "             url_or_id\n",
            "gdown: error: the following arguments are required: url_or_id\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Z1RqRo0_JiavaZw2yzZG6WETdZQ8qX86\n",
            "From (redirected): https://drive.google.com/uc?id=1Z1RqRo0_JiavaZw2yzZG6WETdZQ8qX86&confirm=t&uuid=3a9e7f2d-ca88-4d1e-8805-97cdc0e60e93\n",
            "To: /content/drive/My Drive/Datasets/fairface/fairface-img-margin050-trainval.zip\n",
            "100% 578M/578M [00:08<00:00, 72.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cores = os.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrb_gzkT-CCv",
        "outputId": "f5791513-51b0-4726-a204-2a291d85d524"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8TM7bpCk3kNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title shell pipeline for unzipping! this needs to run every time\n",
        "\n",
        "!unzip -q \"/content/drive/My Drive/Datasets/celeba/img_align_celeba.zip\" -d \"/content/celeba/\""
      ],
      "metadata": {
        "id": "SS8uNZYunxqx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content' # setting it to the local environment"
      ],
      "metadata": {
        "id": "4uA1b3RswNS5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "cn679gCpx6pD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a transform that is smaller per suggestion of rasmus\n",
        "\n",
        "image_size = 64\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                          std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "oLYdzNAHyveC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfering files from gdrive to here so that they would work without us uploading manually all the time\n",
        "# import module\n",
        "import shutil\n",
        "\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/identity_CelebA.txt', '/content/celeba/identity_CelebA.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_attr_celeba.txt', '/content/celeba/list_attr_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_bbox_celeba.txt', '/content/celeba/list_bbox_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_landmarks_align_celeba.txt', '/content/celeba/list_landmarks_align_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_eval_partition.txt', '/content/celeba/list_eval_partition.txt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6CEeXlqPztTx",
        "outputId": "693d14ad-d9c9-4987-edb9-bccc3de8ab4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/celeba/list_eval_partition.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CelebA\n",
        "\n",
        "\n",
        "# it creates a folder on the go!\n",
        "\n",
        "try:\n",
        "    dataset = CelebA(\n",
        "        root='/content',\n",
        "        split='train',\n",
        "        target_type='attr',\n",
        "        transform=transform,\n",
        "        download=False # this works now!!!! its just important that it is in the root folder\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"CelebA error:\", e)"
      ],
      "metadata": {
        "id": "iyQji5uMx2yu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = '/content/celeba'\n",
        "\n",
        "print(\"Root contents:\", os.listdir(data_dir))\n",
        "print(\"Images folder exists:\", os.path.isdir(os.path.join(data_dir, 'img_align_celeba')))\n",
        "print(\"Sample images:\", os.listdir(os.path.join(data_dir, 'img_align_celeba'))[:3])\n",
        "print(\"Has attribute file:\", os.path.isfile(os.path.join(data_dir, 'list_attr_celeba.txt')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jz1k1xJybHu",
        "outputId": "82549f4c-0c60-48db-b5a1-1f89825a4510",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root contents: ['identity_CelebA.txt', 'list_bbox_celeba.txt', 'list_attr_celeba.txt', 'list_eval_partition.txt', 'img_align_celeba', 'list_landmarks_align_celeba.txt']\n",
            "Images folder exists: True\n",
            "Sample images: ['085474.jpg', '129511.jpg', '100524.jpg']\n",
            "Has attribute file: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check 2 & the moment of truth!!\n",
        "\n",
        "# adding a dataloader and a basic model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "tw8aIHtsyj0Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting the training data, different distributions\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "test_dataset = CelebA(\n",
        "    root='/content',\n",
        "    split='test',\n",
        "    target_type='attr',\n",
        "    transform=transform,\n",
        "    download=False # i set it to true in case there is some secret metadata?? it is looking for\n",
        ")\n"
      ],
      "metadata": {
        "id": "ABpA6NKeQYRI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Get the identity information from the training dataset\n",
        "identity_labels = dataset.identity\n",
        "# Convert to a pandas Series for easier counting\n",
        "identity_series = pd.Series(identity_labels.squeeze().numpy())\n",
        "identity_counts = identity_series.value_counts()\n",
        "top_1000_identities = identity_counts.nlargest(1000)\n",
        "# Get the indices corresponding to the top 1000 identities\n",
        "top_1000_indices = identity_series[identity_series.isin(top_1000_identities.index)].index\n",
        "# Create a subset of the dataset containing only the top 1000 identities\n",
        "dataset_top_1000 = Subset(dataset, top_1000_indices)\n",
        "\n",
        "\n",
        "min_samples = top_1000_identities.min()\n",
        "max_samples = top_1000_identities.max()\n",
        "\n",
        "print(f\"Minimum samples per identity: {min_samples}\")\n",
        "print(f\"Maximum samples per identity: {max_samples}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIYrBM44Fd_0",
        "outputId": "c762cc4f-53f4-425b-eb46-508bc71e1c67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum samples per identity: 30\n",
            "Maximum samples per identity: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Make sure numpy is imported if it hasn't been already\n",
        "\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "gender_labels_test_subset = []\n",
        "for i in top_1000_indices:\n",
        "  # Note: As discussed before, using training indices on the test dataset\n",
        "  # might lead to issues or misalignment. Assuming this is intended for now.\n",
        "  if i < len(test_dataset):\n",
        "    gender_labels_test_subset.append(test_dataset.attr[i, male_idx])\n",
        "\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "gender_labels_test_subset_np = np.array(gender_labels_test_subset)\n",
        "\n",
        "\n",
        "# Now use np.where on the NumPy array\n",
        "# This is the part that fixes the DeprecationWarning\n",
        "female_test_subset_indices = np.where(gender_labels_test_subset_np == 0)[0]\n",
        "male_test_subset_indices   = np.where(gender_labels_test_subset_np ==  1)[0]\n",
        "\n",
        "\n",
        "print(len(female_test_subset_indices))\n",
        "print(len(male_test_subset_indices))\n",
        "\n",
        "\n",
        "N_test = min(len(female_test_subset_indices), len(male_test_subset_indices))\n",
        "\n",
        "rng_test = np.random.default_rng(seed=42)\n",
        "shuffled_female_test_subset_indices = np.copy(female_test_subset_indices)\n",
        "shuffled_male_test_subset_indices   = np.copy(male_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_female_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_male_test_subset_indices)\n",
        "\n",
        "\n",
        "test_subsets = {}\n",
        "\n",
        "# Create training subsets\n",
        "test_subsets_f = {}\n",
        "test_subsets_m = {}\n",
        "# even split for all examples. we can change this later but we want to be able to generalize... we want there to be the same number of examples for men and women and for these to be in the same set...\n",
        "# we will put this to the loop.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkTrbz2Az2Zy",
        "outputId": "b0b360f3-e721-452b-d72a-38ad9a555a66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300\n",
            "1510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "# choose smallest n\n",
        "# proportions = [0, 0.1, 0.25, 0.5, 0.75, 1.0] # changed this bc it doesn't make sense\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "# You need to create subsets from the test_dataset using test_dataset-specific indices\n",
        "# The previous code was creating subsets of the training dataset.\n",
        "# It seems like you want to create training subsets with varying gender proportions\n",
        "# and test subsets for evaluation (separated by gender).\n",
        "\n",
        "# For the training subsets (assuming you still want to use indices from the training dataset,\n",
        "# but with the identity filtering from before):\n",
        "# You will need to re-calculate the gender labels for the *training* dataset based on top_1000_indices.\n",
        "male_idx_train = dataset.attr_names.index('Male')\n",
        "gender_labels_train_subset = dataset.attr[top_1000_indices, male_idx_train] # Use gender from training dataset\n",
        "female_train_subset_indices = np.where(gender_labels_train_subset == 0)[0]\n",
        "male_train_subset_indices   = np.where(gender_labels_train_subset ==  1)[0]\n",
        "\n",
        "N_train = min(len(female_train_subset_indices), len(male_train_subset_indices))\n",
        "\n",
        "rng_train = np.random.default_rng(seed=42)\n",
        "shuffled_female_train_subset_indices = np.copy(female_train_subset_indices)\n",
        "shuffled_male_train_subset_indices   = np.copy(male_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_female_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_male_train_subset_indices)\n",
        "\n",
        "\n",
        "# Create training subsets\n",
        "train_subsets = {}\n",
        "for p in proportions:\n",
        "    num_females_train = int(N_train * p)\n",
        "    num_males_train = N_train - num_females_train\n",
        "\n",
        "    q = min(p, 1-p)\n",
        "    num_females_test = int(N_test * q) # even split for testing\n",
        "    num_males_test = num_females_test\n",
        "\n",
        "    chosen_female_train = shuffled_female_train_subset_indices[:num_females_train] if num_females_train > 0 else np.array([], dtype=int)\n",
        "    chosen_male_train   = shuffled_male_train_subset_indices[:num_males_train]   if num_males_train > 0   else np.array([], dtype=int)\n",
        "\n",
        "    chosen_female_test = shuffled_female_test_subset_indices[:num_females_test]\n",
        "    chosen_male_test   = shuffled_male_test_subset_indices[:num_males_test]\n",
        "\n",
        "    # These indices are relative to the 'dataset_top_1000' subset,\n",
        "    # so you need to map them back to the original 'dataset' indices if Subset requires it.\n",
        "    # Since top_1000_indices is the mapping, we can directly use that:\n",
        "    original_indices_train = np.concatenate([\n",
        "        top_1000_indices[chosen_female_train],\n",
        "        top_1000_indices[chosen_male_train]\n",
        "    ]).astype(int)\n",
        "    rng_train.shuffle(original_indices_train)\n",
        "    train_subsets[p] = Subset(dataset, original_indices_train)\n",
        "    test_subsets_f[p] = Subset(test_dataset, chosen_female_test)\n",
        "    test_subsets_m[p] = Subset(test_dataset, chosen_male_test)\n",
        "\n",
        "\n",
        "\n",
        "# Verification as before\n",
        "for p in proportions:\n",
        "    # Verification for the training subset\n",
        "    indices_train = train_subsets[p].indices\n",
        "    # Need to get genders for these original training indices from the *full* training dataset\n",
        "    genders_train = dataset.attr[indices_train, male_idx_train]\n",
        "    percent_female_train = (genders_train == 0).sum()/len(indices_train) if len(indices_train) > 0 else 0\n",
        "    print(f\"Train Subset (Prop {int(p*100)}%): Target {int(p*100)}% -- Actual {percent_female_train*100:.2f}% females, {(genders_train == 0).sum()} samples\")\n",
        "\n",
        "\n",
        "    number_female_test = len(test_subsets_f[p].indices)\n",
        "    number_male_test = len(test_subsets_m[p].indices)\n",
        "    print(f\"Number of female test samples: {number_female_test}\")\n",
        "    print(f\"Number of male test samples: {number_male_test}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2x4LIJezqg1",
        "outputId": "f78e535c-b74d-4f1c-9ee8-dd06ea5dcba0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Subset (Prop 25%): Target 25% -- Actual 24.99% females, 2480 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n",
            "Train Subset (Prop 50%): Target 50% -- Actual 50.00% females, 4961 samples\n",
            "Number of female test samples: 755\n",
            "Number of male test samples: 755\n",
            "Train Subset (Prop 75%): Target 75% -- Actual 74.99% females, 7441 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_subsets[0.5], batch_size=batch_size, shuffle=True)\n",
        "# val_loader = DataLoader(test_subsets[0.5], batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "V4HgisJbFAtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "goy8JCnuN5y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#title this is a more complicated model but used more commonly in FR\n",
        "\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.5, easy_margin=False):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = torch.cos(torch.tensor(self.m))\n",
        "        self.sin_m = torch.sin(torch.tensor(self.m))\n",
        "        self.th = torch.cos(torch.tensor(3.14159265 - self.m))\n",
        "        self.mm = torch.sin(torch.tensor(3.14159265 - self.m)) * self.m\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - torch.clamp(cosine ** 2, 0, 1))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        one_hot = torch.zeros_like(cosine)\n",
        "        one_hot.scatter_(1, label.view(-1, 1), 1)\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "_wFThr7cNaP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, arc_head, dataloader, device):\n",
        "    model.eval()\n",
        "    arc_head.eval()\n",
        "    total, correct, running_loss = 0, 0, 0.0\n",
        "    for i, (images, identity_labels) in enumerate(dataloader):\n",
        "        images, labels = images.to(device), identity_labels[:,0].to(device) # Selecting the first column of identity_labels\n",
        "        features = model(images)\n",
        "        logits = arc_head(features, labels) # Using the modified labels for arc_head\n",
        "        loss = criterion(logits, labels)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += images.size(0)\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    print(f\"Test set: loss={avg_loss:.4f}, accuracy={acc*100:.2f}, data loader{dataloader}%\")\n",
        "    return avg_loss, acc\n",
        "\n"
      ],
      "metadata": {
        "id": "EARq75acrB5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "backbone = models.resnet18(weights=None)\n",
        "feature_dim = backbone.fc.in_features\n",
        "backbone.fc = nn.Identity()\n",
        "\n",
        "n_classes=(dataset.identity.unique())\n",
        "print(n_classes)\n",
        "# arc_head = ArcMarginProduct(feature_dim, out_features=n_classes).to(device)\n",
        "arc_head = ArcMarginProduct(feature_dim, 10177).to(device) # Tracy update\n",
        "\n"
      ],
      "metadata": {
        "id": "Vr67fLnNNXWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fde6e0-2fa9-4912-f5c5-bbc8d43f4c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    1,     2,     3,  ..., 10175, 10176, 10177])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check for the eval code\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "\n",
        "\n",
        "\n",
        "# for epoch in range(1):\n",
        "#   avg_loss, acc = evaluate(model, arc_head, val_loader, device)"
      ],
      "metadata": {
        "id": "_oK13oicDSxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "best_acc = 0.0\n",
        "\n",
        "\n",
        "for proportion in proportions:\n",
        "    model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "    best_acc = 0.0\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize val_loader_f and val_loader_m to None\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    # Check if the subsets have any samples before creating DataLoaders\n",
        "    if len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=True)\n",
        "    if len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        arc_head.train()\n",
        "        total, correct, running_loss = 0, 0, 0.0\n",
        "        for i, (images, identity_labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), identity_labels[:,0].to(device) # Selecting the first column of identity_labels\n",
        "            features = model(images)\n",
        "            logits = arc_head(features, labels) # Using the modified labels for arc_head\n",
        "            loss = criterion(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += images.size(0)\n",
        "            if (i+1) % 50 == 0: print(f\"Batch {i+1}/{len(train_loader)} - Loss {loss.item():.4f}\")\n",
        "        print(f\"Epoch {epoch+1}: Loss={running_loss/total:.4f}  Accuracy={correct/total*100:.2f}%\")\n",
        "\n",
        "        # Check if the validation loaders are not empty before evaluating\n",
        "\n",
        "        if val_loader_f:\n",
        "            val_loss_f, val_acc_f = evaluate(model, arc_head, val_loader_f, device)\n",
        "        if val_loader_m:\n",
        "            val_loss_m, val_acc_m = evaluate(model, arc_head, val_loader_m, device)\n",
        "\n",
        "        if val_loader_m is not None and val_loader_f is not None:\n",
        "          val_acc = (val_acc_f + val_acc_m) / 2\n",
        "        elif val_loader_m:\n",
        "          val_acc = val_acc_m\n",
        "        elif val_loader_f:\n",
        "          val_acc = val_acc_f\n",
        "        else:\n",
        "          val_acc = 0.0\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "          best_acc = val_acc\n",
        "          torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'arc_head_state_dict': arc_head.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'val_acc': val_acc,\n",
        "          }, f'model{proportion}_checkpoint.pth')\n",
        "\n",
        "          if val_loader_m and val_loader_f:\n",
        "            print(f\"New best model saved at epoch {epoch+1} with average acc {val_acc_f*100:.2f}, female acc {val_acc_f*100:.2f}, male acc {val_acc_m*100:.2f}%\")\n",
        "          elif val_loader_m:\n",
        "            print(f\"New best model saved at epoch {epoch+1} with male acc {val_acc_m*100:.2f}%\")\n",
        "          elif val_loader_f:\n",
        "            print(f\"New best model saved at epoch {epoch+1} with female acc {val_acc_f*100:.2f}%\")"
      ],
      "metadata": {
        "id": "MvwwRTTpMt5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715662f3-fb15-469d-a1ff-831cb3dfcc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 50/156 - Loss 3.8559\n",
            "Batch 100/156 - Loss 2.8617\n",
            "Batch 150/156 - Loss 3.2255\n",
            "Epoch 1: Loss=2.9338  Accuracy=71.79%\n",
            "Test set: loss=1.5582, accuracy=88.59%\n",
            "Test set: loss=1.5568, accuracy=89.12%\n",
            "New best model saved at epoch 1 with average acc 88.59, female acc 88.59, male acc 89.12%\n",
            "Batch 50/156 - Loss 3.2611\n",
            "Batch 100/156 - Loss 3.1904\n",
            "Batch 150/156 - Loss 4.8022\n",
            "Epoch 2: Loss=2.9162  Accuracy=72.49%\n",
            "Test set: loss=1.6898, accuracy=86.47%\n",
            "Test set: loss=1.6631, accuracy=88.06%\n",
            "Batch 50/156 - Loss 2.4079\n",
            "Batch 100/156 - Loss 1.6515\n",
            "Batch 150/156 - Loss 2.6151\n",
            "Epoch 3: Loss=2.7828  Accuracy=75.84%\n",
            "Test set: loss=1.3345, accuracy=89.39%\n",
            "Test set: loss=1.4649, accuracy=89.66%\n",
            "New best model saved at epoch 3 with average acc 89.39, female acc 89.39, male acc 89.66%\n",
            "Batch 50/156 - Loss 3.0508\n",
            "Batch 100/156 - Loss 3.8725\n",
            "Batch 150/156 - Loss 3.3005\n",
            "Epoch 4: Loss=2.6610  Accuracy=77.30%\n",
            "Test set: loss=1.6540, accuracy=87.53%\n",
            "Test set: loss=1.6887, accuracy=87.53%\n",
            "Batch 50/156 - Loss 2.2793\n",
            "Batch 100/156 - Loss 2.3232\n",
            "Batch 150/156 - Loss 2.8048\n",
            "Epoch 5: Loss=2.5710  Accuracy=77.87%\n",
            "Test set: loss=1.9922, accuracy=85.15%\n",
            "Test set: loss=2.1485, accuracy=83.55%\n",
            "Batch 50/156 - Loss 2.2392\n",
            "Batch 100/156 - Loss 3.1714\n",
            "Batch 150/156 - Loss 3.3365\n",
            "Epoch 6: Loss=2.5334  Accuracy=78.95%\n",
            "Test set: loss=2.1802, accuracy=85.15%\n",
            "Test set: loss=1.9369, accuracy=84.08%\n",
            "Batch 50/156 - Loss 2.9705\n",
            "Batch 100/156 - Loss 2.3699\n",
            "Batch 150/156 - Loss 2.5338\n",
            "Epoch 7: Loss=2.4184  Accuracy=79.88%\n",
            "Test set: loss=1.9715, accuracy=84.62%\n",
            "Test set: loss=1.8327, accuracy=86.21%\n",
            "Batch 50/156 - Loss 2.6519\n",
            "Batch 100/156 - Loss 2.9730\n",
            "Batch 150/156 - Loss 3.0006\n",
            "Epoch 8: Loss=2.3980  Accuracy=79.70%\n",
            "Test set: loss=1.7038, accuracy=87.27%\n",
            "Test set: loss=1.5941, accuracy=87.80%\n",
            "Batch 50/156 - Loss 2.3581\n",
            "Batch 100/156 - Loss 1.6399\n",
            "Batch 150/156 - Loss 1.6215\n",
            "Epoch 9: Loss=2.3004  Accuracy=82.00%\n",
            "Test set: loss=1.7635, accuracy=88.86%\n",
            "Test set: loss=1.5062, accuracy=90.19%\n",
            "Batch 50/156 - Loss 3.0578\n",
            "Batch 100/156 - Loss 2.2911\n",
            "Batch 150/156 - Loss 2.1420\n",
            "Epoch 10: Loss=2.4469  Accuracy=79.66%\n",
            "Test set: loss=1.8454, accuracy=86.47%\n",
            "Test set: loss=1.7646, accuracy=86.47%\n",
            "Batch 50/156 - Loss 2.2696\n",
            "Batch 100/156 - Loss 2.7688\n",
            "Batch 150/156 - Loss 2.4821\n",
            "Epoch 11: Loss=2.2333  Accuracy=82.14%\n",
            "Test set: loss=1.8538, accuracy=88.06%\n",
            "Test set: loss=1.7581, accuracy=88.59%\n",
            "Batch 50/156 - Loss 2.7043\n",
            "Batch 100/156 - Loss 2.0212\n",
            "Batch 150/156 - Loss 3.0883\n",
            "Epoch 12: Loss=2.3737  Accuracy=80.06%\n",
            "Test set: loss=1.6973, accuracy=84.35%\n",
            "Test set: loss=1.7874, accuracy=84.62%\n",
            "Batch 50/156 - Loss 3.6117\n",
            "Batch 100/156 - Loss 2.7268\n",
            "Batch 150/156 - Loss 1.7609\n",
            "Epoch 13: Loss=2.2245  Accuracy=82.72%\n",
            "Test set: loss=1.5689, accuracy=89.39%\n",
            "Test set: loss=1.4835, accuracy=90.45%\n",
            "New best model saved at epoch 13 with average acc 89.39, female acc 89.39, male acc 90.45%\n",
            "Batch 50/156 - Loss 2.3794\n",
            "Batch 100/156 - Loss 2.7526\n",
            "Batch 150/156 - Loss 1.7128\n",
            "Epoch 14: Loss=2.2582  Accuracy=81.36%\n",
            "Test set: loss=1.9449, accuracy=85.94%\n",
            "Test set: loss=1.8919, accuracy=87.80%\n",
            "Batch 50/156 - Loss 1.8017\n",
            "Batch 100/156 - Loss 1.0283\n",
            "Batch 150/156 - Loss 1.1855\n",
            "Epoch 15: Loss=2.0215  Accuracy=83.92%\n",
            "Test set: loss=1.9797, accuracy=84.62%\n",
            "Test set: loss=1.7920, accuracy=87.27%\n",
            "Batch 50/156 - Loss 1.3447\n",
            "Batch 100/156 - Loss 1.1883\n",
            "Batch 150/156 - Loss 2.2036\n",
            "Epoch 16: Loss=1.9031  Accuracy=85.46%\n",
            "Test set: loss=1.6665, accuracy=88.33%\n",
            "Test set: loss=1.5151, accuracy=90.19%\n",
            "Batch 50/156 - Loss 1.2374\n",
            "Batch 100/156 - Loss 2.0339\n",
            "Batch 150/156 - Loss 1.7311\n",
            "Epoch 17: Loss=1.8966  Accuracy=85.79%\n",
            "Test set: loss=1.4030, accuracy=90.19%\n",
            "Test set: loss=1.4672, accuracy=90.72%\n",
            "New best model saved at epoch 17 with average acc 90.19, female acc 90.19, male acc 90.72%\n",
            "Batch 50/156 - Loss 1.4565\n",
            "Batch 100/156 - Loss 2.0115\n",
            "Batch 150/156 - Loss 3.7686\n",
            "Epoch 18: Loss=1.9369  Accuracy=85.80%\n",
            "Test set: loss=1.6738, accuracy=86.47%\n",
            "Test set: loss=1.7592, accuracy=87.80%\n",
            "Batch 50/156 - Loss 2.8077\n",
            "Batch 100/156 - Loss 1.9869\n",
            "Batch 150/156 - Loss 2.3501\n",
            "Epoch 19: Loss=1.9898  Accuracy=84.73%\n",
            "Test set: loss=1.8698, accuracy=82.23%\n",
            "Test set: loss=1.4661, accuracy=85.94%\n",
            "Batch 50/156 - Loss 2.1550\n",
            "Batch 100/156 - Loss 2.8228\n",
            "Batch 150/156 - Loss 1.9950\n",
            "Epoch 20: Loss=1.9201  Accuracy=84.71%\n",
            "Test set: loss=2.0295, accuracy=87.27%\n",
            "Test set: loss=1.7257, accuracy=88.59%\n",
            "Batch 50/156 - Loss 0.6522\n",
            "Batch 100/156 - Loss 2.0989\n",
            "Batch 150/156 - Loss 0.9018\n",
            "Epoch 21: Loss=1.7672  Accuracy=87.20%\n",
            "Test set: loss=1.8625, accuracy=88.06%\n",
            "Test set: loss=1.7978, accuracy=88.59%\n",
            "Batch 50/156 - Loss 1.7873\n",
            "Batch 100/156 - Loss 2.3635\n",
            "Batch 150/156 - Loss 1.7340\n",
            "Epoch 22: Loss=1.8387  Accuracy=86.75%\n",
            "Test set: loss=1.7869, accuracy=87.53%\n",
            "Test set: loss=1.6383, accuracy=88.86%\n",
            "Batch 50/156 - Loss 2.1695\n",
            "Batch 100/156 - Loss 1.4251\n",
            "Batch 150/156 - Loss 2.1413\n",
            "Epoch 23: Loss=1.7943  Accuracy=86.90%\n",
            "Test set: loss=1.7159, accuracy=87.53%\n",
            "Test set: loss=1.6400, accuracy=89.39%\n",
            "Batch 50/156 - Loss 1.8163\n",
            "Batch 100/156 - Loss 1.2724\n",
            "Batch 150/156 - Loss 0.9733\n",
            "Epoch 24: Loss=1.6398  Accuracy=87.62%\n",
            "Test set: loss=1.7461, accuracy=87.27%\n",
            "Test set: loss=1.7348, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.9311\n",
            "Batch 100/156 - Loss 2.5431\n",
            "Batch 150/156 - Loss 1.9614\n",
            "Epoch 25: Loss=1.6295  Accuracy=88.57%\n",
            "Test set: loss=2.2562, accuracy=83.29%\n",
            "Test set: loss=1.7259, accuracy=87.00%\n",
            "Batch 50/156 - Loss 1.8519\n",
            "Batch 100/156 - Loss 2.0782\n",
            "Batch 150/156 - Loss 1.3863\n",
            "Epoch 26: Loss=1.6556  Accuracy=87.93%\n",
            "Test set: loss=2.0563, accuracy=87.53%\n",
            "Test set: loss=1.5868, accuracy=89.39%\n",
            "Batch 50/156 - Loss 1.1575\n",
            "Batch 100/156 - Loss 1.4725\n",
            "Batch 150/156 - Loss 2.0535\n",
            "Epoch 27: Loss=1.6778  Accuracy=87.68%\n",
            "Test set: loss=1.8798, accuracy=87.00%\n",
            "Test set: loss=1.6534, accuracy=88.86%\n",
            "Batch 50/156 - Loss 2.3757\n",
            "Batch 100/156 - Loss 2.0114\n",
            "Batch 150/156 - Loss 1.6852\n",
            "Epoch 28: Loss=1.6253  Accuracy=87.86%\n",
            "Test set: loss=1.9009, accuracy=88.33%\n",
            "Test set: loss=1.2956, accuracy=92.04%\n",
            "Batch 50/156 - Loss 1.0992\n",
            "Batch 100/156 - Loss 0.9941\n",
            "Batch 150/156 - Loss 1.4703\n",
            "Epoch 29: Loss=1.5822  Accuracy=88.41%\n",
            "Test set: loss=1.9132, accuracy=86.74%\n",
            "Test set: loss=1.6010, accuracy=88.59%\n",
            "Batch 50/156 - Loss 1.3508\n",
            "Batch 100/156 - Loss 0.4023\n",
            "Batch 150/156 - Loss 1.2520\n",
            "Epoch 30: Loss=1.4514  Accuracy=89.49%\n",
            "Test set: loss=1.8551, accuracy=88.06%\n",
            "Test set: loss=1.6077, accuracy=89.92%\n",
            "Batch 50/156 - Loss 1.3457\n",
            "Batch 100/156 - Loss 1.5573\n",
            "Batch 150/156 - Loss 1.5485\n",
            "Epoch 31: Loss=1.3953  Accuracy=90.53%\n",
            "Test set: loss=2.2866, accuracy=85.15%\n",
            "Test set: loss=1.7476, accuracy=88.06%\n",
            "Batch 50/156 - Loss 1.4258\n",
            "Batch 100/156 - Loss 1.2823\n",
            "Batch 150/156 - Loss 1.1726\n",
            "Epoch 32: Loss=1.3590  Accuracy=90.42%\n",
            "Test set: loss=1.7485, accuracy=87.53%\n",
            "Test set: loss=1.5958, accuracy=89.39%\n",
            "Batch 50/156 - Loss 1.2291\n",
            "Batch 100/156 - Loss 1.7426\n",
            "Batch 150/156 - Loss 1.2728\n",
            "Epoch 33: Loss=1.2971  Accuracy=90.58%\n",
            "Test set: loss=1.9192, accuracy=84.88%\n",
            "Test set: loss=1.7015, accuracy=85.68%\n",
            "Batch 50/156 - Loss 1.3935\n",
            "Batch 100/156 - Loss 1.0985\n",
            "Batch 150/156 - Loss 1.3844\n",
            "Epoch 34: Loss=1.3846  Accuracy=89.53%\n",
            "Test set: loss=2.1369, accuracy=86.47%\n",
            "Test set: loss=1.9147, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.3464\n",
            "Batch 100/156 - Loss 2.1463\n",
            "Batch 150/156 - Loss 1.0348\n",
            "Epoch 35: Loss=1.2120  Accuracy=91.17%\n",
            "Test set: loss=1.7826, accuracy=87.53%\n",
            "Test set: loss=1.5638, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.4277\n",
            "Batch 100/156 - Loss 2.0862\n",
            "Batch 150/156 - Loss 1.1250\n",
            "Epoch 36: Loss=1.0932  Accuracy=92.14%\n",
            "Test set: loss=2.0063, accuracy=86.74%\n",
            "Test set: loss=1.5004, accuracy=89.92%\n",
            "Batch 50/156 - Loss 1.1218\n",
            "Batch 100/156 - Loss 0.7460\n",
            "Batch 150/156 - Loss 1.2018\n",
            "Epoch 37: Loss=1.0764  Accuracy=92.03%\n",
            "Test set: loss=2.1965, accuracy=86.47%\n",
            "Test set: loss=1.8011, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.5673\n",
            "Batch 100/156 - Loss 1.3052\n",
            "Batch 150/156 - Loss 1.1549\n",
            "Epoch 38: Loss=1.0848  Accuracy=91.85%\n",
            "Test set: loss=2.3447, accuracy=85.68%\n",
            "Test set: loss=1.7338, accuracy=88.33%\n",
            "Batch 50/156 - Loss 1.7830\n",
            "Batch 100/156 - Loss 0.8696\n",
            "Batch 150/156 - Loss 1.4607\n",
            "Epoch 39: Loss=1.0793  Accuracy=91.64%\n",
            "Test set: loss=2.3368, accuracy=84.88%\n",
            "Test set: loss=1.7726, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.6110\n",
            "Batch 100/156 - Loss 1.2437\n",
            "Batch 150/156 - Loss 1.8866\n",
            "Epoch 40: Loss=1.0538  Accuracy=91.46%\n",
            "Test set: loss=2.1441, accuracy=84.35%\n",
            "Test set: loss=1.8838, accuracy=84.62%\n",
            "Batch 50/156 - Loss 1.4729\n",
            "Batch 100/156 - Loss 1.1698\n",
            "Batch 150/156 - Loss 0.6696\n",
            "Epoch 41: Loss=1.0200  Accuracy=91.82%\n",
            "Test set: loss=2.1485, accuracy=87.53%\n",
            "Test set: loss=1.6865, accuracy=89.12%\n",
            "Batch 50/156 - Loss 0.5616\n",
            "Batch 100/156 - Loss 0.8103\n",
            "Batch 150/156 - Loss 1.6546\n",
            "Epoch 42: Loss=0.9899  Accuracy=92.23%\n",
            "Test set: loss=1.8246, accuracy=88.59%\n",
            "Test set: loss=1.6849, accuracy=89.12%\n",
            "Batch 50/156 - Loss 1.1151\n",
            "Batch 100/156 - Loss 0.6701\n",
            "Batch 150/156 - Loss 1.3904\n",
            "Epoch 43: Loss=0.8814  Accuracy=93.17%\n",
            "Test set: loss=2.2548, accuracy=84.35%\n",
            "Test set: loss=1.5468, accuracy=88.86%\n",
            "Batch 50/156 - Loss 1.0742\n",
            "Batch 100/156 - Loss 0.8789\n",
            "Batch 150/156 - Loss 0.5621\n",
            "Epoch 44: Loss=0.9791  Accuracy=92.79%\n",
            "Test set: loss=2.1902, accuracy=85.68%\n",
            "Test set: loss=1.8446, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.1745\n",
            "Batch 100/156 - Loss 0.3578\n",
            "Batch 150/156 - Loss 0.4328\n",
            "Epoch 45: Loss=0.8762  Accuracy=93.54%\n",
            "Test set: loss=2.1406, accuracy=85.15%\n",
            "Test set: loss=1.9306, accuracy=87.27%\n",
            "Batch 50/156 - Loss 1.3497\n",
            "Batch 100/156 - Loss 0.9585\n",
            "Batch 150/156 - Loss 1.0204\n",
            "Epoch 46: Loss=0.8982  Accuracy=93.46%\n",
            "Test set: loss=1.9541, accuracy=87.27%\n",
            "Test set: loss=1.8944, accuracy=87.00%\n",
            "Batch 50/156 - Loss 0.0947\n",
            "Batch 100/156 - Loss 0.6634\n",
            "Batch 150/156 - Loss 0.2727\n",
            "Epoch 47: Loss=0.7800  Accuracy=94.34%\n",
            "Test set: loss=2.0977, accuracy=86.21%\n",
            "Test set: loss=2.1113, accuracy=86.47%\n",
            "Batch 50/156 - Loss 0.1317\n",
            "Batch 100/156 - Loss 0.5130\n",
            "Batch 150/156 - Loss 0.3223\n",
            "Epoch 48: Loss=0.7146  Accuracy=95.00%\n",
            "Test set: loss=2.7290, accuracy=83.55%\n",
            "Test set: loss=2.0480, accuracy=86.74%\n",
            "Batch 50/156 - Loss 0.7380\n",
            "Batch 100/156 - Loss 0.5863\n",
            "Batch 150/156 - Loss 1.1145\n",
            "Epoch 49: Loss=0.7676  Accuracy=94.54%\n",
            "Test set: loss=2.0321, accuracy=88.06%\n",
            "Test set: loss=1.7424, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.7167\n",
            "Batch 100/156 - Loss 0.6091\n",
            "Batch 150/156 - Loss 0.9314\n",
            "Epoch 50: Loss=0.6913  Accuracy=95.09%\n",
            "Test set: loss=2.3866, accuracy=86.21%\n",
            "Test set: loss=1.8849, accuracy=88.06%\n",
            "Batch 50/156 - Loss 1.1670\n",
            "Batch 100/156 - Loss 0.3536\n",
            "Batch 150/156 - Loss 0.9636\n",
            "Epoch 1: Loss=0.5764  Accuracy=95.89%\n",
            "Test set: loss=2.1380, accuracy=86.89%\n",
            "Test set: loss=2.0595, accuracy=87.68%\n",
            "New best model saved at epoch 1 with average acc 86.89, female acc 86.89, male acc 87.68%\n",
            "Batch 50/156 - Loss 0.5710\n",
            "Batch 100/156 - Loss 0.0199\n",
            "Batch 150/156 - Loss 0.6425\n",
            "Epoch 2: Loss=0.4924  Accuracy=96.47%\n",
            "Test set: loss=2.2454, accuracy=86.49%\n",
            "Test set: loss=1.9013, accuracy=88.34%\n",
            "New best model saved at epoch 2 with average acc 86.49, female acc 86.49, male acc 88.34%\n",
            "Batch 50/156 - Loss 0.3174\n",
            "Batch 100/156 - Loss 0.3233\n",
            "Batch 150/156 - Loss 0.5009\n",
            "Epoch 3: Loss=0.4560  Accuracy=96.83%\n",
            "Test set: loss=2.2595, accuracy=85.56%\n",
            "Test set: loss=1.9430, accuracy=86.36%\n",
            "Batch 50/156 - Loss 0.3757\n",
            "Batch 100/156 - Loss 0.6585\n",
            "Batch 150/156 - Loss 0.3352\n",
            "Epoch 4: Loss=0.5044  Accuracy=96.41%\n",
            "Test set: loss=2.5096, accuracy=84.64%\n",
            "Test set: loss=2.3027, accuracy=85.17%\n",
            "Batch 50/156 - Loss 0.3580\n",
            "Batch 100/156 - Loss 0.0389\n",
            "Batch 150/156 - Loss 0.6382\n",
            "Epoch 5: Loss=0.4792  Accuracy=96.57%\n",
            "Test set: loss=2.0307, accuracy=87.55%\n",
            "Test set: loss=1.7366, accuracy=89.54%\n",
            "New best model saved at epoch 5 with average acc 87.55, female acc 87.55, male acc 89.54%\n",
            "Batch 50/156 - Loss 0.3786\n",
            "Batch 100/156 - Loss 0.5571\n",
            "Batch 150/156 - Loss 1.1735\n",
            "Epoch 6: Loss=0.4052  Accuracy=97.18%\n",
            "Test set: loss=2.0490, accuracy=87.02%\n",
            "Test set: loss=1.7061, accuracy=88.34%\n",
            "Batch 50/156 - Loss 0.6419\n",
            "Batch 100/156 - Loss 0.0347\n",
            "Batch 150/156 - Loss 0.3214\n",
            "Epoch 7: Loss=0.4084  Accuracy=97.11%\n",
            "Test set: loss=2.3192, accuracy=87.02%\n",
            "Test set: loss=1.8950, accuracy=88.34%\n",
            "Batch 50/156 - Loss 0.3370\n",
            "Batch 100/156 - Loss 0.5378\n",
            "Batch 150/156 - Loss 0.3584\n",
            "Epoch 8: Loss=0.4211  Accuracy=96.90%\n",
            "Test set: loss=2.5854, accuracy=84.11%\n",
            "Test set: loss=2.0109, accuracy=86.75%\n",
            "Batch 50/156 - Loss 0.2825\n",
            "Batch 100/156 - Loss 0.6378\n",
            "Batch 150/156 - Loss 0.0107\n",
            "Epoch 9: Loss=0.4422  Accuracy=96.84%\n",
            "Test set: loss=2.1652, accuracy=87.55%\n",
            "Test set: loss=1.9907, accuracy=89.01%\n",
            "Batch 50/156 - Loss 0.3244\n",
            "Batch 100/156 - Loss 0.4865\n",
            "Batch 150/156 - Loss 0.5545\n",
            "Epoch 10: Loss=0.4166  Accuracy=97.07%\n",
            "Test set: loss=2.0513, accuracy=85.56%\n",
            "Test set: loss=1.7648, accuracy=87.15%\n",
            "Batch 50/156 - Loss 0.7088\n",
            "Batch 100/156 - Loss 0.0324\n",
            "Batch 150/156 - Loss 0.9173\n",
            "Epoch 11: Loss=0.3110  Accuracy=97.78%\n",
            "Test set: loss=2.1837, accuracy=87.95%\n",
            "Test set: loss=1.9462, accuracy=89.14%\n",
            "Batch 50/156 - Loss 0.7608\n",
            "Batch 100/156 - Loss 0.6118\n",
            "Batch 150/156 - Loss 0.0214\n",
            "Epoch 12: Loss=0.3516  Accuracy=97.52%\n",
            "Test set: loss=2.1148, accuracy=86.49%\n",
            "Test set: loss=1.6769, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.3072\n",
            "Batch 100/156 - Loss 0.3469\n",
            "Batch 150/156 - Loss 0.5696\n",
            "Epoch 13: Loss=0.3555  Accuracy=97.49%\n",
            "Test set: loss=2.3579, accuracy=85.83%\n",
            "Test set: loss=1.8972, accuracy=89.14%\n",
            "Batch 50/156 - Loss 0.0160\n",
            "Batch 100/156 - Loss 0.3213\n",
            "Batch 150/156 - Loss 0.9403\n",
            "Epoch 14: Loss=0.3761  Accuracy=97.34%\n",
            "Test set: loss=2.5326, accuracy=85.56%\n",
            "Test set: loss=1.9722, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.7373\n",
            "Batch 100/156 - Loss 0.3273\n",
            "Batch 150/156 - Loss 0.6268\n",
            "Epoch 15: Loss=0.4017  Accuracy=97.17%\n",
            "Test set: loss=2.2966, accuracy=86.09%\n",
            "Test set: loss=1.6667, accuracy=89.80%\n",
            "Batch 50/156 - Loss 0.6305\n",
            "Batch 100/156 - Loss 0.1694\n",
            "Batch 150/156 - Loss 0.0173\n",
            "Epoch 16: Loss=0.3372  Accuracy=97.69%\n",
            "Test set: loss=2.2362, accuracy=87.55%\n",
            "Test set: loss=1.8808, accuracy=88.74%\n",
            "Batch 50/156 - Loss 0.3352\n",
            "Batch 100/156 - Loss 0.0160\n",
            "Batch 150/156 - Loss 0.0205\n",
            "Epoch 17: Loss=0.3206  Accuracy=97.77%\n",
            "Test set: loss=2.2832, accuracy=86.75%\n",
            "Test set: loss=1.8856, accuracy=89.01%\n",
            "Batch 50/156 - Loss 0.3256\n",
            "Batch 100/156 - Loss 0.0622\n",
            "Batch 150/156 - Loss 0.5052\n",
            "Epoch 18: Loss=0.2950  Accuracy=97.97%\n",
            "Test set: loss=2.0178, accuracy=87.68%\n",
            "Test set: loss=1.7140, accuracy=90.20%\n",
            "New best model saved at epoch 18 with average acc 87.68, female acc 87.68, male acc 90.20%\n",
            "Batch 50/156 - Loss 0.7507\n",
            "Batch 100/156 - Loss 0.0188\n",
            "Batch 150/156 - Loss 0.0388\n",
            "Epoch 19: Loss=0.3132  Accuracy=97.86%\n",
            "Test set: loss=1.9139, accuracy=89.01%\n",
            "Test set: loss=1.7270, accuracy=90.33%\n",
            "New best model saved at epoch 19 with average acc 89.01, female acc 89.01, male acc 90.33%\n",
            "Batch 50/156 - Loss 0.0218\n",
            "Batch 100/156 - Loss 0.0350\n",
            "Batch 150/156 - Loss 0.0080\n",
            "Epoch 20: Loss=0.2757  Accuracy=98.16%\n",
            "Test set: loss=2.2744, accuracy=88.08%\n",
            "Test set: loss=1.9601, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.3537\n",
            "Batch 100/156 - Loss 0.3196\n",
            "Batch 150/156 - Loss 0.4466\n",
            "Epoch 21: Loss=0.3326  Accuracy=97.67%\n",
            "Test set: loss=1.7783, accuracy=88.87%\n",
            "Test set: loss=1.8894, accuracy=87.81%\n",
            "Batch 50/156 - Loss 1.1862\n",
            "Batch 100/156 - Loss 0.8764\n",
            "Batch 150/156 - Loss 0.7925\n",
            "Epoch 22: Loss=0.2951  Accuracy=97.91%\n",
            "Test set: loss=2.5704, accuracy=85.56%\n",
            "Test set: loss=1.9449, accuracy=88.48%\n",
            "Batch 50/156 - Loss 0.3622\n",
            "Batch 100/156 - Loss 0.0181\n",
            "Batch 150/156 - Loss 0.0950\n",
            "Epoch 23: Loss=0.2805  Accuracy=98.19%\n",
            "Test set: loss=2.2572, accuracy=85.96%\n",
            "Test set: loss=1.6663, accuracy=89.80%\n",
            "Batch 50/156 - Loss 0.1455\n",
            "Batch 100/156 - Loss 0.3059\n",
            "Batch 150/156 - Loss 1.2332\n",
            "Epoch 24: Loss=0.2617  Accuracy=98.30%\n",
            "Test set: loss=2.0774, accuracy=87.95%\n",
            "Test set: loss=1.7428, accuracy=89.93%\n",
            "Batch 50/156 - Loss 0.0251\n",
            "Batch 100/156 - Loss 0.3014\n",
            "Batch 150/156 - Loss 0.6138\n",
            "Epoch 25: Loss=0.2795  Accuracy=97.96%\n",
            "Test set: loss=2.0693, accuracy=88.34%\n",
            "Test set: loss=2.0019, accuracy=89.27%\n",
            "Batch 50/156 - Loss 0.0133\n",
            "Batch 100/156 - Loss 0.3159\n",
            "Batch 150/156 - Loss 0.0189\n",
            "Epoch 26: Loss=0.2629  Accuracy=98.22%\n",
            "Test set: loss=2.1601, accuracy=88.34%\n",
            "Test set: loss=1.9096, accuracy=89.54%\n",
            "Batch 50/156 - Loss 0.3276\n",
            "Batch 100/156 - Loss 0.3132\n",
            "Batch 150/156 - Loss 0.0279\n",
            "Epoch 27: Loss=0.2629  Accuracy=98.21%\n",
            "Test set: loss=1.9779, accuracy=89.27%\n",
            "Test set: loss=1.8847, accuracy=89.67%\n",
            "Batch 50/156 - Loss 0.0172\n",
            "Batch 100/156 - Loss 0.0350\n",
            "Batch 150/156 - Loss 0.0160\n",
            "Epoch 28: Loss=0.2256  Accuracy=98.63%\n",
            "Test set: loss=2.1756, accuracy=85.30%\n",
            "Test set: loss=1.8810, accuracy=87.55%\n",
            "Batch 50/156 - Loss 0.3212\n",
            "Batch 100/156 - Loss 0.6579\n",
            "Batch 150/156 - Loss 0.0152\n",
            "Epoch 29: Loss=0.2068  Accuracy=98.65%\n",
            "Test set: loss=2.1787, accuracy=87.81%\n",
            "Test set: loss=1.7724, accuracy=89.67%\n",
            "Batch 50/156 - Loss 0.5696\n",
            "Batch 100/156 - Loss 0.0159\n",
            "Batch 150/156 - Loss 0.4125\n",
            "Epoch 30: Loss=0.2476  Accuracy=98.35%\n",
            "Test set: loss=2.1225, accuracy=88.61%\n",
            "Test set: loss=1.9799, accuracy=88.74%\n",
            "Batch 50/156 - Loss 0.0207\n",
            "Batch 100/156 - Loss 0.3935\n",
            "Batch 150/156 - Loss 0.6234\n",
            "Epoch 31: Loss=0.2594  Accuracy=98.17%\n",
            "Test set: loss=2.3282, accuracy=87.42%\n",
            "Test set: loss=2.0406, accuracy=88.74%\n",
            "Batch 50/156 - Loss 0.0997\n",
            "Batch 100/156 - Loss 0.2686\n",
            "Batch 150/156 - Loss 0.0244\n",
            "Epoch 32: Loss=0.2453  Accuracy=98.39%\n",
            "Test set: loss=2.0912, accuracy=88.61%\n",
            "Test set: loss=1.9262, accuracy=88.34%\n",
            "Batch 50/156 - Loss 0.0218\n",
            "Batch 100/156 - Loss 0.5057\n",
            "Batch 150/156 - Loss 1.6053\n",
            "Epoch 33: Loss=0.3082  Accuracy=97.82%\n",
            "Test set: loss=2.1974, accuracy=87.68%\n",
            "Test set: loss=1.8170, accuracy=89.93%\n",
            "Batch 50/156 - Loss 0.6189\n",
            "Batch 100/156 - Loss 0.0124\n",
            "Batch 150/156 - Loss 0.5552\n",
            "Epoch 34: Loss=0.2526  Accuracy=98.28%\n",
            "Test set: loss=2.6841, accuracy=84.77%\n",
            "Test set: loss=2.1534, accuracy=87.15%\n",
            "Batch 50/156 - Loss 0.8086\n",
            "Batch 100/156 - Loss 0.3344\n",
            "Batch 150/156 - Loss 0.3541\n",
            "Epoch 35: Loss=0.3234  Accuracy=97.85%\n",
            "Test set: loss=2.1253, accuracy=87.81%\n",
            "Test set: loss=1.9954, accuracy=88.48%\n",
            "Batch 50/156 - Loss 0.1017\n",
            "Batch 100/156 - Loss 0.0181\n",
            "Batch 150/156 - Loss 0.3273\n",
            "Epoch 36: Loss=0.2938  Accuracy=98.01%\n",
            "Test set: loss=2.3976, accuracy=85.70%\n",
            "Test set: loss=2.0512, accuracy=87.68%\n",
            "Batch 50/156 - Loss 0.3266\n",
            "Batch 100/156 - Loss 0.0147\n",
            "Batch 150/156 - Loss 0.0124\n",
            "Epoch 37: Loss=0.2487  Accuracy=98.33%\n",
            "Test set: loss=2.4034, accuracy=86.49%\n",
            "Test set: loss=2.0155, accuracy=89.14%\n",
            "Batch 50/156 - Loss 0.0149\n",
            "Batch 100/156 - Loss 0.6383\n",
            "Batch 150/156 - Loss 0.3141\n",
            "Epoch 38: Loss=0.2459  Accuracy=98.36%\n",
            "Test set: loss=2.4953, accuracy=85.83%\n",
            "Test set: loss=2.2044, accuracy=87.95%\n",
            "Batch 50/156 - Loss 0.6133\n",
            "Batch 100/156 - Loss 0.0156\n",
            "Batch 150/156 - Loss 0.5935\n",
            "Epoch 39: Loss=0.2776  Accuracy=98.02%\n",
            "Test set: loss=2.4646, accuracy=84.77%\n",
            "Test set: loss=1.9970, accuracy=87.42%\n",
            "Batch 50/156 - Loss 0.2364\n",
            "Batch 100/156 - Loss 0.0205\n",
            "Batch 150/156 - Loss 0.0243\n",
            "Epoch 40: Loss=0.2509  Accuracy=98.26%\n",
            "Test set: loss=2.4132, accuracy=86.23%\n",
            "Test set: loss=1.9556, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.0239\n",
            "Batch 100/156 - Loss 0.3165\n",
            "Batch 150/156 - Loss 0.0095\n",
            "Epoch 41: Loss=0.2062  Accuracy=98.65%\n",
            "Test set: loss=2.4917, accuracy=87.02%\n",
            "Test set: loss=2.1561, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.0075\n",
            "Batch 100/156 - Loss 0.1391\n",
            "Batch 150/156 - Loss 0.2105\n",
            "Epoch 42: Loss=0.1886  Accuracy=98.80%\n",
            "Test set: loss=2.4309, accuracy=86.62%\n",
            "Test set: loss=1.9446, accuracy=88.87%\n",
            "Batch 50/156 - Loss 0.3107\n",
            "Batch 100/156 - Loss 0.3242\n",
            "Batch 150/156 - Loss 1.1930\n",
            "Epoch 43: Loss=0.2121  Accuracy=98.67%\n",
            "Test set: loss=2.2855, accuracy=86.49%\n",
            "Test set: loss=2.0610, accuracy=87.68%\n",
            "Batch 50/156 - Loss 0.0583\n",
            "Batch 100/156 - Loss 0.0322\n",
            "Batch 150/156 - Loss 0.6262\n",
            "Epoch 44: Loss=0.2146  Accuracy=98.48%\n",
            "Test set: loss=2.5372, accuracy=86.49%\n",
            "Test set: loss=2.1812, accuracy=87.95%\n",
            "Batch 50/156 - Loss 0.2627\n",
            "Batch 100/156 - Loss 0.3266\n",
            "Batch 150/156 - Loss 0.4139\n",
            "Epoch 45: Loss=0.2633  Accuracy=98.15%\n",
            "Test set: loss=2.6515, accuracy=84.37%\n",
            "Test set: loss=2.0417, accuracy=87.68%\n",
            "Batch 50/156 - Loss 0.0144\n",
            "Batch 100/156 - Loss 0.0181\n",
            "Batch 150/156 - Loss 0.7915\n",
            "Epoch 46: Loss=0.2274  Accuracy=98.29%\n",
            "Test set: loss=1.9791, accuracy=88.08%\n",
            "Test set: loss=1.9807, accuracy=88.48%\n",
            "Batch 50/156 - Loss 0.0146\n",
            "Batch 100/156 - Loss 0.3466\n",
            "Batch 150/156 - Loss 0.1005\n",
            "Epoch 47: Loss=0.2779  Accuracy=98.01%\n",
            "Test set: loss=2.5020, accuracy=84.64%\n",
            "Test set: loss=2.1573, accuracy=86.89%\n",
            "Batch 50/156 - Loss 0.3107\n",
            "Batch 100/156 - Loss 0.5338\n",
            "Batch 150/156 - Loss 0.0083\n",
            "Epoch 48: Loss=0.1867  Accuracy=98.85%\n",
            "Test set: loss=2.2289, accuracy=88.74%\n",
            "Test set: loss=1.8621, accuracy=90.46%\n",
            "Batch 50/156 - Loss 0.0345\n",
            "Batch 100/156 - Loss 0.0357\n",
            "Batch 150/156 - Loss 0.0119\n",
            "Epoch 49: Loss=0.3242  Accuracy=97.69%\n",
            "Test set: loss=2.4333, accuracy=86.89%\n",
            "Test set: loss=2.0804, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.3199\n",
            "Batch 100/156 - Loss 0.0129\n",
            "Batch 150/156 - Loss 0.3036\n",
            "Epoch 50: Loss=0.1729  Accuracy=98.89%\n",
            "Test set: loss=2.3130, accuracy=86.36%\n",
            "Test set: loss=2.3674, accuracy=86.75%\n",
            "Batch 50/156 - Loss 0.0064\n",
            "Batch 100/156 - Loss 0.0047\n",
            "Batch 150/156 - Loss 0.3226\n",
            "Epoch 1: Loss=0.1439  Accuracy=99.00%\n",
            "Test set: loss=2.1067, accuracy=86.74%\n",
            "Test set: loss=2.2546, accuracy=87.00%\n",
            "New best model saved at epoch 1 with average acc 86.74, female acc 86.74, male acc 87.00%\n",
            "Batch 50/156 - Loss 0.4325\n",
            "Batch 100/156 - Loss 0.2875\n",
            "Batch 150/156 - Loss 0.1824\n",
            "Epoch 2: Loss=0.1360  Accuracy=99.02%\n",
            "Test set: loss=1.9990, accuracy=87.00%\n",
            "Test set: loss=1.9651, accuracy=87.27%\n",
            "New best model saved at epoch 2 with average acc 87.00, female acc 87.00, male acc 87.27%\n",
            "Batch 50/156 - Loss 0.3218\n",
            "Batch 100/156 - Loss 0.0048\n",
            "Batch 150/156 - Loss 0.6162\n",
            "Epoch 3: Loss=0.1421  Accuracy=99.05%\n",
            "Test set: loss=2.1407, accuracy=87.80%\n",
            "Test set: loss=1.9038, accuracy=89.66%\n",
            "New best model saved at epoch 3 with average acc 87.80, female acc 87.80, male acc 89.66%\n",
            "Batch 50/156 - Loss 0.0419\n",
            "Batch 100/156 - Loss 0.0112\n",
            "Batch 150/156 - Loss 0.3210\n",
            "Epoch 4: Loss=0.1711  Accuracy=98.83%\n",
            "Test set: loss=2.9310, accuracy=81.96%\n",
            "Test set: loss=2.3912, accuracy=84.88%\n",
            "Batch 50/156 - Loss 0.2816\n",
            "Batch 100/156 - Loss 0.0223\n",
            "Batch 150/156 - Loss 0.0053\n",
            "Epoch 5: Loss=0.1695  Accuracy=98.73%\n",
            "Test set: loss=2.4235, accuracy=87.27%\n",
            "Test set: loss=1.8903, accuracy=89.92%\n",
            "Batch 50/156 - Loss 0.0153\n",
            "Batch 100/156 - Loss 0.0036\n",
            "Batch 150/156 - Loss 0.0048\n",
            "Epoch 6: Loss=0.1368  Accuracy=99.08%\n",
            "Test set: loss=1.9738, accuracy=88.86%\n",
            "Test set: loss=1.8377, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.0062\n",
            "Batch 100/156 - Loss 0.0248\n",
            "Batch 150/156 - Loss 0.0038\n",
            "Epoch 7: Loss=0.0909  Accuracy=99.41%\n",
            "Test set: loss=2.1501, accuracy=88.86%\n",
            "Test set: loss=1.7508, accuracy=90.72%\n",
            "New best model saved at epoch 7 with average acc 88.86, female acc 88.86, male acc 90.72%\n",
            "Batch 50/156 - Loss 0.0028\n",
            "Batch 100/156 - Loss 0.3064\n",
            "Batch 150/156 - Loss 0.2555\n",
            "Epoch 8: Loss=0.0998  Accuracy=99.42%\n",
            "Test set: loss=1.8658, accuracy=88.86%\n",
            "Test set: loss=1.8192, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0075\n",
            "Batch 100/156 - Loss 0.0019\n",
            "Batch 150/156 - Loss 0.2286\n",
            "Epoch 9: Loss=0.1319  Accuracy=99.07%\n",
            "Test set: loss=2.2120, accuracy=87.00%\n",
            "Test set: loss=1.9329, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.2065\n",
            "Batch 100/156 - Loss 0.0065\n",
            "Batch 150/156 - Loss 0.5751\n",
            "Epoch 10: Loss=0.1650  Accuracy=98.83%\n",
            "Test set: loss=2.3400, accuracy=85.94%\n",
            "Test set: loss=1.7834, accuracy=89.12%\n",
            "Batch 50/156 - Loss 0.0108\n",
            "Batch 100/156 - Loss 0.2893\n",
            "Batch 150/156 - Loss 0.0044\n",
            "Epoch 11: Loss=0.1498  Accuracy=98.91%\n",
            "Test set: loss=2.4639, accuracy=85.68%\n",
            "Test set: loss=2.0492, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.0054\n",
            "Batch 100/156 - Loss 0.0017\n",
            "Batch 150/156 - Loss 0.2873\n",
            "Epoch 12: Loss=0.0863  Accuracy=99.48%\n",
            "Test set: loss=2.1969, accuracy=85.94%\n",
            "Test set: loss=1.9093, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.2883\n",
            "Batch 100/156 - Loss 0.1738\n",
            "Batch 150/156 - Loss 0.3423\n",
            "Epoch 13: Loss=0.1435  Accuracy=98.91%\n",
            "Test set: loss=2.0232, accuracy=85.15%\n",
            "Test set: loss=1.9110, accuracy=86.74%\n",
            "Batch 50/156 - Loss 0.0044\n",
            "Batch 100/156 - Loss 0.0056\n",
            "Batch 150/156 - Loss 0.0070\n",
            "Epoch 14: Loss=0.1311  Accuracy=99.09%\n",
            "Test set: loss=2.0613, accuracy=89.39%\n",
            "Test set: loss=1.9778, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.0021\n",
            "Batch 100/156 - Loss 0.0184\n",
            "Batch 150/156 - Loss 0.1763\n",
            "Epoch 15: Loss=0.1257  Accuracy=99.14%\n",
            "Test set: loss=2.0912, accuracy=86.74%\n",
            "Test set: loss=1.8475, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0070\n",
            "Batch 100/156 - Loss 0.5098\n",
            "Batch 150/156 - Loss 0.0084\n",
            "Epoch 16: Loss=0.1368  Accuracy=98.93%\n",
            "Test set: loss=2.6434, accuracy=86.21%\n",
            "Test set: loss=1.9280, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.0126\n",
            "Batch 100/156 - Loss 0.2848\n",
            "Batch 150/156 - Loss 0.0020\n",
            "Epoch 17: Loss=0.1280  Accuracy=99.15%\n",
            "Test set: loss=2.5649, accuracy=86.74%\n",
            "Test set: loss=2.0929, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.2741\n",
            "Batch 100/156 - Loss 0.0086\n",
            "Batch 150/156 - Loss 0.0038\n",
            "Epoch 18: Loss=0.1354  Accuracy=99.01%\n",
            "Test set: loss=2.5400, accuracy=84.08%\n",
            "Test set: loss=2.0080, accuracy=87.53%\n",
            "Batch 50/156 - Loss 0.0250\n",
            "Batch 100/156 - Loss 0.0031\n",
            "Batch 150/156 - Loss 0.3082\n",
            "Epoch 19: Loss=0.1146  Accuracy=99.17%\n",
            "Test set: loss=2.1630, accuracy=88.06%\n",
            "Test set: loss=1.8272, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.0031\n",
            "Batch 100/156 - Loss 0.0092\n",
            "Batch 150/156 - Loss 0.3137\n",
            "Epoch 20: Loss=0.0904  Accuracy=99.41%\n",
            "Test set: loss=2.2988, accuracy=85.94%\n",
            "Test set: loss=1.8033, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.0029\n",
            "Batch 100/156 - Loss 0.0040\n",
            "Batch 150/156 - Loss 0.6206\n",
            "Epoch 21: Loss=0.0949  Accuracy=99.33%\n",
            "Test set: loss=2.8285, accuracy=84.35%\n",
            "Test set: loss=1.9250, accuracy=87.53%\n",
            "Batch 50/156 - Loss 0.0049\n",
            "Batch 100/156 - Loss 0.0031\n",
            "Batch 150/156 - Loss 0.0102\n",
            "Epoch 22: Loss=0.1415  Accuracy=98.93%\n",
            "Test set: loss=2.0217, accuracy=87.00%\n",
            "Test set: loss=1.8310, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.0182\n",
            "Batch 100/156 - Loss 0.0098\n",
            "Batch 150/156 - Loss 0.1764\n",
            "Epoch 23: Loss=0.1613  Accuracy=98.78%\n",
            "Test set: loss=2.1578, accuracy=88.33%\n",
            "Test set: loss=1.8557, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.2997\n",
            "Batch 100/156 - Loss 0.1194\n",
            "Batch 150/156 - Loss 0.0053\n",
            "Epoch 24: Loss=0.1225  Accuracy=99.01%\n",
            "Test set: loss=2.6468, accuracy=86.74%\n",
            "Test set: loss=1.9254, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.3149\n",
            "Batch 100/156 - Loss 0.2996\n",
            "Batch 150/156 - Loss 0.0071\n",
            "Epoch 25: Loss=0.0874  Accuracy=99.39%\n",
            "Test set: loss=2.5449, accuracy=86.47%\n",
            "Test set: loss=2.1001, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0025\n",
            "Batch 100/156 - Loss 0.2315\n",
            "Batch 150/156 - Loss 0.0037\n",
            "Epoch 26: Loss=0.0489  Accuracy=99.72%\n",
            "Test set: loss=2.0085, accuracy=89.39%\n",
            "Test set: loss=1.9221, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.3178\n",
            "Batch 100/156 - Loss 0.0099\n",
            "Batch 150/156 - Loss 0.0089\n",
            "Epoch 27: Loss=0.0912  Accuracy=99.29%\n",
            "Test set: loss=2.5285, accuracy=87.27%\n",
            "Test set: loss=2.0261, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0046\n",
            "Batch 100/156 - Loss 0.0336\n",
            "Batch 150/156 - Loss 0.6507\n",
            "Epoch 28: Loss=0.1369  Accuracy=99.06%\n",
            "Test set: loss=2.2890, accuracy=85.41%\n",
            "Test set: loss=1.5584, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0020\n",
            "Batch 100/156 - Loss 0.0328\n",
            "Batch 150/156 - Loss 0.0040\n",
            "Epoch 29: Loss=0.0673  Accuracy=99.52%\n",
            "Test set: loss=2.4090, accuracy=88.06%\n",
            "Test set: loss=1.9723, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.2155\n",
            "Batch 100/156 - Loss 0.0033\n",
            "Batch 150/156 - Loss 0.0063\n",
            "Epoch 30: Loss=0.0929  Accuracy=99.37%\n",
            "Test set: loss=2.2951, accuracy=87.80%\n",
            "Test set: loss=1.6842, accuracy=91.51%\n",
            "Batch 50/156 - Loss 0.0047\n",
            "Batch 100/156 - Loss 0.0075\n",
            "Batch 150/156 - Loss 0.0091\n",
            "Epoch 31: Loss=0.1747  Accuracy=98.76%\n",
            "Test set: loss=2.2005, accuracy=87.53%\n",
            "Test set: loss=1.8195, accuracy=89.12%\n",
            "Batch 50/156 - Loss 0.0072\n",
            "Batch 100/156 - Loss 0.0072\n",
            "Batch 150/156 - Loss 0.2900\n",
            "Epoch 32: Loss=0.1185  Accuracy=99.18%\n",
            "Test set: loss=1.9659, accuracy=89.92%\n",
            "Test set: loss=1.7010, accuracy=90.98%\n",
            "New best model saved at epoch 32 with average acc 89.92, female acc 89.92, male acc 90.98%\n",
            "Batch 50/156 - Loss 0.0054\n",
            "Batch 100/156 - Loss 0.3328\n",
            "Batch 150/156 - Loss 0.0035\n",
            "Epoch 33: Loss=0.1162  Accuracy=99.13%\n",
            "Test set: loss=1.6976, accuracy=89.66%\n",
            "Test set: loss=1.7782, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.3090\n",
            "Batch 100/156 - Loss 0.0043\n",
            "Batch 150/156 - Loss 0.0099\n",
            "Epoch 34: Loss=0.1001  Accuracy=99.29%\n",
            "Test set: loss=1.8867, accuracy=89.12%\n",
            "Test set: loss=1.7994, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.2581\n",
            "Batch 100/156 - Loss 0.0038\n",
            "Batch 150/156 - Loss 0.0035\n",
            "Epoch 35: Loss=0.0819  Accuracy=99.45%\n",
            "Test set: loss=2.5858, accuracy=86.47%\n",
            "Test set: loss=1.6631, accuracy=92.04%\n",
            "Batch 50/156 - Loss 0.0017\n",
            "Batch 100/156 - Loss 0.0060\n",
            "Batch 150/156 - Loss 0.0022\n",
            "Epoch 36: Loss=0.0546  Accuracy=99.65%\n",
            "Test set: loss=2.1714, accuracy=88.86%\n",
            "Test set: loss=1.8665, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0020\n",
            "Batch 100/156 - Loss 0.0018\n",
            "Batch 150/156 - Loss 0.0025\n",
            "Epoch 37: Loss=0.0610  Accuracy=99.60%\n",
            "Test set: loss=1.9829, accuracy=90.45%\n",
            "Test set: loss=1.9347, accuracy=90.98%\n",
            "New best model saved at epoch 37 with average acc 90.45, female acc 90.45, male acc 90.98%\n",
            "Batch 50/156 - Loss 0.0026\n",
            "Batch 100/156 - Loss 0.6546\n",
            "Batch 150/156 - Loss 0.0033\n",
            "Epoch 38: Loss=0.0487  Accuracy=99.68%\n",
            "Test set: loss=2.1949, accuracy=89.39%\n",
            "Test set: loss=1.8651, accuracy=90.98%\n",
            "Batch 50/156 - Loss 0.0013\n",
            "Batch 100/156 - Loss 0.0024\n",
            "Batch 150/156 - Loss 0.0079\n",
            "Epoch 39: Loss=0.0601  Accuracy=99.62%\n",
            "Test set: loss=2.3052, accuracy=88.59%\n",
            "Test set: loss=1.7652, accuracy=91.25%\n",
            "Batch 50/156 - Loss 0.2069\n",
            "Batch 100/156 - Loss 0.0027\n",
            "Batch 150/156 - Loss 0.0044\n",
            "Epoch 40: Loss=0.0809  Accuracy=99.50%\n",
            "Test set: loss=2.1818, accuracy=87.00%\n",
            "Test set: loss=1.5646, accuracy=92.04%\n",
            "Batch 50/156 - Loss 0.3034\n",
            "Batch 100/156 - Loss 0.0096\n",
            "Batch 150/156 - Loss 0.3612\n",
            "Epoch 41: Loss=0.0887  Accuracy=99.39%\n",
            "Test set: loss=2.1637, accuracy=89.39%\n",
            "Test set: loss=1.7152, accuracy=91.51%\n",
            "Batch 50/156 - Loss 0.4698\n",
            "Batch 100/156 - Loss 0.0032\n",
            "Batch 150/156 - Loss 0.2834\n",
            "Epoch 42: Loss=0.0816  Accuracy=99.40%\n",
            "Test set: loss=2.4956, accuracy=86.21%\n",
            "Test set: loss=1.8142, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.3399\n",
            "Batch 100/156 - Loss 0.0792\n",
            "Batch 150/156 - Loss 0.2757\n",
            "Epoch 43: Loss=0.0697  Accuracy=99.49%\n",
            "Test set: loss=1.9769, accuracy=90.19%\n",
            "Test set: loss=1.7491, accuracy=91.51%\n",
            "New best model saved at epoch 43 with average acc 90.19, female acc 90.19, male acc 91.51%\n",
            "Batch 50/156 - Loss 0.0010\n",
            "Batch 100/156 - Loss 0.0029\n",
            "Batch 150/156 - Loss 0.0028\n",
            "Epoch 44: Loss=0.0526  Accuracy=99.63%\n",
            "Test set: loss=2.4992, accuracy=87.80%\n",
            "Test set: loss=1.8009, accuracy=90.98%\n",
            "Batch 50/156 - Loss 0.0014\n",
            "Batch 100/156 - Loss 0.0018\n",
            "Batch 150/156 - Loss 0.0560\n",
            "Epoch 45: Loss=0.0430  Accuracy=99.72%\n",
            "Test set: loss=2.1413, accuracy=88.06%\n",
            "Test set: loss=1.6879, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.0018\n",
            "Batch 100/156 - Loss 0.3272\n",
            "Batch 150/156 - Loss 0.0031\n",
            "Epoch 46: Loss=0.0501  Accuracy=99.67%\n",
            "Test set: loss=2.2511, accuracy=87.80%\n",
            "Test set: loss=1.9196, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0040\n",
            "Batch 100/156 - Loss 0.0018\n",
            "Batch 150/156 - Loss 0.0018\n",
            "Epoch 47: Loss=0.0732  Accuracy=99.43%\n",
            "Test set: loss=2.5632, accuracy=87.27%\n",
            "Test set: loss=2.0237, accuracy=89.92%\n",
            "Batch 50/156 - Loss 0.0018\n",
            "Batch 100/156 - Loss 0.0034\n",
            "Batch 150/156 - Loss 0.3226\n",
            "Epoch 48: Loss=0.0709  Accuracy=99.55%\n",
            "Test set: loss=2.0786, accuracy=87.53%\n",
            "Test set: loss=1.8171, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0132\n",
            "Batch 100/156 - Loss 0.0036\n",
            "Batch 150/156 - Loss 0.0012\n",
            "Epoch 49: Loss=0.0925  Accuracy=99.27%\n",
            "Test set: loss=2.2174, accuracy=88.86%\n",
            "Test set: loss=2.2210, accuracy=88.59%\n",
            "Batch 50/156 - Loss 0.0051\n",
            "Batch 100/156 - Loss 0.0014\n",
            "Batch 150/156 - Loss 0.0493\n",
            "Epoch 50: Loss=0.0923  Accuracy=99.31%\n",
            "Test set: loss=2.2610, accuracy=85.94%\n",
            "Test set: loss=1.9600, accuracy=87.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title putting in the utils here for easier dev\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "V5RyGgI-xDUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=8/2550,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ozdMyUbKyike"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Selecting the first column of y (assuming it's the identity label)\n",
        "                loss = nn.CrossEntropyLoss()(output, y[:, 0])\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "P0OJ1YNLy6DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Selecting the first column of targets, assuming it represents the identity label\n",
        "        labels = targets[:, 0]\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train':  # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train_trades':  # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "BPenGmbpy_C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # change this to adam!!!\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "\n",
        "        val_loss_f, val_acc_f = evaluate(model, arc_head, val_loader_f, device)\n",
        "        val_loss_m, val_acc_m = evaluate(model, arc_head, val_loader_m, device)\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Avergae accuracy: {val_acc}, female: {val_acc_f}, male: {val_acc_m}')\n",
        "        print('================================================================')\n",
        "\n"
      ],
      "metadata": {
        "id": "toQXbD13z_VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# def train(model, train_loader, val_loader, pgd_attack,\n",
        "#           mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "#           checkpoint_path='model1.pt'):\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     # change this to adam!!!\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "#     best_acc = 0\n",
        "#     for epoch in range(epochs):\n",
        "#         # training\n",
        "#         train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "#         # evaluate clean accuracy\n",
        "#         test_loss, test_acc = evaluate(model, arc_head, val_loader, device)\n",
        "\n",
        "#         # remember best acc@1 and save checkpoint\n",
        "#         is_best = test_acc > best_acc\n",
        "#         best_acc = max(test_acc, best_acc)\n",
        "\n",
        "#         # save checkpoint if is a new best\n",
        "#         if is_best:\n",
        "#             torch.save(model.state_dict(), checkpoint_path)\n",
        "#         print(f'Accuracy: {test_acc}')\n",
        "#         print('================================================================')"
      ],
      "metadata": {
        "id": "EtEYn5jT-Cev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training loop with backbone using ArcFace\n",
        "\n",
        "# sanity check for attack, rasmus' suggestion\n",
        "\n",
        "\n",
        "epsilon = 8/255\n",
        "pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)  # instantiate the LinfPGDAttack\n",
        "training_mode = \"adv_train\"\n",
        "\n",
        "\n",
        "# note for us!! check the training loop and confirm whether it is similar to the previous one we had\n",
        "\n",
        "\n",
        "for proportion in proportions:\n",
        "    model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "    best_acc = 0.0\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize val_loader_f and val_loader_m to None\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    # Check if the subsets have any samples before creating DataLoaders\n",
        "    if len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=True)\n",
        "    if len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "    train(model, train_loader=train_loader, mode=training_mode, val_loader_f=val_loader_f, val_loader_m= val_loader_m, pgd_attack=pgd, learning_rate=0.001, checkpoint_path='model_adv.pt', epochs=70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "hmUtIMuSzm_r",
        "outputId": "bebf3e8f-3a5c-4599-eb2b-79038ae29702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 0.545271\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.505271\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.540523\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-1815b8fea78d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_m\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_loader_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_adv.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-e3fc131f7568>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader_f, val_loader_m, pgd_attack, mode, epochs, batch_size, learning_rate, momentum, weight_decay, checkpoint_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_ep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-d34fff666fa2>\u001b[0m in \u001b[0;36mtrain_ep\u001b[0;34m(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_ep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Selecting the first column of targets, assuming it represents the identity label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/celeba.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2296\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mResampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 )\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                     \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpulls_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                         \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check for attack, rasmus' suggestion\n",
        "\n",
        "epsilon = 8/255\n",
        "pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)  # instantiate the LinfPGDAttack\n",
        "training_mode = \"adv_train\"\n",
        "\n",
        "\n",
        "# note for us!! check the training loop and confirm whether it is similar to the previous one we had\n",
        "\n",
        "\n",
        "for proportion in proportions:\n",
        "    model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "    best_acc = 0.0\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize val_loader_f and val_loader_m to None\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    # Check if the subsets have any samples before creating DataLoaders\n",
        "    if len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=True)\n",
        "    if len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "    train(model, train_loader=train_loader, mode=training_mode, val_loader_f=val_loader_f, val_loader_m= val_loader_m, pgd_attack=pgd, learning_rate=0.001, checkpoint_path='model_adv.pt', epochs=70)\n"
      ],
      "metadata": {
        "id": "Z7dq1q1Xg37s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting this with a simpler model\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                loss = nn.CrossEntropyLoss()(output, y)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "iI3HBGKtzrWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "def make_dataloader(data_path, batch_size):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform_train)\n",
        "    val_dataset = datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def eval_test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n",
        "\n",
        "def mixup_data(x, y, mixup_alpha=1.0):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=0.003,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Lp-OlBD7pJmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        labels = targets[:, 0] # the first column is the identity label\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer)\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "id": "fKvyFN-QiJQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "        # evaluate clean accuracy\n",
        "        test_loss, test_acc = eval_test(model, val_loader, device)\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = test_acc > best_acc\n",
        "        best_acc = max(test_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print('================================================================')"
      ],
      "metadata": {
        "id": "H38K2cfuoASW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title resnet module\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n"
      ],
      "metadata": {
        "id": "4nLHgl8ekD7z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting this with a simpler model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = y[:, 0] # Assuming the first column is the identity label\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "                loss = nn.CrossEntropyLoss()(output, labels)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv\n",
        "\n",
        "# Modified train_ep function to handle multi-dimensional targets from CelebA\n",
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = targets[:, 0] # Assuming the first column is the identity label\n",
        "\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            # Pass the original multi-dimensional targets to the attack\n",
        "            adv_x = pgd_attack(inputs, targets) # The attack will extract labels internally\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            # Note: The original trades_loss function expects a 1D target tensor.\n",
        "            # You might need to adapt trades_loss similar to LinfPGDAttack if you plan to use this mode\n",
        "            # with CelebA's multi-dimensional targets. For now, using extracted labels might work,\n",
        "            # but the original TRADES formulation uses clean labels for the KL divergence part.\n",
        "            # This might require further adjustments based on the specific TRADES implementation\n",
        "            # you are using.\n",
        "            # Assuming for now that trades_loss can handle the extracted 1D labels.\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=labels, optimizer=optimizer)\n",
        "\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels.\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, labels)\n",
        "        #     adv_x = pgd_attack(inputs, targets) # Pass original targets to attack\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels from adv_x?\n",
        "        #     # This part of mixup with adversarial training might need careful consideration of how targets are handled.\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, labels) # Using extracted labels\n",
        "\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     # Use the extracted 1D labels for criterion\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "# Keep the rest of the training loop as is, ensuring the train function calls the modified train_ep\n",
        "# and evaluate functions (which already handle the identity label extraction).\n",
        "# Make sure the 'train' function signature matches how it's called in the loop:\n",
        "# train(model, train_loader=train_loader, mode=training_mode, val_loader_f=val_loader_f, val_loader_m= val_loader_m, pgd_attack=pgd, learning_rate=0.001, checkpoint_path='model_adv.pt', epochs=70)\n",
        "# The 'train' function you provided in the notebook takes 'val_loader', but your calling code\n",
        "# passes 'val_loader_f'. You should update the train function signature or the call site\n",
        "# to be consistent. Since you added logic for val_loader_f and val_loader_m in the loop\n",
        "# where the error occurred, it seems you intend to evaluate on female and male subsets\n",
        "# separately during training. You should adjust the 'train' function to accept both\n",
        "# val_loader_f and val_loader_m and call your 'evaluate' function twice for each epoch.\n",
        "\n",
        "\n",
        "# Here's the updated train function to accept female and male validation loaders\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # change this to adam!!!\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "    # Note: Your training loop defined the optimizer with arc_head parameters.\n",
        "    # The train function here needs to accept or define the optimizer appropriately.\n",
        "    # Let's assume the optimizer is defined outside and passed in, or redefined here\n",
        "    # to include arc_head parameters if using ArcFace. If using the simple ResNet18\n",
        "    # without ArcFace for this adversarial training part, the current optimizer definition\n",
        "    # (SGD or Adam on model.parameters() only) might be correct.\n",
        "    # Based on the trace, it seems you are using the simple ResNet18 for the PGD attack part.\n",
        "    # Let's keep the SGD optimizer as in the original train function definition.\n",
        "    # If you are using ArcFace, you need to adapt this train function as well.\n",
        "\n",
        "    # Based on the code in the failing cell block, the optimizer was defined *inside* the loop\n",
        "    # and included arc_head parameters. Let's adjust the train function to match that,\n",
        "    # assuming you are training with ArcFace in this section.\n",
        "    # **However**, the eval_test and eval_robust functions called within this 'train' function\n",
        "    # only take 'model', not 'arc_head'. You will need to adapt eval_test and eval_robust\n",
        "    # if they are intended to work with the ArcFace setup (which predicts identity classes\n",
        "    # using features from the backbone and the arc_head).\n",
        "    # Let's assume for now that the adversarial training is on the simpler ResNet18\n",
        "    # for a standard classification task (which seems to be what the PGD attack code expects),\n",
        "    # and the ArcFace training is a separate block of code. If you intend to do adversarial\n",
        "    # training *with* the ArcFace setup, you'll need more significant modifications to the\n",
        "    # attack and evaluation functions.\n",
        "\n",
        "    # Reverting to the optimizer definition as seen in the cell where the error occurred,\n",
        "    # which includes arc_head parameters, assuming you want to train the entire ArcFace model adversarially.\n",
        "    # **IMPORTANT:** This requires adapting eval_test and eval_robust to work with the ArcFace model.\n",
        "    # For simplicity and to fix the immediate error, let's assume the PGD attack is on the\n",
        "    # simple classification task and the ArcFace part is separate, or you need to pass both\n",
        "    # model and arc_head to the train_ep and evaluation functions.\n",
        "\n",
        "    # Let's assume the intention is to train the simple ResNet18 adversarially.\n",
        "    # In this case, the optimizer should only optimize model.parameters().\n",
        "    # If you want to adversarially train the ArcFace setup, you need to pass\n",
        "    # both the model (backbone) and arc_head to train_ep and the attack.\n",
        "\n",
        "    # Given the structure of the failing code block, it seems you are initializing a simple ResNet18\n",
        "    # *inside* the loop for each proportion and then calling this `train` function.\n",
        "    # This suggests adversarial training on the simple ResNet18.\n",
        "    # The optimizer should then be for the model's parameters only.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Using Adam as in your failing block, but only for model\n",
        "\n",
        "    best_acc = 0.0 # Keep track of best average accuracy across genders\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        # Pass the extracted labels in train_ep as modified above\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "\n",
        "        # Evaluate clean accuracy on both subsets using the eval_test function\n",
        "        # Note: eval_test currently calculates standard classification accuracy,\n",
        "        # not accuracy in an embedding space with ArcFace distance.\n",
        "        # If you want to evaluate with ArcFace, you need a different evaluation function.\n",
        "        # Assuming standard classification evaluation for now.\n",
        "        val_acc_f = 0.0\n",
        "        val_acc_m = 0.0\n",
        "        val_loss_f = 0.0\n",
        "        val_loss_m = 0.0\n",
        "\n",
        "        if val_loader_f and len(val_loader_f.dataset) > 0:\n",
        "            # eval_test is designed for 1D targets, but the DataLoader yields multi-dimensional targets.\n",
        "            # You need to adapt eval_test or the DataLoader to yield 1D targets for this evaluation.\n",
        "            # Or, pass the appropriate labels to eval_test.\n",
        "            # Let's modify eval_test slightly to extract the identity label.\n",
        "\n",
        "            # For consistency with the error source, let's assume eval_test needs 1D targets.\n",
        "            # We'll extract the labels before calling eval_test. This is not ideal as it\n",
        "            # involves iterating through the dataset again. A better approach would be to\n",
        "            # modify eval_test to accept multi-dimensional targets and extract the label internally.\n",
        "            # Given the constraint to fix the immediate error, let's assume eval_test needs 1D targets.\n",
        "            # However, eval_test takes a DataLoader, so modifying it is better.\n",
        "\n",
        "            # Let's adapt eval_test to extract the identity label.\n",
        "            # Need to redefine eval_test to handle the multi-dimensional target.\n",
        "            # **This requires modifying the eval_test function definition.**\n",
        "            # Assuming the `evaluate` function defined earlier (which takes arc_head and extracts labels)\n",
        "            # is the correct evaluation function for your setup, you should use that here.\n",
        "\n",
        "            # Replacing eval_test calls with the `evaluate` function used previously.\n",
        "            # This assumes `evaluate` is defined and available in this scope.\n",
        "            # The `evaluate` function also requires the arc_head, which is not passed to this `train` function.\n",
        "            # This highlights a mismatch in your code structure.\n",
        "            # Either the `train` function needs `arc_head` or the adversarial training part is meant for\n",
        "            # a model without ArcFace.\n",
        "\n",
        "            # Let's assume you want to adversarially train the simple ResNet18.\n",
        "            # In that case, the targets should be 1D class labels, and the model should output logits\n",
        "            # for the number of classes (identities). The ResNet18 is instantiated with num_classes=1000,\n",
        "            # which aligns with the idea of predicting identity.\n",
        "            # The `CelebA` dataset yields `identity_labels` which is already a tensor where the first column\n",
        "            # is the identity.\n",
        "\n",
        "            # Let's return to modifying `train_ep` and `LinfPGDAttack` to handle the multi-dimensional\n",
        "            # targets from the DataLoader and extract the first column for the loss calculation.\n",
        "            # This was done in the code block above this function definition.\n",
        "\n",
        "            # Now, for evaluation, we need a function that evaluates the model on the identity prediction task.\n",
        "            # The `evaluate` function defined earlier does this with the ArcFace setup.\n",
        "            # If you are using the simple ResNet18 directly for classification,\n",
        "            # you need an evaluation function that takes the model and a DataLoader,\n",
        "            # extracts the identity label, and calculates accuracy. Let's use `eval_test` but\n",
        "            # modify it to extract the target.\n",
        "\n",
        "            # Redefine eval_test here to handle CelebA targets\n",
        "            def eval_test_celeba(model, dataloader, device):\n",
        "                model.eval()\n",
        "                test_loss = 0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs, targets in dataloader:\n",
        "                        inputs, targets = inputs.to(device), targets.to(device)\n",
        "                        labels = targets[:, 0] # Extract identity label\n",
        "                        outputs = model(inputs)\n",
        "                        test_loss += F.cross_entropy(outputs, labels).item() * inputs.size(0)\n",
        "                        pred = outputs.max(1, keepdim=True)[1]\n",
        "                        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "                        total += inputs.size(0)\n",
        "                test_loss /= total if total > 0 else 1\n",
        "                accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "                print(f'Test: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)')\n",
        "                return test_loss, accuracy\n",
        "\n",
        "            val_loss_f, val_acc_f = eval_test_celeba(model, val_loader_f, device)\n",
        "\n",
        "        if val_loader_m and len(val_loader_m.dataset) > 0:\n",
        "            val_loss_m, val_acc_m = eval_test_celeba(model, val_loader_m, device)\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Average accuracy: {val_acc:.2f}, female: {val_acc_f:.2f}, male: {val_acc_m:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "fLKbosPFkqTN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "batch_size = 64\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=1000).to(device) # ResNet for identity classification\n",
        "    # Note: Number of classes (1000) should match the number of unique identities.\n",
        "    # You printed n_classes = dataset.identity.unique(), which might be the correct number.\n",
        "    # Adjust 1000 if needed based on actual unique identity count.\n",
        "    # Example: num_classes = len(dataset.identity.unique()) if identity labels are 0-indexed and contiguous,\n",
        "    # or num_classes = dataset.identity.unique().max() + 1 if they are 0-indexed but not contiguous,\n",
        "    # or the exact count of unique identities if they are arbitrary numbers.\n",
        "    # Based on your ArcFace code, it looks like you have 10177 identities. Let's use that.\n",
        "    # num_classes = 10177 # Update based on your actual data\n",
        "    # However, the ResNet18 linear layer is initialized for 1000 classes.\n",
        "    # To use 10177 identities with ResNet18, you need to adjust the linear layer size.\n",
        "    # Let's assume for this adversarial training part, you are training for a fixed number of classes like 1000\n",
        "    # as specified in the ResNet18 definition. If you intend 10177, you must modify the ResNet18 definition\n",
        "    # or re-initialize the linear layer. Sticking to 1000 for now as per the code.\n",
        "\n",
        "    num_identity_classes = 1000 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    if proportion in test_subsets_f and len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "    if proportion in test_subsets_m and len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "\n",
        "\n",
        "    # Call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader_f=val_loader_f, val_loader_m=val_loader_m,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=70) # Save checkpoints with proportion"
      ],
      "metadata": {
        "id": "-THEtjHpv_T_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f0b73f-e5d7-4d6e-9f4f-c6e40665fc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 6.841019\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.855889\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.630322\n",
            "Train Epoch: 0 [09664/39936 (97%)]\t Loss: 0.733501\n",
            "Test: Average loss: 0.5166, Accuracy: 330/377 (88%)\n",
            "Test: Average loss: 0.5107, Accuracy: 326/377 (86%)\n",
            "Average accuracy: 87.00, female: 87.53, male: 86.47\n",
            "================================================================\n",
            "Train Epoch: 1 [00064/39936 (1%)]\t Loss: 0.667204\n",
            "Train Epoch: 1 [03264/39936 (33%)]\t Loss: 0.585597\n",
            "Train Epoch: 1 [06464/39936 (65%)]\t Loss: 0.665744\n",
            "Train Epoch: 1 [09664/39936 (97%)]\t Loss: 0.494106\n",
            "Test: Average loss: 0.8490, Accuracy: 32/377 (8%)\n",
            "Test: Average loss: 0.8726, Accuracy: 36/377 (10%)\n",
            "Average accuracy: 9.02, female: 8.49, male: 9.55\n",
            "================================================================\n",
            "Train Epoch: 2 [00064/39936 (1%)]\t Loss: 0.841729\n",
            "Train Epoch: 2 [03264/39936 (33%)]\t Loss: 0.738220\n",
            "Train Epoch: 2 [06464/39936 (65%)]\t Loss: 0.666434\n",
            "Train Epoch: 2 [09664/39936 (97%)]\t Loss: 0.684564\n",
            "Test: Average loss: 0.6515, Accuracy: 346/377 (92%)\n",
            "Test: Average loss: 0.6441, Accuracy: 341/377 (90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "================================================================\n",
            "Train Epoch: 3 [00064/39936 (1%)]\t Loss: 0.651965\n",
            "Train Epoch: 3 [03264/39936 (33%)]\t Loss: 0.684205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R382yizsxyUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}