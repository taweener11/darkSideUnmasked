{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taweener11/darkSideUnmasked/blob/main/5_28_intersectional_fixed_demogpairs_dataset_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title setting up reproducible pipeline that uses our shared colab folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYi8w2zpnKKE",
        "outputId": "92660f61-6c36-42a2-ba1a-79376ee786a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/My Drive/Datasets/demogpairs/DemogPairs.zip\" -d \"/content/demogpairs/\""
      ],
      "metadata": {
        "id": "ZMtrSvA3WA8J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title defining path\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "ROOT_DIR = '/content'\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "DEMOGPAIRS_FOLDER = os.path.join(ROOT_DIR, 'demogpairs')"
      ],
      "metadata": {
        "id": "C8td5koAnUZg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_metadata_file(filepath, gender_label, race_label):\n",
        "    \"\"\"\n",
        "    Read a DemogPairs metadata txt file and collect image paths with labels.\n",
        "\n",
        "    Args:\n",
        "      filepath (str): path to the metadata txt file\n",
        "      gender_label (int): 0 for female, 1 for male\n",
        "      race_label (str): string label for race, e.g. 'black', 'white', 'asian'\n",
        "\n",
        "    Returns:\n",
        "      List of tuples: (image_relative_path, gender_label, race_label)\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line or line.lower().startswith('db_code'):\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            # parts[1] is the image path relative to DemogPairs folder\n",
        "            img_path = parts[1]\n",
        "            samples.append((img_path, gender_label, race_label))\n",
        "    return samples\n"
      ],
      "metadata": {
        "id": "_OPo_ERHpYU_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "metadata_dir = '/content/demogpairs/Metadata'  # adjust path\n",
        "\n",
        "# map filenames to gender and race labels\n",
        "metadata_info = {\n",
        "    'Black_Females.txt': (0, 'black'),\n",
        "    'Black_Males.txt': (1, 'black'),\n",
        "    'White_Females.txt': (0, 'white'),\n",
        "    'White_Males.txt': (1, 'white'),\n",
        "    'Asian_Females.txt': (0, 'asian'),\n",
        "    'Asian_Males.txt': (1, 'asian')\n",
        "}\n",
        "\n",
        "all_samples = []\n",
        "\n",
        "for fname, (gender, race) in metadata_info.items():\n",
        "    full_path = os.path.join(metadata_dir, fname)\n",
        "    print(f\"Reading {full_path} ...\")\n",
        "    samples = read_metadata_file(full_path, gender, race)\n",
        "    all_samples.extend(samples)\n",
        "\n",
        "print(f\"Total samples loaded: {len(all_samples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uMJX7lHpaYx",
        "outputId": "708a6a45-4f86-4421-d1cc-bbe707f35459"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/demogpairs/Metadata/Black_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/Black_Males.txt ...\n",
            "Reading /content/demogpairs/Metadata/White_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/White_Males.txt ...\n",
            "Reading /content/demogpairs/Metadata/Asian_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/Asian_Males.txt ...\n",
            "Total samples loaded: 10800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demogpairs_root = '/content/demogpairs/DemogPairs'\n",
        "\n",
        "# build final dataset list with full paths\n",
        "dataset = []\n",
        "for rel_path, gender, race in all_samples:\n",
        "    img_full_path = os.path.join(demogpairs_root, rel_path)\n",
        "    if os.path.isfile(img_full_path):\n",
        "        dataset.append((img_full_path, gender, race))\n",
        "    else:\n",
        "        print(f\"Missing file: {img_full_path}\")\n",
        "\n",
        "print(f\"Final dataset size after filtering missing files: {len(dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih0pUXU9qU2o",
        "outputId": "d196d0af-6d26-49ec-b65b-c63319bb8a05"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataset size after filtering missing files: 10800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "gender_labels = np.array([s[1] for s in dataset])\n",
        "race_labels = np.array([s[2] for s in dataset])\n",
        "\n",
        "unique_races = np.unique(race_labels)\n",
        "balanced_indices = []\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# Find minimal count per race-gender group\n",
        "min_count = min(\n",
        "    np.sum((gender_labels == gender) & (race_labels == race))\n",
        "    for race in unique_races\n",
        "    for gender in [0,1]\n",
        ")\n",
        "print(f\"Balancing all race-gender groups to {min_count} samples each\")\n",
        "\n",
        "for race in unique_races:\n",
        "    for gender in [0,1]:\n",
        "        group_indices = np.where((race_labels == race) & (gender_labels == gender))[0]\n",
        "        rng.shuffle(group_indices)\n",
        "        balanced_indices.extend(group_indices[:min_count])\n",
        "\n",
        "balanced_dataset = [dataset[i] for i in balanced_indices]\n",
        "print(f\"Balanced dataset size (race+gender): {len(balanced_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsyPM0oEqhdV",
        "outputId": "3cf6c0ef-62d0-4d72-a221-f1a359e7bed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balancing all race-gender groups to 1800 samples each\n",
            "Balanced dataset size (race+gender): 10800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example: training indices subset (can be full train, or a subset)\n",
        "train_indices = np.arange(len(dataset))  # or your train split indices\n",
        "\n",
        "# Extract gender and race array for those indices\n",
        "gender_labels = np.array([dataset[i][1] for i in train_indices])  # 0=female, 1=male\n",
        "race_labels = np.array([dataset[i][2] for i in train_indices])    # e.g. 'black', 'white', 'asian'\n",
        "\n",
        "unique_races = np.unique(race_labels)\n",
        "print(f\"Unique races in training data: {unique_races}\")\n",
        "\n",
        "# Group indices by race and gender\n",
        "indices_by_race_gender = {}\n",
        "for race in unique_races:\n",
        "    for gender in [0, 1]:\n",
        "        mask = (race_labels == race) & (gender_labels == gender)\n",
        "        group_indices = train_indices[mask]\n",
        "        indices_by_race_gender[(race, gender)] = group_indices\n",
        "        print(f\"Count for {race} {'female' if gender == 0 else 'male'}: {len(group_indices)}\")\n",
        "\n",
        "# Find minimal count (for balanced sampling across all race+gender groups)\n",
        "min_count = min(len(idxs) for idxs in indices_by_race_gender.values())\n",
        "print(f\"Balancing all race-gender groups to {min_count} samples each\")\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "train_subsets_f = {}\n",
        "train_subsets_m = {}\n",
        "train_subsets = {}\n",
        "\n",
        "subset_sizes = [100, 500, 1000]  # example subset sizes total (must be divisible by number of groups)\n",
        "\n",
        "# Number of race-gender groups (race groups * 2 genders)\n",
        "num_groups = len(unique_races) * 2\n",
        "\n",
        "for size in subset_sizes:\n",
        "    # Adjust size to nearest lower multiple of num_groups\n",
        "    if size % num_groups != 0:\n",
        "        adjusted_size = (size // num_groups) * num_groups\n",
        "        print(f\"Adjusting subset size {size} -> {adjusted_size} for divisibility by {num_groups}\")\n",
        "        size = adjusted_size\n",
        "\n",
        "    per_group_n = size // num_groups\n",
        "    balanced_indices = []\n",
        "\n",
        "    for (race, gender), group_indices in indices_by_race_gender.items():\n",
        "        if len(group_indices) < per_group_n:\n",
        "            raise ValueError(f\"Group {race} {gender} has fewer samples ({len(group_indices)}) than requested {per_group_n}\")\n",
        "        shuffled = np.copy(group_indices)\n",
        "        rng.shuffle(shuffled)\n",
        "        balanced_indices.extend(shuffled[:per_group_n])\n",
        "\n",
        "        if gender == 0:\n",
        "            train_subsets_f.setdefault(size, []).extend(shuffled[:per_group_n])\n",
        "        else:\n",
        "            train_subsets_m.setdefault(size, []).extend(shuffled[:per_group_n])\n",
        "\n",
        "    balanced_indices = np.array(balanced_indices)\n",
        "    rng.shuffle(balanced_indices)\n",
        "\n",
        "    train_subsets[size] = balanced_indices\n",
        "\n",
        "print(f\"Females: {len(train_subsets_f[size])}, Males: {len(train_subsets_m[size])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzEQOxm5rm8G",
        "outputId": "2a9cfb66-2a96-46c6-863d-9747c95c6795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique races in training data: ['asian' 'black' 'white']\n",
            "Count for asian female: 1800\n",
            "Count for asian male: 1800\n",
            "Count for black female: 1800\n",
            "Count for black male: 1800\n",
            "Count for white female: 1800\n",
            "Count for white male: 1800\n",
            "Balancing all race-gender groups to 1800 samples each\n",
            "Adjusting subset size 100 -> 96 for divisibility by 6\n",
            "Adjusting subset size 500 -> 498 for divisibility by 6\n",
            "Adjusting subset size 1000 -> 996 for divisibility by 6\n",
            "Females: 498, Males: 498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Assuming dataset: list of (img_path, gender, race)\n",
        "dataset_gender_labels = np.array([s[1] for s in dataset])\n",
        "dataset_race_labels = np.array([s[2] for s in dataset])\n",
        "\n",
        "full_indices = np.arange(len(dataset))\n",
        "\n",
        "# Stratify on gender for train/test split (change as needed)\n",
        "train_indices, test_indices = train_test_split(\n",
        "    full_indices, test_size=0.2, random_state=42, stratify=dataset_gender_labels\n",
        ")\n",
        "print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "unique_races = np.unique(dataset_race_labels)\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# ========== Group 1: Gender partitions (all races combined) ==========\n",
        "gender_train_subsets = {}\n",
        "gender_test_subsets_f = {}\n",
        "gender_test_subsets_m = {}\n",
        "\n",
        "gender_train_labels = dataset_gender_labels[train_indices]\n",
        "gender_test_labels = dataset_gender_labels[test_indices]\n",
        "\n",
        "for p in proportions:\n",
        "    female_idx_train = np.where(gender_train_labels == 0)[0]\n",
        "    male_idx_train = np.where(gender_train_labels == 1)[0]\n",
        "    N_train = min(len(female_idx_train), len(male_idx_train))\n",
        "\n",
        "    rng.shuffle(female_idx_train)\n",
        "    rng.shuffle(male_idx_train)\n",
        "\n",
        "    num_female_train = int(N_train * p)\n",
        "    num_male_train = N_train - num_female_train\n",
        "\n",
        "    female_chosen = train_indices[female_idx_train[:num_female_train]]\n",
        "    male_chosen = train_indices[male_idx_train[:num_male_train]]\n",
        "\n",
        "    combined_train = np.concatenate([female_chosen, male_chosen])\n",
        "    rng.shuffle(combined_train)\n",
        "\n",
        "    gender_train_subsets[p] = Subset(dataset, combined_train)\n",
        "\n",
        "    # Balanced test set: half female, half male\n",
        "    female_idx_test = np.where(gender_test_labels == 0)[0]\n",
        "    male_idx_test = np.where(gender_test_labels == 1)[0]\n",
        "    N_test = min(len(female_idx_test), len(male_idx_test))\n",
        "\n",
        "    half_test = N_test // 2\n",
        "    female_test_chosen = test_indices[female_idx_test[:half_test]]\n",
        "    male_test_chosen = test_indices[male_idx_test[:half_test]]\n",
        "\n",
        "    gender_test_subsets_f[p] = Subset(dataset, female_test_chosen)\n",
        "    gender_test_subsets_m[p] = Subset(dataset, male_test_chosen)\n",
        "\n",
        "# ========== Group 2: Race partitions (all genders combined) ==========\n",
        "race_train_subsets = {}\n",
        "race_test_subsets = {}\n",
        "\n",
        "test_race_indices = {\n",
        "    race: test_indices[dataset_race_labels[test_indices] == race] for race in unique_races\n",
        "}\n",
        "\n",
        "for race in unique_races:\n",
        "    race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "    race_train_indices = train_indices[race_mask_train]\n",
        "    nonrace_mask_train = ~race_mask_train\n",
        "    nonrace_train_indices = train_indices[nonrace_mask_train]\n",
        "\n",
        "    total_race = len(race_train_indices)\n",
        "    total_nonrace = len(nonrace_train_indices)\n",
        "\n",
        "    if total_race == 0 or total_nonrace == 0:\n",
        "        print(f\"Warning: insufficient samples for race {race}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    for p in proportions:\n",
        "        num_race_samples = int(total_race * p)\n",
        "        # Number of non-race samples to keep proportional (approximate)\n",
        "        num_nonrace_samples = (\n",
        "            min(total_nonrace, int(num_race_samples * (1 - p) / p)) if p > 0 else total_nonrace\n",
        "        )\n",
        "\n",
        "        rng.shuffle(race_train_indices)\n",
        "        rng.shuffle(nonrace_train_indices)\n",
        "\n",
        "        chosen_race = race_train_indices[:num_race_samples]\n",
        "        chosen_nonrace = nonrace_train_indices[:num_nonrace_samples]\n",
        "\n",
        "        combined = np.concatenate([chosen_race, chosen_nonrace])\n",
        "        rng.shuffle(combined)\n",
        "\n",
        "        race_train_subsets[(race, p)] = Subset(dataset, combined)\n",
        "        race_test_subsets[race] = Subset(dataset, test_race_indices[race])\n",
        "\n",
        "# ========== Group 3: Intersectional partitions (race + gender) ==========\n",
        "intersection_train_subsets = {}\n",
        "intersection_test_subsets = {}\n",
        "\n",
        "for p in proportions:\n",
        "    train_indices_per_group = []\n",
        "    test_indices_per_group = []\n",
        "\n",
        "    for race in unique_races:\n",
        "        for gender in [0, 1]:  # 0: female, 1: male\n",
        "            # Mask for train intersection group\n",
        "            train_mask = (dataset_race_labels[train_indices] == race) & \\\n",
        "                         (dataset_gender_labels[train_indices] == gender)\n",
        "            test_mask = (dataset_race_labels[test_indices] == race) & \\\n",
        "                        (dataset_gender_labels[test_indices] == gender)\n",
        "\n",
        "            train_group_indices = train_indices[train_mask]\n",
        "            test_group_indices = test_indices[test_mask]\n",
        "\n",
        "            n_train = len(train_group_indices)\n",
        "            n_test = len(test_group_indices)\n",
        "\n",
        "            if n_train == 0 or n_test == 0:\n",
        "                print(f\"Skipping group race={race} gender={gender} due to insufficient samples \"\n",
        "                      f\"(train: {n_train}, test: {n_test})\")\n",
        "                continue\n",
        "\n",
        "            # Number of samples to pick scaled by p (at least 1 if non-empty)\n",
        "            num_train_samples = max(1, int(n_train * p))\n",
        "            num_test_samples = max(1, int(n_test * p))\n",
        "\n",
        "            rng.shuffle(train_group_indices)\n",
        "            rng.shuffle(test_group_indices)\n",
        "\n",
        "            chosen_train = train_group_indices[:num_train_samples]\n",
        "            chosen_test = test_group_indices[:num_test_samples]\n",
        "\n",
        "            train_indices_per_group.append(chosen_train)\n",
        "            test_indices_per_group.append(chosen_test)\n",
        "\n",
        "    if len(train_indices_per_group) == 0 or len(test_indices_per_group) == 0:\n",
        "        print(f\"No valid intersectional groups found for proportion {p}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    combined_train_indices = np.concatenate(train_indices_per_group)\n",
        "    combined_test_indices = np.concatenate(test_indices_per_group)\n",
        "    rng.shuffle(combined_train_indices)\n",
        "    rng.shuffle(combined_test_indices)\n",
        "\n",
        "    intersection_train_subsets[p] = Subset(dataset, combined_train_indices)\n",
        "    intersection_test_subsets[p] = Subset(dataset, combined_test_indices)\n",
        "\n",
        "# ========== Summary Printing ==========\n",
        "print(\"\\n=== Gender partitions (all races combined) ===\")\n",
        "for p in proportions:\n",
        "    train_subset = gender_train_subsets[p]\n",
        "    train_genders = [dataset[i][1] for i in train_subset.indices]\n",
        "    total_train = len(train_genders)\n",
        "    pct_female = np.mean(np.array(train_genders) == 0) if total_train > 0 else 0\n",
        "    print(f\"Prop {int(p*100)}: Train samples = {total_train}, Female ratio = {pct_female*100:.2f}%\")\n",
        "    print(f\"  Test Females: {len(gender_test_subsets_f[p].indices)}\")\n",
        "    print(f\"  Test Males: {len(gender_test_subsets_m[p].indices)}\")\n",
        "\n",
        "print(\"\\n=== Race partitions (all genders combined) ===\")\n",
        "for (race, p), subset in sorted(race_train_subsets.items()):\n",
        "    train_size = len(subset.indices)\n",
        "    print(f\"Race {race} Proportion {int(p*100)}: Train samples = {train_size}\")\n",
        "for race in unique_races:\n",
        "    test_sub = race_test_subsets.get(race, None)\n",
        "    if test_sub:\n",
        "        print(f\"Race {race} Test samples: {len(test_sub.indices)}\")\n",
        "\n",
        "print(\"\\n=== Intersectional partitions (race + gender) ===\")\n",
        "for p, subset in intersection_train_subsets.items():\n",
        "    train_genders = [dataset[i][1] for i in subset.indices]\n",
        "    train_races = [dataset[i][2] for i in subset.indices]\n",
        "    total_train = len(train_genders)\n",
        "    pct_female = np.mean(np.array(train_genders) == 0) if total_train > 0 else 0\n",
        "    print(f\"Prop {int(p*100)}: Train samples = {total_train}, Female ratio = {pct_female*100:.2f}%\")\n",
        "    for race in unique_races:\n",
        "        for gender in [0, 1]:\n",
        "            count = sum(\n",
        "                (np.array(train_races) == race) & (np.array(train_genders) == gender)\n",
        "            )\n",
        "            gender_str = \"Female\" if gender == 0 else \"Male\"\n",
        "            print(f\"  Race {race} {gender_str}: {count} samples\")\n",
        "    test_subset = intersection_test_subsets.get(p, None)\n",
        "    if test_subset:\n",
        "        test_genders = [dataset[i][1] for i in test_subset.indices]\n",
        "        test_races = [dataset[i][2] for i in test_subset.indices]\n",
        "        total_test = len(test_genders)\n",
        "        pct_female_test = np.mean(np.array(test_genders) == 0) if total_test > 0 else 0\n",
        "        print(f\"  Test samples = {total_test}, Female ratio = {pct_female_test * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfsZY4qZN84X",
        "outputId": "92d9df38-15a6-4b0c-f47e-189b4e09c25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 8640, Test samples: 2160\n",
            "\n",
            "=== Gender partitions (all races combined) ===\n",
            "Prop 25: Train samples = 4320, Female ratio = 25.00%\n",
            "  Test Females: 540\n",
            "  Test Males: 540\n",
            "Prop 50: Train samples = 4320, Female ratio = 50.00%\n",
            "  Test Females: 540\n",
            "  Test Males: 540\n",
            "Prop 75: Train samples = 4320, Female ratio = 75.00%\n",
            "  Test Females: 540\n",
            "  Test Males: 540\n",
            "\n",
            "=== Race partitions (all genders combined) ===\n",
            "Race asian Proportion 25: Train samples = 2876\n",
            "Race asian Proportion 50: Train samples = 2876\n",
            "Race asian Proportion 75: Train samples = 2876\n",
            "Race black Proportion 25: Train samples = 2912\n",
            "Race black Proportion 50: Train samples = 2912\n",
            "Race black Proportion 75: Train samples = 2912\n",
            "Race white Proportion 25: Train samples = 2848\n",
            "Race white Proportion 50: Train samples = 2850\n",
            "Race white Proportion 75: Train samples = 2850\n",
            "Race asian Test samples: 724\n",
            "Race black Test samples: 687\n",
            "Race white Test samples: 749\n",
            "\n",
            "=== Intersectional partitions (race + gender) ===\n",
            "Prop 25: Train samples = 2158, Female ratio = 50.00%\n",
            "  Race asian Female: 362 samples\n",
            "  Race asian Male: 357 samples\n",
            "  Race black Female: 368 samples\n",
            "  Race black Male: 359 samples\n",
            "  Race white Female: 349 samples\n",
            "  Race white Male: 363 samples\n",
            "  Test samples = 538, Female ratio = 50.00%\n",
            "Prop 50: Train samples = 4319, Female ratio = 49.99%\n",
            "  Race asian Female: 724 samples\n",
            "  Race asian Male: 714 samples\n",
            "  Race black Female: 737 samples\n",
            "  Race black Male: 719 samples\n",
            "  Race white Female: 698 samples\n",
            "  Race white Male: 727 samples\n",
            "  Test samples = 1079, Female ratio = 49.95%\n",
            "Prop 75: Train samples = 6478, Female ratio = 50.00%\n",
            "  Race asian Female: 1086 samples\n",
            "  Race asian Male: 1071 samples\n",
            "  Race black Female: 1106 samples\n",
            "  Race black Male: 1078 samples\n",
            "  Race white Female: 1047 samples\n",
            "  Race white Male: 1090 samples\n",
            "  Test samples = 1618, Female ratio = 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# ----------------------------- #\n",
        "# Assume dataset is a list of (img_path, gender, race)\n",
        "dataset_gender_labels = np.array([s[1] for s in dataset])\n",
        "dataset_race_labels = np.array([s[2] for s in dataset])\n",
        "\n",
        "# Stratified train/test split by gender (to preserve gender distribution)\n",
        "full_indices = np.arange(len(dataset))\n",
        "train_indices, test_indices = train_test_split(\n",
        "    full_indices,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=dataset_gender_labels\n",
        ")\n",
        "print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]  # proportions to create subsets on\n",
        "unique_races = np.unique(dataset_race_labels)\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# --------------------------------- #\n",
        "# Group 1: Gender partitions (all races combined)\n",
        "gender_train_subsets = {}\n",
        "gender_test_subsets_f = {}\n",
        "gender_test_subsets_m = {}\n",
        "\n",
        "gender_train_labels = dataset_gender_labels[train_indices]\n",
        "gender_test_labels = dataset_gender_labels[test_indices]\n",
        "\n",
        "for p in proportions:\n",
        "    female_idx_train = np.where(gender_train_labels == 0)[0]\n",
        "    male_idx_train = np.where(gender_train_labels == 1)[0]\n",
        "    N_train = min(len(female_idx_train), len(male_idx_train))\n",
        "\n",
        "    rng.shuffle(female_idx_train)\n",
        "    rng.shuffle(male_idx_train)\n",
        "\n",
        "    num_female_train = int(N_train * p)\n",
        "    num_male_train = N_train - num_female_train\n",
        "\n",
        "    chosen_female_train_abs = train_indices[female_idx_train[:num_female_train]]\n",
        "    chosen_male_train_abs = train_indices[male_idx_train[:num_male_train]]\n",
        "\n",
        "    combined_train = np.concatenate([chosen_female_train_abs, chosen_male_train_abs])\n",
        "    rng.shuffle(combined_train)\n",
        "\n",
        "    gender_train_subsets[p] = Subset(dataset, combined_train)\n",
        "\n",
        "    # Balanced test sets (50% female, 50% male) overall\n",
        "    female_idx_test = np.where(gender_test_labels == 0)[0]\n",
        "    male_idx_test = np.where(gender_test_labels == 1)[0]\n",
        "    N_test = min(len(female_idx_test), len(male_idx_test))\n",
        "\n",
        "    rng.shuffle(female_idx_test)\n",
        "    rng.shuffle(male_idx_test)\n",
        "\n",
        "    half_test = N_test // 2\n",
        "    chosen_female_test = test_indices[female_idx_test[:half_test]]\n",
        "    chosen_male_test = test_indices[male_idx_test[:half_test]]\n",
        "\n",
        "    gender_test_subsets_f[p] = Subset(dataset, chosen_female_test)\n",
        "    gender_test_subsets_m[p] = Subset(dataset, chosen_male_test)\n",
        "\n",
        "# --------------------------------- #\n",
        "# Group 2: Race partitions (all genders; varying race proportions)\n",
        "race_train_subsets = {}\n",
        "race_test_subsets = {}\n",
        "\n",
        "# Precompute test indices by race for reporting + subset creation\n",
        "test_race_indices = {\n",
        "    race: test_indices[dataset_race_labels[test_indices] == race] for race in unique_races\n",
        "}\n",
        "\n",
        "# Balanced test set for all races combined (same as gender balanced test set)\n",
        "female_test_idx = np.where(gender_test_labels == 0)[0]\n",
        "male_test_idx = np.where(gender_test_labels == 1)[0]\n",
        "N_test = min(len(female_test_idx), len(male_test_idx))\n",
        "rng.shuffle(female_test_idx)\n",
        "rng.shuffle(male_test_idx)\n",
        "half_test = N_test // 2\n",
        "global_test_females = test_indices[female_test_idx[:half_test]]\n",
        "global_test_males = test_indices[male_test_idx[:half_test]]\n",
        "common_test_subset_f = Subset(dataset, global_test_females)\n",
        "common_test_subset_m = Subset(dataset, global_test_males)\n",
        "\n",
        "for race in unique_races:\n",
        "    race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "    race_train_indices = train_indices[race_mask_train]\n",
        "    nonrace_train_indices = train_indices[~race_mask_train]\n",
        "\n",
        "    total_race = len(race_train_indices)\n",
        "    total_nonrace = len(nonrace_train_indices)\n",
        "\n",
        "    for p in proportions:\n",
        "        num_race_samples = int(total_race * p)\n",
        "        num_nonrace_samples = (\n",
        "            min(total_nonrace, int(num_race_samples * (1 - p) / p)) if p > 0 else total_nonrace\n",
        "        )\n",
        "\n",
        "        if num_race_samples == 0 or num_nonrace_samples == 0:\n",
        "            print(f\"Warning: insufficient data for race {race} with proportion {p}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        rng.shuffle(race_train_indices)\n",
        "        rng.shuffle(nonrace_train_indices)\n",
        "\n",
        "        chosen_race = race_train_indices[:num_race_samples]\n",
        "        chosen_nonrace = nonrace_train_indices[:num_nonrace_samples]\n",
        "\n",
        "        combined = np.concatenate([chosen_race, chosen_nonrace])\n",
        "        rng.shuffle(combined)\n",
        "\n",
        "        race_train_subsets[(race, p)] = Subset(dataset, combined)\n",
        "        # Store test subsets keyed by race only (tests do not depend on p)\n",
        "        race_test_subsets[race] = Subset(dataset, test_race_indices[race])\n",
        "\n",
        "# --------------------------------- #\n",
        "# Group 3: Intersectional partitions (race + gender combined)\n",
        "intersection_train_subsets = {}\n",
        "intersection_test_subsets_f = {}\n",
        "intersection_test_subsets_m = {}\n",
        "\n",
        "for race in unique_races:\n",
        "    race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "    race_train_indices = train_indices[race_mask_train]\n",
        "    gender_train_race = dataset_gender_labels[race_train_indices]\n",
        "\n",
        "    race_mask_test = (dataset_race_labels[test_indices] == race)\n",
        "    race_test_indices = test_indices[race_mask_test]\n",
        "    gender_test_race = dataset_gender_labels[race_test_indices]\n",
        "\n",
        "    female_train_idx = np.where(gender_train_race == 0)[0]\n",
        "    male_train_idx = np.where(gender_train_race == 1)[0]\n",
        "    female_test_idx = np.where(gender_test_race == 0)[0]\n",
        "    male_test_idx = np.where(gender_test_race == 1)[0]\n",
        "\n",
        "    N_train = min(len(female_train_idx), len(male_train_idx))\n",
        "    N_test = min(len(female_test_idx), len(male_test_idx))\n",
        "\n",
        "    if N_train == 0 or N_test == 0:\n",
        "        print(f\"Warning: insufficient data for race {race}, skipping intersectional subsets.\")\n",
        "        continue\n",
        "\n",
        "    for p in proportions:\n",
        "        rng.shuffle(female_train_idx)\n",
        "        rng.shuffle(male_train_idx)\n",
        "        rng.shuffle(female_test_idx)\n",
        "        rng.shuffle(male_test_idx)\n",
        "\n",
        "        num_female_train = int(N_train * p)\n",
        "        num_male_train = N_train - num_female_train\n",
        "\n",
        "        chosen_female_train_abs = race_train_indices[female_train_idx[:num_female_train]]\n",
        "        chosen_male_train_abs = race_train_indices[male_train_idx[:num_male_train]]\n",
        "\n",
        "        combined_train = np.concatenate([chosen_female_train_abs, chosen_male_train_abs])\n",
        "        rng.shuffle(combined_train)\n",
        "\n",
        "        # Unlike train, keep test balanced 50/50 regardless of p because test is fixed\n",
        "        half_test = N_test // 2\n",
        "        chosen_female_test_abs = race_test_indices[female_test_idx[:half_test]]\n",
        "        chosen_male_test_abs = race_test_indices[male_test_idx[:half_test]]\n",
        "\n",
        "        # Save per (race, p) key to avoid overwriting:\n",
        "        intersection_train_subsets[(race, p)] = Subset(dataset, combined_train)\n",
        "        intersection_test_subsets_f[(race, p)] = Subset(dataset, chosen_female_test_abs)\n",
        "        intersection_test_subsets_m[(race, p)] = Subset(dataset, chosen_male_test_abs)\n",
        "\n",
        "# --------------- Helpers for DataLoaders ----------------\n",
        "\n",
        "def get_train_loader(group, key, batch_size=64):\n",
        "    if group == 'gender':\n",
        "        dataset_subset = gender_train_subsets[key]\n",
        "    elif group == 'race':\n",
        "        dataset_subset = race_train_subsets[key]\n",
        "    elif group == 'intersectional':\n",
        "        dataset_subset = intersection_train_subsets[key]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown group '{group}'\")\n",
        "    return DataLoader(dataset_subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "def get_test_loaders(group, key, batch_size=64):\n",
        "    if group == 'gender':\n",
        "        return (\n",
        "            DataLoader(gender_test_subsets_f[key], batch_size=batch_size, shuffle=False),\n",
        "            DataLoader(gender_test_subsets_m[key], batch_size=batch_size, shuffle=False),\n",
        "        )\n",
        "    elif group == 'race':\n",
        "        # key can be race or (race, p); test subsets only saved by race, so get first element if tuple\n",
        "        race = key if not isinstance(key, tuple) else key[0]\n",
        "        return (DataLoader(race_test_subsets[race], batch_size=batch_size, shuffle=False), None)\n",
        "    elif group == 'intersectional':\n",
        "        # Intersectional test subsets saved with keys (race, p)\n",
        "        return (\n",
        "            DataLoader(intersection_test_subsets_f[key], batch_size=batch_size, shuffle=False),\n",
        "            DataLoader(intersection_test_subsets_m[key], batch_size=batch_size, shuffle=False),\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown group '{group}'\")\n",
        "\n",
        "# --------------- Example usage ----------------\n",
        "\n",
        "batch_size = 64\n",
        "# Gender group loader example (50% females)\n",
        "train_loader_gender = get_train_loader('gender', 0.5, batch_size)\n",
        "# Race group loader example (50% proportion of 'black')\n",
        "train_loader_race = get_train_loader('race', ('black', 0.5), batch_size)\n",
        "# Intersectional group loader example ('white', 75% females)\n",
        "train_loader_intersection = get_train_loader('intersectional', ('white', 0.75), batch_size)\n",
        "\n",
        "print(f\"Gender group train loader batches: {len(train_loader_gender)}\")\n",
        "print(f\"Race group train loader batches: {len(train_loader_race)}\")\n",
        "print(f\"Intersectional group train loader batches: {len(train_loader_intersection)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBiKz7G9yyFb",
        "outputId": "b57c0efb-5ac0-49d9-d0b3-d59f00840e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 8640, Test samples: 2160\n",
            "Gender group train loader batches: 68\n",
            "Race group train loader batches: 46\n",
            "Intersectional group train loader batches: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "\n",
        "def evaluate(model, arc_head, loader, device):\n",
        "    model.eval()\n",
        "    arc_head.eval()\n",
        "    running_corrects = 0\n",
        "    running_loss = 0\n",
        "    total_samples = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.squeeze(1).to(device)\n",
        "            features = model(images)\n",
        "            logits = arc_head(features, labels)\n",
        "            loss = criterion(logits, labels)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "    avg_loss = running_loss / total_samples if total_samples > 0 else 0\n",
        "    avg_acc = running_corrects / total_samples if total_samples > 0 else 0\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "def train_and_eval_group(name, train_subsets, test_subsets_f, test_subsets_m, proportions,\n",
        "                         backbone, arc_head, batch_size=64, epochs=5):\n",
        "    if all(isinstance(k, tuple) and len(k) == 3 for k in train_subsets.keys()):\n",
        "        # Intersectional groups keyed by (race, gender, p)\n",
        "        for (race, gender, p) in sorted(train_subsets.keys()):\n",
        "            gender_str = \"Female\" if gender == 0 else \"Male\"\n",
        "            print(f\"\\n== Group: {name} - {race} {gender_str} - Proportion {p} ==\")\n",
        "            model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "            arc = arc_head.to(device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = torch.optim.Adam(list(model.parameters()) + list(arc.parameters()), lr=1e-3)\n",
        "            best_acc = 0\n",
        "\n",
        "            train_loader = DataLoader(train_subsets[(race, gender, p)], batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(test_subsets_f[(race, gender, p)], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "                arc.train()\n",
        "                total_loss = total_correct = total_samples = 0\n",
        "                for images, labels in train_loader:\n",
        "                    images = images.to(device)\n",
        "                    labels = labels.squeeze(1).to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    features = model(images)\n",
        "                    logits = arc(features, labels)\n",
        "                    loss = criterion(logits, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    preds = torch.argmax(logits, dim=1)\n",
        "                    total_correct += (preds == labels).sum().item()\n",
        "                    total_loss += loss.item() * images.size(0)\n",
        "                    total_samples += images.size(0)\n",
        "                train_loss = total_loss / total_samples if total_samples > 0 else 0\n",
        "                train_acc = total_correct / total_samples if total_samples > 0 else 0\n",
        "                print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}%\")\n",
        "\n",
        "                val_loss, val_acc = evaluate(model, arc, val_loader, device)\n",
        "                print(f\"Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")\n",
        "\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    print(f\"New best val acc at epoch {epoch+1}: {best_acc*100:.2f}%\")\n",
        "        print(f\"== Completed training group {name} ==\\n\")\n",
        "\n",
        "    else:\n",
        "        # Non-intersectional groups, keyed by proportion or (race,p)\n",
        "        for p in proportions:\n",
        "            print(f\"\\n== Group: {name} - Proportion {p} ==\")\n",
        "            model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "            arc = arc_head.to(device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = torch.optim.Adam(list(model.parameters()) + list(arc.parameters()), lr=1e-3)\n",
        "            best_acc = 0\n",
        "\n",
        "            train_loader = DataLoader(train_subsets[p], batch_size=batch_size, shuffle=True)\n",
        "            val_loader_f = DataLoader(test_subsets_f[p], batch_size=batch_size, shuffle=False)\n",
        "            val_loader_m = DataLoader(test_subsets_m[p], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "                arc.train()\n",
        "                total_loss = total_correct = total_samples = 0\n",
        "                for images, labels in train_loader:\n",
        "                    images = images.to(device)\n",
        "                    labels = labels.squeeze(1).to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    features = model(images)\n",
        "                    logits = arc(features, labels)\n",
        "                    loss = criterion(logits, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    preds = torch.argmax(logits, dim=1)\n",
        "                    total_correct += (preds == labels).sum().item()\n",
        "                    total_loss += loss.item() * images.size(0)\n",
        "                    total_samples += images.size(0)\n",
        "                train_loss = total_loss / total_samples if total_samples > 0 else 0\n",
        "                train_acc = total_correct / total_samples if total_samples > 0 else 0\n",
        "                print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}%\")\n",
        "\n",
        "                val_loss_f, val_acc_f = evaluate(model, arc, val_loader_f, device)\n",
        "                val_loss_m, val_acc_m = evaluate(model, arc, val_loader_m, device)\n",
        "                avg_val_acc = (val_acc_f + val_acc_m) / 2\n",
        "                print(f\"Val Acc Female: {val_acc_f*100:.2f}%, Male: {val_acc_m*100:.2f}%, Avg: {avg_val_acc*100:.2f}%\")\n",
        "\n",
        "                if avg_val_acc > best_acc:\n",
        "                    best_acc = avg_val_acc\n",
        "                    print(f\"New best avg val acc at epoch {epoch+1}: {best_acc*100:.2f}%\")\n",
        "        print(f\"== Completed training group {name} ==\\n\")\n",
        "\n",
        "# Dummy backbone and ArcHead as before\n",
        "class DummyArcHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "    def forward(self, x, y):\n",
        "        return self.fc(x)\n",
        "\n",
        "backbone = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),\n",
        "    nn.Flatten()\n",
        ")\n",
        "\n",
        "arc_head = DummyArcHead()\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.data = torch.randn(size, 3, 64, 64)\n",
        "        self.labels = torch.randint(0, 10, (size, 1))\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "class DummySubset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.data = DummyDataset(size)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "unique_races = ['asian', 'black', 'white']\n",
        "\n",
        "# Dummy subsets for gender level (all races combined)\n",
        "gender_train_subsets = {p: DummySubset(80) for p in proportions}\n",
        "gender_test_subsets_f = {p: DummySubset(10) for p in proportions}\n",
        "gender_test_subsets_m = {p: DummySubset(10) for p in proportions}\n",
        "\n",
        "# Dummy subsets for race level (all genders combined)\n",
        "race_train_subsets = {(race, p): DummySubset(80) for race in unique_races for p in proportions}\n",
        "race_test_subsets_f = {p: DummySubset(10) for p in proportions}  # reuse female/male for simplicity below\n",
        "race_test_subsets_m = {p: DummySubset(10) for p in proportions}\n",
        "\n",
        "# Dummy subsets for intersectional groups (each race × gender × proportion)\n",
        "intersection_train_subsets = {}\n",
        "intersection_test_subsets_f = {}\n",
        "intersection_test_subsets_m = {}\n",
        "for race in unique_races:\n",
        "    for gender in [0, 1]:\n",
        "        for p in proportions:\n",
        "            intersection_train_subsets[(race, gender, p)] = DummySubset(80)\n",
        "            intersection_test_subsets_f[(race, gender, p)] = DummySubset(10) if gender == 0 else DummySubset(0)\n",
        "            intersection_test_subsets_m[(race, gender, p)] = DummySubset(10) if gender == 1 else DummySubset(0)\n",
        "\n",
        "print(\"=== Starting training on gender partitions ===\")\n",
        "train_and_eval_group(\"Gender\", gender_train_subsets, gender_test_subsets_f, gender_test_subsets_m,\n",
        "                     proportions, backbone, arc_head, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "print(\"=== Starting training on race partitions ===\")\n",
        "for race in unique_races:\n",
        "    test_f_filtered = {p: race_test_subsets_f[p] for p in proportions}\n",
        "    test_m_filtered = {p: race_test_subsets_m[p] for p in proportions}\n",
        "    train_filt = {p: race_train_subsets[(race, p)] for p in proportions}\n",
        "    train_and_eval_group(f\"Race-{race}\", train_filt, test_f_filtered, test_m_filtered,\n",
        "                         proportions, backbone, arc_head, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "print(\"=== Starting training on intersectional partitions ===\")\n",
        "train_and_eval_group(\"Intersectional\", intersection_train_subsets,\n",
        "                     intersection_test_subsets_f, intersection_test_subsets_m,\n",
        "                     proportions, backbone, arc_head, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "print(\"All training runs complete.\")"
      ],
      "metadata": {
        "id": "Bqjbt__q4rDY",
        "outputId": "29337288-cd58-4bf7-93af-9b690c3989dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Starting training on gender partitions ===\n",
            "\n",
            "== Group: Gender - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.2851, Train Acc=16.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "New best avg val acc at epoch 1: 10.00%\n",
            "Epoch 2: Train Loss=2.2834, Train Acc=16.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 3: Train Loss=2.2820, Train Acc=16.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 4: Train Loss=2.2812, Train Acc=16.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 5: Train Loss=2.2800, Train Acc=16.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "\n",
            "== Group: Gender - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.2935, Train Acc=13.75%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "New best avg val acc at epoch 1: 10.00%\n",
            "Epoch 2: Train Loss=2.2915, Train Acc=13.75%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 3: Train Loss=2.2899, Train Acc=13.75%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 4: Train Loss=2.2884, Train Acc=13.75%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 5: Train Loss=2.2871, Train Acc=13.75%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "\n",
            "== Group: Gender - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3544, Train Acc=5.00%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 2: Train Loss=2.3498, Train Acc=5.00%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 3: Train Loss=2.3457, Train Acc=5.00%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 4: Train Loss=2.3421, Train Acc=5.00%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 5: Train Loss=2.3381, Train Acc=5.00%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "New best avg val acc at epoch 5: 5.00%\n",
            "== Completed training group Gender ==\n",
            "\n",
            "=== Starting training on race partitions ===\n",
            "\n",
            "== Group: Race-asian - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.2781, Train Acc=10.00%\n",
            "Val Acc Female: 0.00%, Male: 10.00%, Avg: 5.00%\n",
            "New best avg val acc at epoch 1: 5.00%\n",
            "Epoch 2: Train Loss=2.2762, Train Acc=16.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "New best avg val acc at epoch 2: 10.00%\n",
            "Epoch 3: Train Loss=2.2748, Train Acc=16.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 4: Train Loss=2.2735, Train Acc=16.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 5: Train Loss=2.2726, Train Acc=16.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "\n",
            "== Group: Race-asian - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3145, Train Acc=10.00%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 2: Train Loss=2.3115, Train Acc=10.00%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 3: Train Loss=2.3094, Train Acc=10.00%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 4: Train Loss=2.3068, Train Acc=10.00%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 5: Train Loss=2.3046, Train Acc=11.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "\n",
            "== Group: Race-asian - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3328, Train Acc=11.25%\n",
            "Val Acc Female: 20.00%, Male: 10.00%, Avg: 15.00%\n",
            "New best avg val acc at epoch 1: 15.00%\n",
            "Epoch 2: Train Loss=2.3288, Train Acc=11.25%\n",
            "Val Acc Female: 20.00%, Male: 10.00%, Avg: 15.00%\n",
            "Epoch 3: Train Loss=2.3256, Train Acc=11.25%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "New best avg val acc at epoch 3: 20.00%\n",
            "Epoch 4: Train Loss=2.3224, Train Acc=11.25%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "Epoch 5: Train Loss=2.3193, Train Acc=11.25%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "== Completed training group Race-asian ==\n",
            "\n",
            "\n",
            "== Group: Race-black - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.2904, Train Acc=15.00%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "New best avg val acc at epoch 1: 10.00%\n",
            "Epoch 2: Train Loss=2.2889, Train Acc=15.00%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 3: Train Loss=2.2882, Train Acc=15.00%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 4: Train Loss=2.2870, Train Acc=15.00%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 5: Train Loss=2.2864, Train Acc=15.00%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "\n",
            "== Group: Race-black - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3239, Train Acc=11.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 2: Train Loss=2.3209, Train Acc=11.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 3: Train Loss=2.3187, Train Acc=11.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 4: Train Loss=2.3166, Train Acc=11.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 5: Train Loss=2.3145, Train Acc=11.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "\n",
            "== Group: Race-black - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3319, Train Acc=2.50%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "New best avg val acc at epoch 1: 20.00%\n",
            "Epoch 2: Train Loss=2.3288, Train Acc=2.50%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "Epoch 3: Train Loss=2.3259, Train Acc=2.50%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "Epoch 4: Train Loss=2.3229, Train Acc=2.50%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "Epoch 5: Train Loss=2.3208, Train Acc=2.50%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "== Completed training group Race-black ==\n",
            "\n",
            "\n",
            "== Group: Race-white - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.3253, Train Acc=11.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "New best avg val acc at epoch 1: 10.00%\n",
            "Epoch 2: Train Loss=2.3221, Train Acc=11.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 3: Train Loss=2.3192, Train Acc=11.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 4: Train Loss=2.3165, Train Acc=11.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "Epoch 5: Train Loss=2.3135, Train Acc=11.25%\n",
            "Val Acc Female: 10.00%, Male: 10.00%, Avg: 10.00%\n",
            "\n",
            "== Group: Race-white - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3088, Train Acc=7.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 2: Train Loss=2.3051, Train Acc=7.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 3: Train Loss=2.3021, Train Acc=7.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 4: Train Loss=2.2988, Train Acc=7.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 5: Train Loss=2.2960, Train Acc=7.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "\n",
            "== Group: Race-white - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.2891, Train Acc=13.75%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "New best avg val acc at epoch 1: 20.00%\n",
            "Epoch 2: Train Loss=2.2867, Train Acc=13.75%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "Epoch 3: Train Loss=2.2853, Train Acc=13.75%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "Epoch 4: Train Loss=2.2841, Train Acc=13.75%\n",
            "Val Acc Female: 30.00%, Male: 10.00%, Avg: 20.00%\n",
            "Epoch 5: Train Loss=2.2822, Train Acc=13.75%\n",
            "Val Acc Female: 20.00%, Male: 10.00%, Avg: 15.00%\n",
            "== Completed training group Race-white ==\n",
            "\n",
            "=== Starting training on intersectional partitions ===\n",
            "\n",
            "== Group: Intersectional - asian Female - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.3135, Train Acc=7.50%\n",
            "Val Loss=2.3289, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.3105, Train Acc=7.50%\n",
            "Val Loss=2.3279, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.3082, Train Acc=8.75%\n",
            "Val Loss=2.3273, Val Acc=10.00%\n",
            "New best val acc at epoch 3: 10.00%\n",
            "Epoch 4: Train Loss=2.3058, Train Acc=11.25%\n",
            "Val Loss=2.3267, Val Acc=10.00%\n",
            "Epoch 5: Train Loss=2.3039, Train Acc=11.25%\n",
            "Val Loss=2.3263, Val Acc=10.00%\n",
            "\n",
            "== Group: Intersectional - asian Female - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3087, Train Acc=11.25%\n",
            "Val Loss=2.2866, Val Acc=20.00%\n",
            "New best val acc at epoch 1: 20.00%\n",
            "Epoch 2: Train Loss=2.3066, Train Acc=8.75%\n",
            "Val Loss=2.2883, Val Acc=10.00%\n",
            "Epoch 3: Train Loss=2.3048, Train Acc=3.75%\n",
            "Val Loss=2.2888, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.3033, Train Acc=10.00%\n",
            "Val Loss=2.2895, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.3018, Train Acc=12.50%\n",
            "Val Loss=2.2905, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - asian Female - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3102, Train Acc=8.75%\n",
            "Val Loss=2.3133, Val Acc=10.00%\n",
            "New best val acc at epoch 1: 10.00%\n",
            "Epoch 2: Train Loss=2.3078, Train Acc=8.75%\n",
            "Val Loss=2.3130, Val Acc=10.00%\n",
            "Epoch 3: Train Loss=2.3060, Train Acc=8.75%\n",
            "Val Loss=2.3126, Val Acc=10.00%\n",
            "Epoch 4: Train Loss=2.3043, Train Acc=8.75%\n",
            "Val Loss=2.3124, Val Acc=10.00%\n",
            "Epoch 5: Train Loss=2.3025, Train Acc=8.75%\n",
            "Val Loss=2.3129, Val Acc=10.00%\n",
            "\n",
            "== Group: Intersectional - asian Male - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.3016, Train Acc=10.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.2998, Train Acc=10.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.2991, Train Acc=10.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.2980, Train Acc=10.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.2971, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - asian Male - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3014, Train Acc=7.50%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.2993, Train Acc=10.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.2974, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.2958, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.2942, Train Acc=12.50%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - asian Male - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3050, Train Acc=7.50%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.3035, Train Acc=6.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.3020, Train Acc=6.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.3008, Train Acc=8.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.2999, Train Acc=8.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - black Female - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.3052, Train Acc=16.25%\n",
            "Val Loss=2.3279, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.3024, Train Acc=16.25%\n",
            "Val Loss=2.3329, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.3005, Train Acc=16.25%\n",
            "Val Loss=2.3375, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.2986, Train Acc=16.25%\n",
            "Val Loss=2.3420, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.2970, Train Acc=16.25%\n",
            "Val Loss=2.3461, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - black Female - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3165, Train Acc=11.25%\n",
            "Val Loss=2.2433, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.3127, Train Acc=11.25%\n",
            "Val Loss=2.2408, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.3098, Train Acc=11.25%\n",
            "Val Loss=2.2384, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.3070, Train Acc=11.25%\n",
            "Val Loss=2.2353, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.3041, Train Acc=11.25%\n",
            "Val Loss=2.2320, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - black Female - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3105, Train Acc=6.25%\n",
            "Val Loss=2.3269, Val Acc=10.00%\n",
            "New best val acc at epoch 1: 10.00%\n",
            "Epoch 2: Train Loss=2.3089, Train Acc=7.50%\n",
            "Val Loss=2.3260, Val Acc=10.00%\n",
            "Epoch 3: Train Loss=2.3082, Train Acc=8.75%\n",
            "Val Loss=2.3257, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.3074, Train Acc=10.00%\n",
            "Val Loss=2.3255, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.3066, Train Acc=11.25%\n",
            "Val Loss=2.3249, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - black Male - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.3123, Train Acc=12.50%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.3099, Train Acc=12.50%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.3078, Train Acc=13.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.3062, Train Acc=13.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.3043, Train Acc=13.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - black Male - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.2960, Train Acc=13.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.2941, Train Acc=15.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.2928, Train Acc=13.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.2915, Train Acc=13.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.2904, Train Acc=13.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - black Male - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3092, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.3077, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.3067, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.3060, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.3047, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - white Female - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.2990, Train Acc=5.00%\n",
            "Val Loss=2.2988, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.2960, Train Acc=6.25%\n",
            "Val Loss=2.2951, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.2933, Train Acc=12.50%\n",
            "Val Loss=2.2918, Val Acc=10.00%\n",
            "New best val acc at epoch 3: 10.00%\n",
            "Epoch 4: Train Loss=2.2905, Train Acc=13.75%\n",
            "Val Loss=2.2882, Val Acc=10.00%\n",
            "Epoch 5: Train Loss=2.2881, Train Acc=13.75%\n",
            "Val Loss=2.2844, Val Acc=10.00%\n",
            "\n",
            "== Group: Intersectional - white Female - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3214, Train Acc=2.50%\n",
            "Val Loss=2.2879, Val Acc=10.00%\n",
            "New best val acc at epoch 1: 10.00%\n",
            "Epoch 2: Train Loss=2.3187, Train Acc=2.50%\n",
            "Val Loss=2.2881, Val Acc=10.00%\n",
            "Epoch 3: Train Loss=2.3168, Train Acc=2.50%\n",
            "Val Loss=2.2878, Val Acc=30.00%\n",
            "New best val acc at epoch 3: 30.00%\n",
            "Epoch 4: Train Loss=2.3145, Train Acc=10.00%\n",
            "Val Loss=2.2865, Val Acc=20.00%\n",
            "Epoch 5: Train Loss=2.3124, Train Acc=10.00%\n",
            "Val Loss=2.2846, Val Acc=10.00%\n",
            "\n",
            "== Group: Intersectional - white Female - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3075, Train Acc=10.00%\n",
            "Val Loss=2.3036, Val Acc=30.00%\n",
            "New best val acc at epoch 1: 30.00%\n",
            "Epoch 2: Train Loss=2.3058, Train Acc=10.00%\n",
            "Val Loss=2.3052, Val Acc=30.00%\n",
            "Epoch 3: Train Loss=2.3043, Train Acc=11.25%\n",
            "Val Loss=2.3070, Val Acc=20.00%\n",
            "Epoch 4: Train Loss=2.3031, Train Acc=7.50%\n",
            "Val Loss=2.3081, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.3019, Train Acc=13.75%\n",
            "Val Loss=2.3089, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - white Male - Proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.3186, Train Acc=5.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.3167, Train Acc=7.50%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.3149, Train Acc=8.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.3135, Train Acc=8.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.3119, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - white Male - Proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3061, Train Acc=5.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.3031, Train Acc=6.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.3002, Train Acc=11.25%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.2973, Train Acc=12.50%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.2954, Train Acc=12.50%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "\n",
            "== Group: Intersectional - white Male - Proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3073, Train Acc=5.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 2: Train Loss=2.3042, Train Acc=5.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 3: Train Loss=2.3015, Train Acc=5.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 4: Train Loss=2.2995, Train Acc=5.00%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "Epoch 5: Train Loss=2.2968, Train Acc=8.75%\n",
            "Val Loss=0.0000, Val Acc=0.00%\n",
            "== Completed training group Intersectional ==\n",
            "\n",
            "All training runs complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title shell pipeline for unzipping! this needs to run every time\n",
        "\n",
        "!unzip -q \"/content/drive/My Drive/Datasets/celeba/img_align_celeba.zip\" -d \"/content/celeba/\""
      ],
      "metadata": {
        "id": "SS8uNZYunxqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content' # setting it to the local environment"
      ],
      "metadata": {
        "id": "4uA1b3RswNS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "cn679gCpx6pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a transform that is smaller per suggestion of rasmus\n",
        "\n",
        "image_size = 64\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                          std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "oLYdzNAHyveC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfering files from gdrive to here so that they would work without us uploading manually all the time\n",
        "# import module\n",
        "import shutil\n",
        "\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/identity_CelebA.txt', '/content/celeba/identity_CelebA.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_attr_celeba.txt', '/content/celeba/list_attr_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_bbox_celeba.txt', '/content/celeba/list_bbox_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_landmarks_align_celeba.txt', '/content/celeba/list_landmarks_align_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_eval_partition.txt', '/content/celeba/list_eval_partition.txt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6CEeXlqPztTx",
        "outputId": "66df1422-b442-4df6-905c-fb7838d1afb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/celeba/list_eval_partition.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CelebA\n",
        "\n",
        "\n",
        "# it creates a folder on the go!\n",
        "\n",
        "try:\n",
        "    dataset = CelebA(\n",
        "        root='/content',\n",
        "        split='train',\n",
        "        target_type='attr',\n",
        "        transform=transform,\n",
        "        download=False # this works now!!!! its just important that it is in the root folder\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"CelebA error:\", e)"
      ],
      "metadata": {
        "id": "iyQji5uMx2yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = '/content/celeba'\n",
        "\n",
        "print(\"Root contents:\", os.listdir(data_dir))\n",
        "print(\"Images folder exists:\", os.path.isdir(os.path.join(data_dir, 'img_align_celeba')))\n",
        "print(\"Sample images:\", os.listdir(os.path.join(data_dir, 'img_align_celeba'))[:3])\n",
        "print(\"Has attribute file:\", os.path.isfile(os.path.join(data_dir, 'list_attr_celeba.txt')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jz1k1xJybHu",
        "outputId": "82549f4c-0c60-48db-b5a1-1f89825a4510",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root contents: ['identity_CelebA.txt', 'list_bbox_celeba.txt', 'list_attr_celeba.txt', 'list_eval_partition.txt', 'img_align_celeba', 'list_landmarks_align_celeba.txt']\n",
            "Images folder exists: True\n",
            "Sample images: ['085474.jpg', '129511.jpg', '100524.jpg']\n",
            "Has attribute file: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check 2 & the moment of truth!!\n",
        "\n",
        "# adding a dataloader and a basic model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "tw8aIHtsyj0Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting the training data, different distributions\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "test_dataset = CelebA(\n",
        "    root='/content',\n",
        "    split='test',\n",
        "    target_type='attr',\n",
        "    transform=transform,\n",
        "    download=False # i set it to true in case there is some secret metadata?? it is looking for\n",
        ")\n"
      ],
      "metadata": {
        "id": "ABpA6NKeQYRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Get the identity information from the training dataset\n",
        "identity_labels = dataset.identity\n",
        "# Convert to a pandas Series for easier counting\n",
        "identity_series = pd.Series(identity_labels.squeeze().numpy())\n",
        "identity_counts = identity_series.value_counts()\n",
        "top_1000_identities = identity_counts.nlargest(1000)\n",
        "# Get the indices corresponding to the top 1000 identities\n",
        "top_1000_indices = identity_series[identity_series.isin(top_1000_identities.index)].index\n",
        "# Create a subset of the dataset containing only the top 1000 identities\n",
        "dataset_top_1000 = Subset(dataset, top_1000_indices)\n",
        "\n",
        "\n",
        "min_samples = top_1000_identities.min()\n",
        "max_samples = top_1000_identities.max()\n",
        "\n",
        "print(f\"Minimum samples per identity: {min_samples}\")\n",
        "print(f\"Maximum samples per identity: {max_samples}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIYrBM44Fd_0",
        "outputId": "c866a686-325f-4b70-934c-a3dce3f2ae77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum samples per identity: 30\n",
            "Maximum samples per identity: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Make sure numpy is imported if it hasn't been already\n",
        "\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "gender_labels_test_subset = []\n",
        "for i in top_1000_indices:\n",
        "  # Note: As discussed before, using training indices on the test dataset\n",
        "  # might lead to issues or misalignment. Assuming this is intended for now.\n",
        "  if i < len(test_dataset):\n",
        "    gender_labels_test_subset.append(test_dataset.attr[i, male_idx])\n",
        "\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "gender_labels_test_subset_np = np.array(gender_labels_test_subset)\n",
        "\n",
        "\n",
        "# Now use np.where on the NumPy array\n",
        "# This is the part that fixes the DeprecationWarning\n",
        "female_test_subset_indices = np.where(gender_labels_test_subset_np == 0)[0]\n",
        "male_test_subset_indices   = np.where(gender_labels_test_subset_np ==  1)[0]\n",
        "\n",
        "\n",
        "print(len(female_test_subset_indices))\n",
        "print(len(male_test_subset_indices))\n",
        "\n",
        "\n",
        "N_test = min(len(female_test_subset_indices), len(male_test_subset_indices))\n",
        "\n",
        "rng_test = np.random.default_rng(seed=42)\n",
        "shuffled_female_test_subset_indices = np.copy(female_test_subset_indices)\n",
        "shuffled_male_test_subset_indices   = np.copy(male_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_female_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_male_test_subset_indices)\n",
        "\n",
        "\n",
        "test_subsets = {}\n",
        "\n",
        "# Create training subsets\n",
        "test_subsets_f = {}\n",
        "test_subsets_m = {}\n",
        "# even split for all examples. we can change this later but we want to be able to generalize... we want there to be the same number of examples for men and women and for these to be in the same set...\n",
        "# we will put this to the loop.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkTrbz2Az2Zy",
        "outputId": "35b89be6-3ea4-46a5-e252-c75fc19616ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300\n",
            "1510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "# choose smallest n\n",
        "# proportions = [0, 0.1, 0.25, 0.5, 0.75, 1.0] # changed this bc it doesn't make sense\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "# You need to create subsets from the test_dataset using test_dataset-specific indices\n",
        "# The previous code was creating subsets of the training dataset.\n",
        "# It seems like you want to create training subsets with varying gender proportions\n",
        "# and test subsets for evaluation (separated by gender).\n",
        "\n",
        "# For the training subsets (assuming you still want to use indices from the training dataset,\n",
        "# but with the identity filtering from before):\n",
        "# You will need to re-calculate the gender labels for the *training* dataset based on top_1000_indices.\n",
        "male_idx_train = dataset.attr_names.index('Male')\n",
        "gender_labels_train_subset = dataset.attr[top_1000_indices, male_idx_train] # Use gender from training dataset\n",
        "female_train_subset_indices = np.where(gender_labels_train_subset == 0)[0]\n",
        "male_train_subset_indices   = np.where(gender_labels_train_subset ==  1)[0]\n",
        "\n",
        "N_train = min(len(female_train_subset_indices), len(male_train_subset_indices))\n",
        "\n",
        "rng_train = np.random.default_rng(seed=42)\n",
        "shuffled_female_train_subset_indices = np.copy(female_train_subset_indices)\n",
        "shuffled_male_train_subset_indices   = np.copy(male_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_female_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_male_train_subset_indices)\n",
        "\n",
        "\n",
        "# Create training subsets\n",
        "train_subsets = {}\n",
        "for p in proportions:\n",
        "    num_females_train = int(N_train * p)\n",
        "    num_males_train = N_train - num_females_train\n",
        "\n",
        "    q = min(p, 1-p)\n",
        "    num_females_test = int(N_test * q) # even split for testing\n",
        "    num_males_test = num_females_test\n",
        "\n",
        "    chosen_female_train = shuffled_female_train_subset_indices[:num_females_train] if num_females_train > 0 else np.array([], dtype=int)\n",
        "    chosen_male_train   = shuffled_male_train_subset_indices[:num_males_train]   if num_males_train > 0   else np.array([], dtype=int)\n",
        "\n",
        "    chosen_female_test = shuffled_female_test_subset_indices[:num_females_test]\n",
        "    chosen_male_test   = shuffled_male_test_subset_indices[:num_males_test]\n",
        "\n",
        "    # These indices are relative to the 'dataset_top_1000' subset,\n",
        "    # so you need to map them back to the original 'dataset' indices if Subset requires it.\n",
        "    # Since top_1000_indices is the mapping, we can directly use that:\n",
        "    original_indices_train = np.concatenate([\n",
        "        top_1000_indices[chosen_female_train],\n",
        "        top_1000_indices[chosen_male_train]\n",
        "    ]).astype(int)\n",
        "    rng_train.shuffle(original_indices_train)\n",
        "    train_subsets[p] = Subset(dataset, original_indices_train)\n",
        "    test_subsets_f[p] = Subset(test_dataset, chosen_female_test)\n",
        "    test_subsets_m[p] = Subset(test_dataset, chosen_male_test)\n",
        "\n",
        "\n",
        "\n",
        "# Verification as before\n",
        "for p in proportions:\n",
        "    # Verification for the training subset\n",
        "    indices_train = train_subsets[p].indices\n",
        "    # Need to get genders for these original training indices from the *full* training dataset\n",
        "    genders_train = dataset.attr[indices_train, male_idx_train]\n",
        "    percent_female_train = (genders_train == 0).sum()/len(indices_train) if len(indices_train) > 0 else 0\n",
        "    print(f\"Train Subset (Prop {int(p*100)}%): Target {int(p*100)}% -- Actual {percent_female_train*100:.2f}% females, {(genders_train == 0).sum()} samples\")\n",
        "\n",
        "\n",
        "    number_female_test = len(test_subsets_f[p].indices)\n",
        "    number_male_test = len(test_subsets_m[p].indices)\n",
        "    print(f\"Number of female test samples: {number_female_test}\")\n",
        "    print(f\"Number of male test samples: {number_male_test}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2x4LIJezqg1",
        "outputId": "057d03ad-080d-4408-ecd0-5e88541ea02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Subset (Prop 25%): Target 25% -- Actual 24.99% females, 2480 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n",
            "Train Subset (Prop 50%): Target 50% -- Actual 50.00% females, 4961 samples\n",
            "Number of female test samples: 755\n",
            "Number of male test samples: 755\n",
            "Train Subset (Prop 75%): Target 75% -- Actual 74.99% females, 7441 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_subsets[0.5], batch_size=batch_size, shuffle=True)\n",
        "# val_loader = DataLoader(test_subsets[0.5], batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "V4HgisJbFAtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "goy8JCnuN5y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#title this is a more complicated model but used more commonly in FR\n",
        "\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.5, easy_margin=False):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = torch.cos(torch.tensor(self.m))\n",
        "        self.sin_m = torch.sin(torch.tensor(self.m))\n",
        "        self.th = torch.cos(torch.tensor(3.14159265 - self.m))\n",
        "        self.mm = torch.sin(torch.tensor(3.14159265 - self.m)) * self.m\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - torch.clamp(cosine ** 2, 0, 1))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        one_hot = torch.zeros_like(cosine)\n",
        "        one_hot.scatter_(1, label.view(-1, 1), 1)\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "_wFThr7cNaP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, arc_head, dataloader, device):\n",
        "    model.eval()\n",
        "    arc_head.eval()\n",
        "    total, correct, running_loss = 0, 0, 0.0\n",
        "    for i, (images, identity_labels) in enumerate(dataloader):\n",
        "        images, labels = images.to(device), identity_labels[:,0].to(device) # Selecting the first column of identity_labels\n",
        "        features = model(images)\n",
        "        logits = arc_head(features, labels) # Using the modified labels for arc_head\n",
        "        loss = criterion(logits, labels)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += images.size(0)\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    print(f\"Test set: loss={avg_loss:.4f}, accuracy={acc*100:.2f}, data loader{dataloader}%\")\n",
        "    return avg_loss, acc\n",
        "\n"
      ],
      "metadata": {
        "id": "EARq75acrB5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "backbone = models.resnet18(weights=None)\n",
        "feature_dim = backbone.fc.in_features\n",
        "backbone.fc = nn.Identity()\n",
        "\n",
        "n_classes=(dataset.identity.unique())\n",
        "print(n_classes)\n",
        "# arc_head = ArcMarginProduct(feature_dim, out_features=n_classes).to(device)\n",
        "arc_head = ArcMarginProduct(feature_dim, 10177).to(device) # Tracy update\n",
        "\n"
      ],
      "metadata": {
        "id": "Vr67fLnNNXWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fde6e0-2fa9-4912-f5c5-bbc8d43f4c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    1,     2,     3,  ..., 10175, 10176, 10177])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check for the eval code\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "\n",
        "\n",
        "\n",
        "# for epoch in range(1):\n",
        "#   avg_loss, acc = evaluate(model, arc_head, val_loader, device)"
      ],
      "metadata": {
        "id": "_oK13oicDSxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "best_acc = 0.0\n",
        "\n",
        "\n",
        "for proportion in proportions:\n",
        "    model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "    best_acc = 0.0\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize val_loader_f and val_loader_m to None\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    # Check if the subsets have any samples before creating DataLoaders\n",
        "    if len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=True)\n",
        "    if len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        arc_head.train()\n",
        "        total, correct, running_loss = 0, 0, 0.0\n",
        "        for i, (images, identity_labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), identity_labels[:,0].to(device) # Selecting the first column of identity_labels\n",
        "            features = model(images)\n",
        "            logits = arc_head(features, labels) # Using the modified labels for arc_head\n",
        "            loss = criterion(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += images.size(0)\n",
        "            if (i+1) % 50 == 0: print(f\"Batch {i+1}/{len(train_loader)} - Loss {loss.item():.4f}\")\n",
        "        print(f\"Epoch {epoch+1}: Loss={running_loss/total:.4f}  Accuracy={correct/total*100:.2f}%\")\n",
        "\n",
        "        # Check if the validation loaders are not empty before evaluating\n",
        "\n",
        "        if val_loader_f:\n",
        "            val_loss_f, val_acc_f = evaluate(model, arc_head, val_loader_f, device)\n",
        "        if val_loader_m:\n",
        "            val_loss_m, val_acc_m = evaluate(model, arc_head, val_loader_m, device)\n",
        "\n",
        "        if val_loader_m is not None and val_loader_f is not None:\n",
        "          val_acc = (val_acc_f + val_acc_m) / 2\n",
        "        elif val_loader_m:\n",
        "          val_acc = val_acc_m\n",
        "        elif val_loader_f:\n",
        "          val_acc = val_acc_f\n",
        "        else:\n",
        "          val_acc = 0.0\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "          best_acc = val_acc\n",
        "          torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'arc_head_state_dict': arc_head.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'val_acc': val_acc,\n",
        "          }, f'model{proportion}_checkpoint.pth')\n",
        "\n",
        "          if val_loader_m and val_loader_f:\n",
        "            print(f\"New best model saved at epoch {epoch+1} with average acc {val_acc_f*100:.2f}, female acc {val_acc_f*100:.2f}, male acc {val_acc_m*100:.2f}%\")\n",
        "          elif val_loader_m:\n",
        "            print(f\"New best model saved at epoch {epoch+1} with male acc {val_acc_m*100:.2f}%\")\n",
        "          elif val_loader_f:\n",
        "            print(f\"New best model saved at epoch {epoch+1} with female acc {val_acc_f*100:.2f}%\")"
      ],
      "metadata": {
        "id": "MvwwRTTpMt5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715662f3-fb15-469d-a1ff-831cb3dfcc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 50/156 - Loss 3.8559\n",
            "Batch 100/156 - Loss 2.8617\n",
            "Batch 150/156 - Loss 3.2255\n",
            "Epoch 1: Loss=2.9338  Accuracy=71.79%\n",
            "Test set: loss=1.5582, accuracy=88.59%\n",
            "Test set: loss=1.5568, accuracy=89.12%\n",
            "New best model saved at epoch 1 with average acc 88.59, female acc 88.59, male acc 89.12%\n",
            "Batch 50/156 - Loss 3.2611\n",
            "Batch 100/156 - Loss 3.1904\n",
            "Batch 150/156 - Loss 4.8022\n",
            "Epoch 2: Loss=2.9162  Accuracy=72.49%\n",
            "Test set: loss=1.6898, accuracy=86.47%\n",
            "Test set: loss=1.6631, accuracy=88.06%\n",
            "Batch 50/156 - Loss 2.4079\n",
            "Batch 100/156 - Loss 1.6515\n",
            "Batch 150/156 - Loss 2.6151\n",
            "Epoch 3: Loss=2.7828  Accuracy=75.84%\n",
            "Test set: loss=1.3345, accuracy=89.39%\n",
            "Test set: loss=1.4649, accuracy=89.66%\n",
            "New best model saved at epoch 3 with average acc 89.39, female acc 89.39, male acc 89.66%\n",
            "Batch 50/156 - Loss 3.0508\n",
            "Batch 100/156 - Loss 3.8725\n",
            "Batch 150/156 - Loss 3.3005\n",
            "Epoch 4: Loss=2.6610  Accuracy=77.30%\n",
            "Test set: loss=1.6540, accuracy=87.53%\n",
            "Test set: loss=1.6887, accuracy=87.53%\n",
            "Batch 50/156 - Loss 2.2793\n",
            "Batch 100/156 - Loss 2.3232\n",
            "Batch 150/156 - Loss 2.8048\n",
            "Epoch 5: Loss=2.5710  Accuracy=77.87%\n",
            "Test set: loss=1.9922, accuracy=85.15%\n",
            "Test set: loss=2.1485, accuracy=83.55%\n",
            "Batch 50/156 - Loss 2.2392\n",
            "Batch 100/156 - Loss 3.1714\n",
            "Batch 150/156 - Loss 3.3365\n",
            "Epoch 6: Loss=2.5334  Accuracy=78.95%\n",
            "Test set: loss=2.1802, accuracy=85.15%\n",
            "Test set: loss=1.9369, accuracy=84.08%\n",
            "Batch 50/156 - Loss 2.9705\n",
            "Batch 100/156 - Loss 2.3699\n",
            "Batch 150/156 - Loss 2.5338\n",
            "Epoch 7: Loss=2.4184  Accuracy=79.88%\n",
            "Test set: loss=1.9715, accuracy=84.62%\n",
            "Test set: loss=1.8327, accuracy=86.21%\n",
            "Batch 50/156 - Loss 2.6519\n",
            "Batch 100/156 - Loss 2.9730\n",
            "Batch 150/156 - Loss 3.0006\n",
            "Epoch 8: Loss=2.3980  Accuracy=79.70%\n",
            "Test set: loss=1.7038, accuracy=87.27%\n",
            "Test set: loss=1.5941, accuracy=87.80%\n",
            "Batch 50/156 - Loss 2.3581\n",
            "Batch 100/156 - Loss 1.6399\n",
            "Batch 150/156 - Loss 1.6215\n",
            "Epoch 9: Loss=2.3004  Accuracy=82.00%\n",
            "Test set: loss=1.7635, accuracy=88.86%\n",
            "Test set: loss=1.5062, accuracy=90.19%\n",
            "Batch 50/156 - Loss 3.0578\n",
            "Batch 100/156 - Loss 2.2911\n",
            "Batch 150/156 - Loss 2.1420\n",
            "Epoch 10: Loss=2.4469  Accuracy=79.66%\n",
            "Test set: loss=1.8454, accuracy=86.47%\n",
            "Test set: loss=1.7646, accuracy=86.47%\n",
            "Batch 50/156 - Loss 2.2696\n",
            "Batch 100/156 - Loss 2.7688\n",
            "Batch 150/156 - Loss 2.4821\n",
            "Epoch 11: Loss=2.2333  Accuracy=82.14%\n",
            "Test set: loss=1.8538, accuracy=88.06%\n",
            "Test set: loss=1.7581, accuracy=88.59%\n",
            "Batch 50/156 - Loss 2.7043\n",
            "Batch 100/156 - Loss 2.0212\n",
            "Batch 150/156 - Loss 3.0883\n",
            "Epoch 12: Loss=2.3737  Accuracy=80.06%\n",
            "Test set: loss=1.6973, accuracy=84.35%\n",
            "Test set: loss=1.7874, accuracy=84.62%\n",
            "Batch 50/156 - Loss 3.6117\n",
            "Batch 100/156 - Loss 2.7268\n",
            "Batch 150/156 - Loss 1.7609\n",
            "Epoch 13: Loss=2.2245  Accuracy=82.72%\n",
            "Test set: loss=1.5689, accuracy=89.39%\n",
            "Test set: loss=1.4835, accuracy=90.45%\n",
            "New best model saved at epoch 13 with average acc 89.39, female acc 89.39, male acc 90.45%\n",
            "Batch 50/156 - Loss 2.3794\n",
            "Batch 100/156 - Loss 2.7526\n",
            "Batch 150/156 - Loss 1.7128\n",
            "Epoch 14: Loss=2.2582  Accuracy=81.36%\n",
            "Test set: loss=1.9449, accuracy=85.94%\n",
            "Test set: loss=1.8919, accuracy=87.80%\n",
            "Batch 50/156 - Loss 1.8017\n",
            "Batch 100/156 - Loss 1.0283\n",
            "Batch 150/156 - Loss 1.1855\n",
            "Epoch 15: Loss=2.0215  Accuracy=83.92%\n",
            "Test set: loss=1.9797, accuracy=84.62%\n",
            "Test set: loss=1.7920, accuracy=87.27%\n",
            "Batch 50/156 - Loss 1.3447\n",
            "Batch 100/156 - Loss 1.1883\n",
            "Batch 150/156 - Loss 2.2036\n",
            "Epoch 16: Loss=1.9031  Accuracy=85.46%\n",
            "Test set: loss=1.6665, accuracy=88.33%\n",
            "Test set: loss=1.5151, accuracy=90.19%\n",
            "Batch 50/156 - Loss 1.2374\n",
            "Batch 100/156 - Loss 2.0339\n",
            "Batch 150/156 - Loss 1.7311\n",
            "Epoch 17: Loss=1.8966  Accuracy=85.79%\n",
            "Test set: loss=1.4030, accuracy=90.19%\n",
            "Test set: loss=1.4672, accuracy=90.72%\n",
            "New best model saved at epoch 17 with average acc 90.19, female acc 90.19, male acc 90.72%\n",
            "Batch 50/156 - Loss 1.4565\n",
            "Batch 100/156 - Loss 2.0115\n",
            "Batch 150/156 - Loss 3.7686\n",
            "Epoch 18: Loss=1.9369  Accuracy=85.80%\n",
            "Test set: loss=1.6738, accuracy=86.47%\n",
            "Test set: loss=1.7592, accuracy=87.80%\n",
            "Batch 50/156 - Loss 2.8077\n",
            "Batch 100/156 - Loss 1.9869\n",
            "Batch 150/156 - Loss 2.3501\n",
            "Epoch 19: Loss=1.9898  Accuracy=84.73%\n",
            "Test set: loss=1.8698, accuracy=82.23%\n",
            "Test set: loss=1.4661, accuracy=85.94%\n",
            "Batch 50/156 - Loss 2.1550\n",
            "Batch 100/156 - Loss 2.8228\n",
            "Batch 150/156 - Loss 1.9950\n",
            "Epoch 20: Loss=1.9201  Accuracy=84.71%\n",
            "Test set: loss=2.0295, accuracy=87.27%\n",
            "Test set: loss=1.7257, accuracy=88.59%\n",
            "Batch 50/156 - Loss 0.6522\n",
            "Batch 100/156 - Loss 2.0989\n",
            "Batch 150/156 - Loss 0.9018\n",
            "Epoch 21: Loss=1.7672  Accuracy=87.20%\n",
            "Test set: loss=1.8625, accuracy=88.06%\n",
            "Test set: loss=1.7978, accuracy=88.59%\n",
            "Batch 50/156 - Loss 1.7873\n",
            "Batch 100/156 - Loss 2.3635\n",
            "Batch 150/156 - Loss 1.7340\n",
            "Epoch 22: Loss=1.8387  Accuracy=86.75%\n",
            "Test set: loss=1.7869, accuracy=87.53%\n",
            "Test set: loss=1.6383, accuracy=88.86%\n",
            "Batch 50/156 - Loss 2.1695\n",
            "Batch 100/156 - Loss 1.4251\n",
            "Batch 150/156 - Loss 2.1413\n",
            "Epoch 23: Loss=1.7943  Accuracy=86.90%\n",
            "Test set: loss=1.7159, accuracy=87.53%\n",
            "Test set: loss=1.6400, accuracy=89.39%\n",
            "Batch 50/156 - Loss 1.8163\n",
            "Batch 100/156 - Loss 1.2724\n",
            "Batch 150/156 - Loss 0.9733\n",
            "Epoch 24: Loss=1.6398  Accuracy=87.62%\n",
            "Test set: loss=1.7461, accuracy=87.27%\n",
            "Test set: loss=1.7348, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.9311\n",
            "Batch 100/156 - Loss 2.5431\n",
            "Batch 150/156 - Loss 1.9614\n",
            "Epoch 25: Loss=1.6295  Accuracy=88.57%\n",
            "Test set: loss=2.2562, accuracy=83.29%\n",
            "Test set: loss=1.7259, accuracy=87.00%\n",
            "Batch 50/156 - Loss 1.8519\n",
            "Batch 100/156 - Loss 2.0782\n",
            "Batch 150/156 - Loss 1.3863\n",
            "Epoch 26: Loss=1.6556  Accuracy=87.93%\n",
            "Test set: loss=2.0563, accuracy=87.53%\n",
            "Test set: loss=1.5868, accuracy=89.39%\n",
            "Batch 50/156 - Loss 1.1575\n",
            "Batch 100/156 - Loss 1.4725\n",
            "Batch 150/156 - Loss 2.0535\n",
            "Epoch 27: Loss=1.6778  Accuracy=87.68%\n",
            "Test set: loss=1.8798, accuracy=87.00%\n",
            "Test set: loss=1.6534, accuracy=88.86%\n",
            "Batch 50/156 - Loss 2.3757\n",
            "Batch 100/156 - Loss 2.0114\n",
            "Batch 150/156 - Loss 1.6852\n",
            "Epoch 28: Loss=1.6253  Accuracy=87.86%\n",
            "Test set: loss=1.9009, accuracy=88.33%\n",
            "Test set: loss=1.2956, accuracy=92.04%\n",
            "Batch 50/156 - Loss 1.0992\n",
            "Batch 100/156 - Loss 0.9941\n",
            "Batch 150/156 - Loss 1.4703\n",
            "Epoch 29: Loss=1.5822  Accuracy=88.41%\n",
            "Test set: loss=1.9132, accuracy=86.74%\n",
            "Test set: loss=1.6010, accuracy=88.59%\n",
            "Batch 50/156 - Loss 1.3508\n",
            "Batch 100/156 - Loss 0.4023\n",
            "Batch 150/156 - Loss 1.2520\n",
            "Epoch 30: Loss=1.4514  Accuracy=89.49%\n",
            "Test set: loss=1.8551, accuracy=88.06%\n",
            "Test set: loss=1.6077, accuracy=89.92%\n",
            "Batch 50/156 - Loss 1.3457\n",
            "Batch 100/156 - Loss 1.5573\n",
            "Batch 150/156 - Loss 1.5485\n",
            "Epoch 31: Loss=1.3953  Accuracy=90.53%\n",
            "Test set: loss=2.2866, accuracy=85.15%\n",
            "Test set: loss=1.7476, accuracy=88.06%\n",
            "Batch 50/156 - Loss 1.4258\n",
            "Batch 100/156 - Loss 1.2823\n",
            "Batch 150/156 - Loss 1.1726\n",
            "Epoch 32: Loss=1.3590  Accuracy=90.42%\n",
            "Test set: loss=1.7485, accuracy=87.53%\n",
            "Test set: loss=1.5958, accuracy=89.39%\n",
            "Batch 50/156 - Loss 1.2291\n",
            "Batch 100/156 - Loss 1.7426\n",
            "Batch 150/156 - Loss 1.2728\n",
            "Epoch 33: Loss=1.2971  Accuracy=90.58%\n",
            "Test set: loss=1.9192, accuracy=84.88%\n",
            "Test set: loss=1.7015, accuracy=85.68%\n",
            "Batch 50/156 - Loss 1.3935\n",
            "Batch 100/156 - Loss 1.0985\n",
            "Batch 150/156 - Loss 1.3844\n",
            "Epoch 34: Loss=1.3846  Accuracy=89.53%\n",
            "Test set: loss=2.1369, accuracy=86.47%\n",
            "Test set: loss=1.9147, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.3464\n",
            "Batch 100/156 - Loss 2.1463\n",
            "Batch 150/156 - Loss 1.0348\n",
            "Epoch 35: Loss=1.2120  Accuracy=91.17%\n",
            "Test set: loss=1.7826, accuracy=87.53%\n",
            "Test set: loss=1.5638, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.4277\n",
            "Batch 100/156 - Loss 2.0862\n",
            "Batch 150/156 - Loss 1.1250\n",
            "Epoch 36: Loss=1.0932  Accuracy=92.14%\n",
            "Test set: loss=2.0063, accuracy=86.74%\n",
            "Test set: loss=1.5004, accuracy=89.92%\n",
            "Batch 50/156 - Loss 1.1218\n",
            "Batch 100/156 - Loss 0.7460\n",
            "Batch 150/156 - Loss 1.2018\n",
            "Epoch 37: Loss=1.0764  Accuracy=92.03%\n",
            "Test set: loss=2.1965, accuracy=86.47%\n",
            "Test set: loss=1.8011, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.5673\n",
            "Batch 100/156 - Loss 1.3052\n",
            "Batch 150/156 - Loss 1.1549\n",
            "Epoch 38: Loss=1.0848  Accuracy=91.85%\n",
            "Test set: loss=2.3447, accuracy=85.68%\n",
            "Test set: loss=1.7338, accuracy=88.33%\n",
            "Batch 50/156 - Loss 1.7830\n",
            "Batch 100/156 - Loss 0.8696\n",
            "Batch 150/156 - Loss 1.4607\n",
            "Epoch 39: Loss=1.0793  Accuracy=91.64%\n",
            "Test set: loss=2.3368, accuracy=84.88%\n",
            "Test set: loss=1.7726, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.6110\n",
            "Batch 100/156 - Loss 1.2437\n",
            "Batch 150/156 - Loss 1.8866\n",
            "Epoch 40: Loss=1.0538  Accuracy=91.46%\n",
            "Test set: loss=2.1441, accuracy=84.35%\n",
            "Test set: loss=1.8838, accuracy=84.62%\n",
            "Batch 50/156 - Loss 1.4729\n",
            "Batch 100/156 - Loss 1.1698\n",
            "Batch 150/156 - Loss 0.6696\n",
            "Epoch 41: Loss=1.0200  Accuracy=91.82%\n",
            "Test set: loss=2.1485, accuracy=87.53%\n",
            "Test set: loss=1.6865, accuracy=89.12%\n",
            "Batch 50/156 - Loss 0.5616\n",
            "Batch 100/156 - Loss 0.8103\n",
            "Batch 150/156 - Loss 1.6546\n",
            "Epoch 42: Loss=0.9899  Accuracy=92.23%\n",
            "Test set: loss=1.8246, accuracy=88.59%\n",
            "Test set: loss=1.6849, accuracy=89.12%\n",
            "Batch 50/156 - Loss 1.1151\n",
            "Batch 100/156 - Loss 0.6701\n",
            "Batch 150/156 - Loss 1.3904\n",
            "Epoch 43: Loss=0.8814  Accuracy=93.17%\n",
            "Test set: loss=2.2548, accuracy=84.35%\n",
            "Test set: loss=1.5468, accuracy=88.86%\n",
            "Batch 50/156 - Loss 1.0742\n",
            "Batch 100/156 - Loss 0.8789\n",
            "Batch 150/156 - Loss 0.5621\n",
            "Epoch 44: Loss=0.9791  Accuracy=92.79%\n",
            "Test set: loss=2.1902, accuracy=85.68%\n",
            "Test set: loss=1.8446, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.1745\n",
            "Batch 100/156 - Loss 0.3578\n",
            "Batch 150/156 - Loss 0.4328\n",
            "Epoch 45: Loss=0.8762  Accuracy=93.54%\n",
            "Test set: loss=2.1406, accuracy=85.15%\n",
            "Test set: loss=1.9306, accuracy=87.27%\n",
            "Batch 50/156 - Loss 1.3497\n",
            "Batch 100/156 - Loss 0.9585\n",
            "Batch 150/156 - Loss 1.0204\n",
            "Epoch 46: Loss=0.8982  Accuracy=93.46%\n",
            "Test set: loss=1.9541, accuracy=87.27%\n",
            "Test set: loss=1.8944, accuracy=87.00%\n",
            "Batch 50/156 - Loss 0.0947\n",
            "Batch 100/156 - Loss 0.6634\n",
            "Batch 150/156 - Loss 0.2727\n",
            "Epoch 47: Loss=0.7800  Accuracy=94.34%\n",
            "Test set: loss=2.0977, accuracy=86.21%\n",
            "Test set: loss=2.1113, accuracy=86.47%\n",
            "Batch 50/156 - Loss 0.1317\n",
            "Batch 100/156 - Loss 0.5130\n",
            "Batch 150/156 - Loss 0.3223\n",
            "Epoch 48: Loss=0.7146  Accuracy=95.00%\n",
            "Test set: loss=2.7290, accuracy=83.55%\n",
            "Test set: loss=2.0480, accuracy=86.74%\n",
            "Batch 50/156 - Loss 0.7380\n",
            "Batch 100/156 - Loss 0.5863\n",
            "Batch 150/156 - Loss 1.1145\n",
            "Epoch 49: Loss=0.7676  Accuracy=94.54%\n",
            "Test set: loss=2.0321, accuracy=88.06%\n",
            "Test set: loss=1.7424, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.7167\n",
            "Batch 100/156 - Loss 0.6091\n",
            "Batch 150/156 - Loss 0.9314\n",
            "Epoch 50: Loss=0.6913  Accuracy=95.09%\n",
            "Test set: loss=2.3866, accuracy=86.21%\n",
            "Test set: loss=1.8849, accuracy=88.06%\n",
            "Batch 50/156 - Loss 1.1670\n",
            "Batch 100/156 - Loss 0.3536\n",
            "Batch 150/156 - Loss 0.9636\n",
            "Epoch 1: Loss=0.5764  Accuracy=95.89%\n",
            "Test set: loss=2.1380, accuracy=86.89%\n",
            "Test set: loss=2.0595, accuracy=87.68%\n",
            "New best model saved at epoch 1 with average acc 86.89, female acc 86.89, male acc 87.68%\n",
            "Batch 50/156 - Loss 0.5710\n",
            "Batch 100/156 - Loss 0.0199\n",
            "Batch 150/156 - Loss 0.6425\n",
            "Epoch 2: Loss=0.4924  Accuracy=96.47%\n",
            "Test set: loss=2.2454, accuracy=86.49%\n",
            "Test set: loss=1.9013, accuracy=88.34%\n",
            "New best model saved at epoch 2 with average acc 86.49, female acc 86.49, male acc 88.34%\n",
            "Batch 50/156 - Loss 0.3174\n",
            "Batch 100/156 - Loss 0.3233\n",
            "Batch 150/156 - Loss 0.5009\n",
            "Epoch 3: Loss=0.4560  Accuracy=96.83%\n",
            "Test set: loss=2.2595, accuracy=85.56%\n",
            "Test set: loss=1.9430, accuracy=86.36%\n",
            "Batch 50/156 - Loss 0.3757\n",
            "Batch 100/156 - Loss 0.6585\n",
            "Batch 150/156 - Loss 0.3352\n",
            "Epoch 4: Loss=0.5044  Accuracy=96.41%\n",
            "Test set: loss=2.5096, accuracy=84.64%\n",
            "Test set: loss=2.3027, accuracy=85.17%\n",
            "Batch 50/156 - Loss 0.3580\n",
            "Batch 100/156 - Loss 0.0389\n",
            "Batch 150/156 - Loss 0.6382\n",
            "Epoch 5: Loss=0.4792  Accuracy=96.57%\n",
            "Test set: loss=2.0307, accuracy=87.55%\n",
            "Test set: loss=1.7366, accuracy=89.54%\n",
            "New best model saved at epoch 5 with average acc 87.55, female acc 87.55, male acc 89.54%\n",
            "Batch 50/156 - Loss 0.3786\n",
            "Batch 100/156 - Loss 0.5571\n",
            "Batch 150/156 - Loss 1.1735\n",
            "Epoch 6: Loss=0.4052  Accuracy=97.18%\n",
            "Test set: loss=2.0490, accuracy=87.02%\n",
            "Test set: loss=1.7061, accuracy=88.34%\n",
            "Batch 50/156 - Loss 0.6419\n",
            "Batch 100/156 - Loss 0.0347\n",
            "Batch 150/156 - Loss 0.3214\n",
            "Epoch 7: Loss=0.4084  Accuracy=97.11%\n",
            "Test set: loss=2.3192, accuracy=87.02%\n",
            "Test set: loss=1.8950, accuracy=88.34%\n",
            "Batch 50/156 - Loss 0.3370\n",
            "Batch 100/156 - Loss 0.5378\n",
            "Batch 150/156 - Loss 0.3584\n",
            "Epoch 8: Loss=0.4211  Accuracy=96.90%\n",
            "Test set: loss=2.5854, accuracy=84.11%\n",
            "Test set: loss=2.0109, accuracy=86.75%\n",
            "Batch 50/156 - Loss 0.2825\n",
            "Batch 100/156 - Loss 0.6378\n",
            "Batch 150/156 - Loss 0.0107\n",
            "Epoch 9: Loss=0.4422  Accuracy=96.84%\n",
            "Test set: loss=2.1652, accuracy=87.55%\n",
            "Test set: loss=1.9907, accuracy=89.01%\n",
            "Batch 50/156 - Loss 0.3244\n",
            "Batch 100/156 - Loss 0.4865\n",
            "Batch 150/156 - Loss 0.5545\n",
            "Epoch 10: Loss=0.4166  Accuracy=97.07%\n",
            "Test set: loss=2.0513, accuracy=85.56%\n",
            "Test set: loss=1.7648, accuracy=87.15%\n",
            "Batch 50/156 - Loss 0.7088\n",
            "Batch 100/156 - Loss 0.0324\n",
            "Batch 150/156 - Loss 0.9173\n",
            "Epoch 11: Loss=0.3110  Accuracy=97.78%\n",
            "Test set: loss=2.1837, accuracy=87.95%\n",
            "Test set: loss=1.9462, accuracy=89.14%\n",
            "Batch 50/156 - Loss 0.7608\n",
            "Batch 100/156 - Loss 0.6118\n",
            "Batch 150/156 - Loss 0.0214\n",
            "Epoch 12: Loss=0.3516  Accuracy=97.52%\n",
            "Test set: loss=2.1148, accuracy=86.49%\n",
            "Test set: loss=1.6769, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.3072\n",
            "Batch 100/156 - Loss 0.3469\n",
            "Batch 150/156 - Loss 0.5696\n",
            "Epoch 13: Loss=0.3555  Accuracy=97.49%\n",
            "Test set: loss=2.3579, accuracy=85.83%\n",
            "Test set: loss=1.8972, accuracy=89.14%\n",
            "Batch 50/156 - Loss 0.0160\n",
            "Batch 100/156 - Loss 0.3213\n",
            "Batch 150/156 - Loss 0.9403\n",
            "Epoch 14: Loss=0.3761  Accuracy=97.34%\n",
            "Test set: loss=2.5326, accuracy=85.56%\n",
            "Test set: loss=1.9722, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.7373\n",
            "Batch 100/156 - Loss 0.3273\n",
            "Batch 150/156 - Loss 0.6268\n",
            "Epoch 15: Loss=0.4017  Accuracy=97.17%\n",
            "Test set: loss=2.2966, accuracy=86.09%\n",
            "Test set: loss=1.6667, accuracy=89.80%\n",
            "Batch 50/156 - Loss 0.6305\n",
            "Batch 100/156 - Loss 0.1694\n",
            "Batch 150/156 - Loss 0.0173\n",
            "Epoch 16: Loss=0.3372  Accuracy=97.69%\n",
            "Test set: loss=2.2362, accuracy=87.55%\n",
            "Test set: loss=1.8808, accuracy=88.74%\n",
            "Batch 50/156 - Loss 0.3352\n",
            "Batch 100/156 - Loss 0.0160\n",
            "Batch 150/156 - Loss 0.0205\n",
            "Epoch 17: Loss=0.3206  Accuracy=97.77%\n",
            "Test set: loss=2.2832, accuracy=86.75%\n",
            "Test set: loss=1.8856, accuracy=89.01%\n",
            "Batch 50/156 - Loss 0.3256\n",
            "Batch 100/156 - Loss 0.0622\n",
            "Batch 150/156 - Loss 0.5052\n",
            "Epoch 18: Loss=0.2950  Accuracy=97.97%\n",
            "Test set: loss=2.0178, accuracy=87.68%\n",
            "Test set: loss=1.7140, accuracy=90.20%\n",
            "New best model saved at epoch 18 with average acc 87.68, female acc 87.68, male acc 90.20%\n",
            "Batch 50/156 - Loss 0.7507\n",
            "Batch 100/156 - Loss 0.0188\n",
            "Batch 150/156 - Loss 0.0388\n",
            "Epoch 19: Loss=0.3132  Accuracy=97.86%\n",
            "Test set: loss=1.9139, accuracy=89.01%\n",
            "Test set: loss=1.7270, accuracy=90.33%\n",
            "New best model saved at epoch 19 with average acc 89.01, female acc 89.01, male acc 90.33%\n",
            "Batch 50/156 - Loss 0.0218\n",
            "Batch 100/156 - Loss 0.0350\n",
            "Batch 150/156 - Loss 0.0080\n",
            "Epoch 20: Loss=0.2757  Accuracy=98.16%\n",
            "Test set: loss=2.2744, accuracy=88.08%\n",
            "Test set: loss=1.9601, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.3537\n",
            "Batch 100/156 - Loss 0.3196\n",
            "Batch 150/156 - Loss 0.4466\n",
            "Epoch 21: Loss=0.3326  Accuracy=97.67%\n",
            "Test set: loss=1.7783, accuracy=88.87%\n",
            "Test set: loss=1.8894, accuracy=87.81%\n",
            "Batch 50/156 - Loss 1.1862\n",
            "Batch 100/156 - Loss 0.8764\n",
            "Batch 150/156 - Loss 0.7925\n",
            "Epoch 22: Loss=0.2951  Accuracy=97.91%\n",
            "Test set: loss=2.5704, accuracy=85.56%\n",
            "Test set: loss=1.9449, accuracy=88.48%\n",
            "Batch 50/156 - Loss 0.3622\n",
            "Batch 100/156 - Loss 0.0181\n",
            "Batch 150/156 - Loss 0.0950\n",
            "Epoch 23: Loss=0.2805  Accuracy=98.19%\n",
            "Test set: loss=2.2572, accuracy=85.96%\n",
            "Test set: loss=1.6663, accuracy=89.80%\n",
            "Batch 50/156 - Loss 0.1455\n",
            "Batch 100/156 - Loss 0.3059\n",
            "Batch 150/156 - Loss 1.2332\n",
            "Epoch 24: Loss=0.2617  Accuracy=98.30%\n",
            "Test set: loss=2.0774, accuracy=87.95%\n",
            "Test set: loss=1.7428, accuracy=89.93%\n",
            "Batch 50/156 - Loss 0.0251\n",
            "Batch 100/156 - Loss 0.3014\n",
            "Batch 150/156 - Loss 0.6138\n",
            "Epoch 25: Loss=0.2795  Accuracy=97.96%\n",
            "Test set: loss=2.0693, accuracy=88.34%\n",
            "Test set: loss=2.0019, accuracy=89.27%\n",
            "Batch 50/156 - Loss 0.0133\n",
            "Batch 100/156 - Loss 0.3159\n",
            "Batch 150/156 - Loss 0.0189\n",
            "Epoch 26: Loss=0.2629  Accuracy=98.22%\n",
            "Test set: loss=2.1601, accuracy=88.34%\n",
            "Test set: loss=1.9096, accuracy=89.54%\n",
            "Batch 50/156 - Loss 0.3276\n",
            "Batch 100/156 - Loss 0.3132\n",
            "Batch 150/156 - Loss 0.0279\n",
            "Epoch 27: Loss=0.2629  Accuracy=98.21%\n",
            "Test set: loss=1.9779, accuracy=89.27%\n",
            "Test set: loss=1.8847, accuracy=89.67%\n",
            "Batch 50/156 - Loss 0.0172\n",
            "Batch 100/156 - Loss 0.0350\n",
            "Batch 150/156 - Loss 0.0160\n",
            "Epoch 28: Loss=0.2256  Accuracy=98.63%\n",
            "Test set: loss=2.1756, accuracy=85.30%\n",
            "Test set: loss=1.8810, accuracy=87.55%\n",
            "Batch 50/156 - Loss 0.3212\n",
            "Batch 100/156 - Loss 0.6579\n",
            "Batch 150/156 - Loss 0.0152\n",
            "Epoch 29: Loss=0.2068  Accuracy=98.65%\n",
            "Test set: loss=2.1787, accuracy=87.81%\n",
            "Test set: loss=1.7724, accuracy=89.67%\n",
            "Batch 50/156 - Loss 0.5696\n",
            "Batch 100/156 - Loss 0.0159\n",
            "Batch 150/156 - Loss 0.4125\n",
            "Epoch 30: Loss=0.2476  Accuracy=98.35%\n",
            "Test set: loss=2.1225, accuracy=88.61%\n",
            "Test set: loss=1.9799, accuracy=88.74%\n",
            "Batch 50/156 - Loss 0.0207\n",
            "Batch 100/156 - Loss 0.3935\n",
            "Batch 150/156 - Loss 0.6234\n",
            "Epoch 31: Loss=0.2594  Accuracy=98.17%\n",
            "Test set: loss=2.3282, accuracy=87.42%\n",
            "Test set: loss=2.0406, accuracy=88.74%\n",
            "Batch 50/156 - Loss 0.0997\n",
            "Batch 100/156 - Loss 0.2686\n",
            "Batch 150/156 - Loss 0.0244\n",
            "Epoch 32: Loss=0.2453  Accuracy=98.39%\n",
            "Test set: loss=2.0912, accuracy=88.61%\n",
            "Test set: loss=1.9262, accuracy=88.34%\n",
            "Batch 50/156 - Loss 0.0218\n",
            "Batch 100/156 - Loss 0.5057\n",
            "Batch 150/156 - Loss 1.6053\n",
            "Epoch 33: Loss=0.3082  Accuracy=97.82%\n",
            "Test set: loss=2.1974, accuracy=87.68%\n",
            "Test set: loss=1.8170, accuracy=89.93%\n",
            "Batch 50/156 - Loss 0.6189\n",
            "Batch 100/156 - Loss 0.0124\n",
            "Batch 150/156 - Loss 0.5552\n",
            "Epoch 34: Loss=0.2526  Accuracy=98.28%\n",
            "Test set: loss=2.6841, accuracy=84.77%\n",
            "Test set: loss=2.1534, accuracy=87.15%\n",
            "Batch 50/156 - Loss 0.8086\n",
            "Batch 100/156 - Loss 0.3344\n",
            "Batch 150/156 - Loss 0.3541\n",
            "Epoch 35: Loss=0.3234  Accuracy=97.85%\n",
            "Test set: loss=2.1253, accuracy=87.81%\n",
            "Test set: loss=1.9954, accuracy=88.48%\n",
            "Batch 50/156 - Loss 0.1017\n",
            "Batch 100/156 - Loss 0.0181\n",
            "Batch 150/156 - Loss 0.3273\n",
            "Epoch 36: Loss=0.2938  Accuracy=98.01%\n",
            "Test set: loss=2.3976, accuracy=85.70%\n",
            "Test set: loss=2.0512, accuracy=87.68%\n",
            "Batch 50/156 - Loss 0.3266\n",
            "Batch 100/156 - Loss 0.0147\n",
            "Batch 150/156 - Loss 0.0124\n",
            "Epoch 37: Loss=0.2487  Accuracy=98.33%\n",
            "Test set: loss=2.4034, accuracy=86.49%\n",
            "Test set: loss=2.0155, accuracy=89.14%\n",
            "Batch 50/156 - Loss 0.0149\n",
            "Batch 100/156 - Loss 0.6383\n",
            "Batch 150/156 - Loss 0.3141\n",
            "Epoch 38: Loss=0.2459  Accuracy=98.36%\n",
            "Test set: loss=2.4953, accuracy=85.83%\n",
            "Test set: loss=2.2044, accuracy=87.95%\n",
            "Batch 50/156 - Loss 0.6133\n",
            "Batch 100/156 - Loss 0.0156\n",
            "Batch 150/156 - Loss 0.5935\n",
            "Epoch 39: Loss=0.2776  Accuracy=98.02%\n",
            "Test set: loss=2.4646, accuracy=84.77%\n",
            "Test set: loss=1.9970, accuracy=87.42%\n",
            "Batch 50/156 - Loss 0.2364\n",
            "Batch 100/156 - Loss 0.0205\n",
            "Batch 150/156 - Loss 0.0243\n",
            "Epoch 40: Loss=0.2509  Accuracy=98.26%\n",
            "Test set: loss=2.4132, accuracy=86.23%\n",
            "Test set: loss=1.9556, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.0239\n",
            "Batch 100/156 - Loss 0.3165\n",
            "Batch 150/156 - Loss 0.0095\n",
            "Epoch 41: Loss=0.2062  Accuracy=98.65%\n",
            "Test set: loss=2.4917, accuracy=87.02%\n",
            "Test set: loss=2.1561, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.0075\n",
            "Batch 100/156 - Loss 0.1391\n",
            "Batch 150/156 - Loss 0.2105\n",
            "Epoch 42: Loss=0.1886  Accuracy=98.80%\n",
            "Test set: loss=2.4309, accuracy=86.62%\n",
            "Test set: loss=1.9446, accuracy=88.87%\n",
            "Batch 50/156 - Loss 0.3107\n",
            "Batch 100/156 - Loss 0.3242\n",
            "Batch 150/156 - Loss 1.1930\n",
            "Epoch 43: Loss=0.2121  Accuracy=98.67%\n",
            "Test set: loss=2.2855, accuracy=86.49%\n",
            "Test set: loss=2.0610, accuracy=87.68%\n",
            "Batch 50/156 - Loss 0.0583\n",
            "Batch 100/156 - Loss 0.0322\n",
            "Batch 150/156 - Loss 0.6262\n",
            "Epoch 44: Loss=0.2146  Accuracy=98.48%\n",
            "Test set: loss=2.5372, accuracy=86.49%\n",
            "Test set: loss=2.1812, accuracy=87.95%\n",
            "Batch 50/156 - Loss 0.2627\n",
            "Batch 100/156 - Loss 0.3266\n",
            "Batch 150/156 - Loss 0.4139\n",
            "Epoch 45: Loss=0.2633  Accuracy=98.15%\n",
            "Test set: loss=2.6515, accuracy=84.37%\n",
            "Test set: loss=2.0417, accuracy=87.68%\n",
            "Batch 50/156 - Loss 0.0144\n",
            "Batch 100/156 - Loss 0.0181\n",
            "Batch 150/156 - Loss 0.7915\n",
            "Epoch 46: Loss=0.2274  Accuracy=98.29%\n",
            "Test set: loss=1.9791, accuracy=88.08%\n",
            "Test set: loss=1.9807, accuracy=88.48%\n",
            "Batch 50/156 - Loss 0.0146\n",
            "Batch 100/156 - Loss 0.3466\n",
            "Batch 150/156 - Loss 0.1005\n",
            "Epoch 47: Loss=0.2779  Accuracy=98.01%\n",
            "Test set: loss=2.5020, accuracy=84.64%\n",
            "Test set: loss=2.1573, accuracy=86.89%\n",
            "Batch 50/156 - Loss 0.3107\n",
            "Batch 100/156 - Loss 0.5338\n",
            "Batch 150/156 - Loss 0.0083\n",
            "Epoch 48: Loss=0.1867  Accuracy=98.85%\n",
            "Test set: loss=2.2289, accuracy=88.74%\n",
            "Test set: loss=1.8621, accuracy=90.46%\n",
            "Batch 50/156 - Loss 0.0345\n",
            "Batch 100/156 - Loss 0.0357\n",
            "Batch 150/156 - Loss 0.0119\n",
            "Epoch 49: Loss=0.3242  Accuracy=97.69%\n",
            "Test set: loss=2.4333, accuracy=86.89%\n",
            "Test set: loss=2.0804, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.3199\n",
            "Batch 100/156 - Loss 0.0129\n",
            "Batch 150/156 - Loss 0.3036\n",
            "Epoch 50: Loss=0.1729  Accuracy=98.89%\n",
            "Test set: loss=2.3130, accuracy=86.36%\n",
            "Test set: loss=2.3674, accuracy=86.75%\n",
            "Batch 50/156 - Loss 0.0064\n",
            "Batch 100/156 - Loss 0.0047\n",
            "Batch 150/156 - Loss 0.3226\n",
            "Epoch 1: Loss=0.1439  Accuracy=99.00%\n",
            "Test set: loss=2.1067, accuracy=86.74%\n",
            "Test set: loss=2.2546, accuracy=87.00%\n",
            "New best model saved at epoch 1 with average acc 86.74, female acc 86.74, male acc 87.00%\n",
            "Batch 50/156 - Loss 0.4325\n",
            "Batch 100/156 - Loss 0.2875\n",
            "Batch 150/156 - Loss 0.1824\n",
            "Epoch 2: Loss=0.1360  Accuracy=99.02%\n",
            "Test set: loss=1.9990, accuracy=87.00%\n",
            "Test set: loss=1.9651, accuracy=87.27%\n",
            "New best model saved at epoch 2 with average acc 87.00, female acc 87.00, male acc 87.27%\n",
            "Batch 50/156 - Loss 0.3218\n",
            "Batch 100/156 - Loss 0.0048\n",
            "Batch 150/156 - Loss 0.6162\n",
            "Epoch 3: Loss=0.1421  Accuracy=99.05%\n",
            "Test set: loss=2.1407, accuracy=87.80%\n",
            "Test set: loss=1.9038, accuracy=89.66%\n",
            "New best model saved at epoch 3 with average acc 87.80, female acc 87.80, male acc 89.66%\n",
            "Batch 50/156 - Loss 0.0419\n",
            "Batch 100/156 - Loss 0.0112\n",
            "Batch 150/156 - Loss 0.3210\n",
            "Epoch 4: Loss=0.1711  Accuracy=98.83%\n",
            "Test set: loss=2.9310, accuracy=81.96%\n",
            "Test set: loss=2.3912, accuracy=84.88%\n",
            "Batch 50/156 - Loss 0.2816\n",
            "Batch 100/156 - Loss 0.0223\n",
            "Batch 150/156 - Loss 0.0053\n",
            "Epoch 5: Loss=0.1695  Accuracy=98.73%\n",
            "Test set: loss=2.4235, accuracy=87.27%\n",
            "Test set: loss=1.8903, accuracy=89.92%\n",
            "Batch 50/156 - Loss 0.0153\n",
            "Batch 100/156 - Loss 0.0036\n",
            "Batch 150/156 - Loss 0.0048\n",
            "Epoch 6: Loss=0.1368  Accuracy=99.08%\n",
            "Test set: loss=1.9738, accuracy=88.86%\n",
            "Test set: loss=1.8377, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.0062\n",
            "Batch 100/156 - Loss 0.0248\n",
            "Batch 150/156 - Loss 0.0038\n",
            "Epoch 7: Loss=0.0909  Accuracy=99.41%\n",
            "Test set: loss=2.1501, accuracy=88.86%\n",
            "Test set: loss=1.7508, accuracy=90.72%\n",
            "New best model saved at epoch 7 with average acc 88.86, female acc 88.86, male acc 90.72%\n",
            "Batch 50/156 - Loss 0.0028\n",
            "Batch 100/156 - Loss 0.3064\n",
            "Batch 150/156 - Loss 0.2555\n",
            "Epoch 8: Loss=0.0998  Accuracy=99.42%\n",
            "Test set: loss=1.8658, accuracy=88.86%\n",
            "Test set: loss=1.8192, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0075\n",
            "Batch 100/156 - Loss 0.0019\n",
            "Batch 150/156 - Loss 0.2286\n",
            "Epoch 9: Loss=0.1319  Accuracy=99.07%\n",
            "Test set: loss=2.2120, accuracy=87.00%\n",
            "Test set: loss=1.9329, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.2065\n",
            "Batch 100/156 - Loss 0.0065\n",
            "Batch 150/156 - Loss 0.5751\n",
            "Epoch 10: Loss=0.1650  Accuracy=98.83%\n",
            "Test set: loss=2.3400, accuracy=85.94%\n",
            "Test set: loss=1.7834, accuracy=89.12%\n",
            "Batch 50/156 - Loss 0.0108\n",
            "Batch 100/156 - Loss 0.2893\n",
            "Batch 150/156 - Loss 0.0044\n",
            "Epoch 11: Loss=0.1498  Accuracy=98.91%\n",
            "Test set: loss=2.4639, accuracy=85.68%\n",
            "Test set: loss=2.0492, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.0054\n",
            "Batch 100/156 - Loss 0.0017\n",
            "Batch 150/156 - Loss 0.2873\n",
            "Epoch 12: Loss=0.0863  Accuracy=99.48%\n",
            "Test set: loss=2.1969, accuracy=85.94%\n",
            "Test set: loss=1.9093, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.2883\n",
            "Batch 100/156 - Loss 0.1738\n",
            "Batch 150/156 - Loss 0.3423\n",
            "Epoch 13: Loss=0.1435  Accuracy=98.91%\n",
            "Test set: loss=2.0232, accuracy=85.15%\n",
            "Test set: loss=1.9110, accuracy=86.74%\n",
            "Batch 50/156 - Loss 0.0044\n",
            "Batch 100/156 - Loss 0.0056\n",
            "Batch 150/156 - Loss 0.0070\n",
            "Epoch 14: Loss=0.1311  Accuracy=99.09%\n",
            "Test set: loss=2.0613, accuracy=89.39%\n",
            "Test set: loss=1.9778, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.0021\n",
            "Batch 100/156 - Loss 0.0184\n",
            "Batch 150/156 - Loss 0.1763\n",
            "Epoch 15: Loss=0.1257  Accuracy=99.14%\n",
            "Test set: loss=2.0912, accuracy=86.74%\n",
            "Test set: loss=1.8475, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0070\n",
            "Batch 100/156 - Loss 0.5098\n",
            "Batch 150/156 - Loss 0.0084\n",
            "Epoch 16: Loss=0.1368  Accuracy=98.93%\n",
            "Test set: loss=2.6434, accuracy=86.21%\n",
            "Test set: loss=1.9280, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.0126\n",
            "Batch 100/156 - Loss 0.2848\n",
            "Batch 150/156 - Loss 0.0020\n",
            "Epoch 17: Loss=0.1280  Accuracy=99.15%\n",
            "Test set: loss=2.5649, accuracy=86.74%\n",
            "Test set: loss=2.0929, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.2741\n",
            "Batch 100/156 - Loss 0.0086\n",
            "Batch 150/156 - Loss 0.0038\n",
            "Epoch 18: Loss=0.1354  Accuracy=99.01%\n",
            "Test set: loss=2.5400, accuracy=84.08%\n",
            "Test set: loss=2.0080, accuracy=87.53%\n",
            "Batch 50/156 - Loss 0.0250\n",
            "Batch 100/156 - Loss 0.0031\n",
            "Batch 150/156 - Loss 0.3082\n",
            "Epoch 19: Loss=0.1146  Accuracy=99.17%\n",
            "Test set: loss=2.1630, accuracy=88.06%\n",
            "Test set: loss=1.8272, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.0031\n",
            "Batch 100/156 - Loss 0.0092\n",
            "Batch 150/156 - Loss 0.3137\n",
            "Epoch 20: Loss=0.0904  Accuracy=99.41%\n",
            "Test set: loss=2.2988, accuracy=85.94%\n",
            "Test set: loss=1.8033, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.0029\n",
            "Batch 100/156 - Loss 0.0040\n",
            "Batch 150/156 - Loss 0.6206\n",
            "Epoch 21: Loss=0.0949  Accuracy=99.33%\n",
            "Test set: loss=2.8285, accuracy=84.35%\n",
            "Test set: loss=1.9250, accuracy=87.53%\n",
            "Batch 50/156 - Loss 0.0049\n",
            "Batch 100/156 - Loss 0.0031\n",
            "Batch 150/156 - Loss 0.0102\n",
            "Epoch 22: Loss=0.1415  Accuracy=98.93%\n",
            "Test set: loss=2.0217, accuracy=87.00%\n",
            "Test set: loss=1.8310, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.0182\n",
            "Batch 100/156 - Loss 0.0098\n",
            "Batch 150/156 - Loss 0.1764\n",
            "Epoch 23: Loss=0.1613  Accuracy=98.78%\n",
            "Test set: loss=2.1578, accuracy=88.33%\n",
            "Test set: loss=1.8557, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.2997\n",
            "Batch 100/156 - Loss 0.1194\n",
            "Batch 150/156 - Loss 0.0053\n",
            "Epoch 24: Loss=0.1225  Accuracy=99.01%\n",
            "Test set: loss=2.6468, accuracy=86.74%\n",
            "Test set: loss=1.9254, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.3149\n",
            "Batch 100/156 - Loss 0.2996\n",
            "Batch 150/156 - Loss 0.0071\n",
            "Epoch 25: Loss=0.0874  Accuracy=99.39%\n",
            "Test set: loss=2.5449, accuracy=86.47%\n",
            "Test set: loss=2.1001, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0025\n",
            "Batch 100/156 - Loss 0.2315\n",
            "Batch 150/156 - Loss 0.0037\n",
            "Epoch 26: Loss=0.0489  Accuracy=99.72%\n",
            "Test set: loss=2.0085, accuracy=89.39%\n",
            "Test set: loss=1.9221, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.3178\n",
            "Batch 100/156 - Loss 0.0099\n",
            "Batch 150/156 - Loss 0.0089\n",
            "Epoch 27: Loss=0.0912  Accuracy=99.29%\n",
            "Test set: loss=2.5285, accuracy=87.27%\n",
            "Test set: loss=2.0261, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0046\n",
            "Batch 100/156 - Loss 0.0336\n",
            "Batch 150/156 - Loss 0.6507\n",
            "Epoch 28: Loss=0.1369  Accuracy=99.06%\n",
            "Test set: loss=2.2890, accuracy=85.41%\n",
            "Test set: loss=1.5584, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0020\n",
            "Batch 100/156 - Loss 0.0328\n",
            "Batch 150/156 - Loss 0.0040\n",
            "Epoch 29: Loss=0.0673  Accuracy=99.52%\n",
            "Test set: loss=2.4090, accuracy=88.06%\n",
            "Test set: loss=1.9723, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.2155\n",
            "Batch 100/156 - Loss 0.0033\n",
            "Batch 150/156 - Loss 0.0063\n",
            "Epoch 30: Loss=0.0929  Accuracy=99.37%\n",
            "Test set: loss=2.2951, accuracy=87.80%\n",
            "Test set: loss=1.6842, accuracy=91.51%\n",
            "Batch 50/156 - Loss 0.0047\n",
            "Batch 100/156 - Loss 0.0075\n",
            "Batch 150/156 - Loss 0.0091\n",
            "Epoch 31: Loss=0.1747  Accuracy=98.76%\n",
            "Test set: loss=2.2005, accuracy=87.53%\n",
            "Test set: loss=1.8195, accuracy=89.12%\n",
            "Batch 50/156 - Loss 0.0072\n",
            "Batch 100/156 - Loss 0.0072\n",
            "Batch 150/156 - Loss 0.2900\n",
            "Epoch 32: Loss=0.1185  Accuracy=99.18%\n",
            "Test set: loss=1.9659, accuracy=89.92%\n",
            "Test set: loss=1.7010, accuracy=90.98%\n",
            "New best model saved at epoch 32 with average acc 89.92, female acc 89.92, male acc 90.98%\n",
            "Batch 50/156 - Loss 0.0054\n",
            "Batch 100/156 - Loss 0.3328\n",
            "Batch 150/156 - Loss 0.0035\n",
            "Epoch 33: Loss=0.1162  Accuracy=99.13%\n",
            "Test set: loss=1.6976, accuracy=89.66%\n",
            "Test set: loss=1.7782, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.3090\n",
            "Batch 100/156 - Loss 0.0043\n",
            "Batch 150/156 - Loss 0.0099\n",
            "Epoch 34: Loss=0.1001  Accuracy=99.29%\n",
            "Test set: loss=1.8867, accuracy=89.12%\n",
            "Test set: loss=1.7994, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.2581\n",
            "Batch 100/156 - Loss 0.0038\n",
            "Batch 150/156 - Loss 0.0035\n",
            "Epoch 35: Loss=0.0819  Accuracy=99.45%\n",
            "Test set: loss=2.5858, accuracy=86.47%\n",
            "Test set: loss=1.6631, accuracy=92.04%\n",
            "Batch 50/156 - Loss 0.0017\n",
            "Batch 100/156 - Loss 0.0060\n",
            "Batch 150/156 - Loss 0.0022\n",
            "Epoch 36: Loss=0.0546  Accuracy=99.65%\n",
            "Test set: loss=2.1714, accuracy=88.86%\n",
            "Test set: loss=1.8665, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0020\n",
            "Batch 100/156 - Loss 0.0018\n",
            "Batch 150/156 - Loss 0.0025\n",
            "Epoch 37: Loss=0.0610  Accuracy=99.60%\n",
            "Test set: loss=1.9829, accuracy=90.45%\n",
            "Test set: loss=1.9347, accuracy=90.98%\n",
            "New best model saved at epoch 37 with average acc 90.45, female acc 90.45, male acc 90.98%\n",
            "Batch 50/156 - Loss 0.0026\n",
            "Batch 100/156 - Loss 0.6546\n",
            "Batch 150/156 - Loss 0.0033\n",
            "Epoch 38: Loss=0.0487  Accuracy=99.68%\n",
            "Test set: loss=2.1949, accuracy=89.39%\n",
            "Test set: loss=1.8651, accuracy=90.98%\n",
            "Batch 50/156 - Loss 0.0013\n",
            "Batch 100/156 - Loss 0.0024\n",
            "Batch 150/156 - Loss 0.0079\n",
            "Epoch 39: Loss=0.0601  Accuracy=99.62%\n",
            "Test set: loss=2.3052, accuracy=88.59%\n",
            "Test set: loss=1.7652, accuracy=91.25%\n",
            "Batch 50/156 - Loss 0.2069\n",
            "Batch 100/156 - Loss 0.0027\n",
            "Batch 150/156 - Loss 0.0044\n",
            "Epoch 40: Loss=0.0809  Accuracy=99.50%\n",
            "Test set: loss=2.1818, accuracy=87.00%\n",
            "Test set: loss=1.5646, accuracy=92.04%\n",
            "Batch 50/156 - Loss 0.3034\n",
            "Batch 100/156 - Loss 0.0096\n",
            "Batch 150/156 - Loss 0.3612\n",
            "Epoch 41: Loss=0.0887  Accuracy=99.39%\n",
            "Test set: loss=2.1637, accuracy=89.39%\n",
            "Test set: loss=1.7152, accuracy=91.51%\n",
            "Batch 50/156 - Loss 0.4698\n",
            "Batch 100/156 - Loss 0.0032\n",
            "Batch 150/156 - Loss 0.2834\n",
            "Epoch 42: Loss=0.0816  Accuracy=99.40%\n",
            "Test set: loss=2.4956, accuracy=86.21%\n",
            "Test set: loss=1.8142, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.3399\n",
            "Batch 100/156 - Loss 0.0792\n",
            "Batch 150/156 - Loss 0.2757\n",
            "Epoch 43: Loss=0.0697  Accuracy=99.49%\n",
            "Test set: loss=1.9769, accuracy=90.19%\n",
            "Test set: loss=1.7491, accuracy=91.51%\n",
            "New best model saved at epoch 43 with average acc 90.19, female acc 90.19, male acc 91.51%\n",
            "Batch 50/156 - Loss 0.0010\n",
            "Batch 100/156 - Loss 0.0029\n",
            "Batch 150/156 - Loss 0.0028\n",
            "Epoch 44: Loss=0.0526  Accuracy=99.63%\n",
            "Test set: loss=2.4992, accuracy=87.80%\n",
            "Test set: loss=1.8009, accuracy=90.98%\n",
            "Batch 50/156 - Loss 0.0014\n",
            "Batch 100/156 - Loss 0.0018\n",
            "Batch 150/156 - Loss 0.0560\n",
            "Epoch 45: Loss=0.0430  Accuracy=99.72%\n",
            "Test set: loss=2.1413, accuracy=88.06%\n",
            "Test set: loss=1.6879, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.0018\n",
            "Batch 100/156 - Loss 0.3272\n",
            "Batch 150/156 - Loss 0.0031\n",
            "Epoch 46: Loss=0.0501  Accuracy=99.67%\n",
            "Test set: loss=2.2511, accuracy=87.80%\n",
            "Test set: loss=1.9196, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0040\n",
            "Batch 100/156 - Loss 0.0018\n",
            "Batch 150/156 - Loss 0.0018\n",
            "Epoch 47: Loss=0.0732  Accuracy=99.43%\n",
            "Test set: loss=2.5632, accuracy=87.27%\n",
            "Test set: loss=2.0237, accuracy=89.92%\n",
            "Batch 50/156 - Loss 0.0018\n",
            "Batch 100/156 - Loss 0.0034\n",
            "Batch 150/156 - Loss 0.3226\n",
            "Epoch 48: Loss=0.0709  Accuracy=99.55%\n",
            "Test set: loss=2.0786, accuracy=87.53%\n",
            "Test set: loss=1.8171, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0132\n",
            "Batch 100/156 - Loss 0.0036\n",
            "Batch 150/156 - Loss 0.0012\n",
            "Epoch 49: Loss=0.0925  Accuracy=99.27%\n",
            "Test set: loss=2.2174, accuracy=88.86%\n",
            "Test set: loss=2.2210, accuracy=88.59%\n",
            "Batch 50/156 - Loss 0.0051\n",
            "Batch 100/156 - Loss 0.0014\n",
            "Batch 150/156 - Loss 0.0493\n",
            "Epoch 50: Loss=0.0923  Accuracy=99.31%\n",
            "Test set: loss=2.2610, accuracy=85.94%\n",
            "Test set: loss=1.9600, accuracy=87.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title putting in the utils here for easier dev\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "V5RyGgI-xDUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=8/2550,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ozdMyUbKyike"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Selecting the first column of y (assuming it's the identity label)\n",
        "                loss = nn.CrossEntropyLoss()(output, y[:, 0])\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "P0OJ1YNLy6DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Selecting the first column of targets, assuming it represents the identity label\n",
        "        labels = targets[:, 0]\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train':  # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train_trades':  # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "BPenGmbpy_C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # change this to adam!!!\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "\n",
        "        val_loss_f, val_acc_f = evaluate(model, arc_head, val_loader_f, device)\n",
        "        val_loss_m, val_acc_m = evaluate(model, arc_head, val_loader_m, device)\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Avergae accuracy: {val_acc}, female: {val_acc_f}, male: {val_acc_m}')\n",
        "        print('================================================================')\n",
        "\n"
      ],
      "metadata": {
        "id": "toQXbD13z_VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# def train(model, train_loader, val_loader, pgd_attack,\n",
        "#           mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "#           checkpoint_path='model1.pt'):\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     # change this to adam!!!\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "#     best_acc = 0\n",
        "#     for epoch in range(epochs):\n",
        "#         # training\n",
        "#         train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "#         # evaluate clean accuracy\n",
        "#         test_loss, test_acc = evaluate(model, arc_head, val_loader, device)\n",
        "\n",
        "#         # remember best acc@1 and save checkpoint\n",
        "#         is_best = test_acc > best_acc\n",
        "#         best_acc = max(test_acc, best_acc)\n",
        "\n",
        "#         # save checkpoint if is a new best\n",
        "#         if is_best:\n",
        "#             torch.save(model.state_dict(), checkpoint_path)\n",
        "#         print(f'Accuracy: {test_acc}')\n",
        "#         print('================================================================')"
      ],
      "metadata": {
        "id": "EtEYn5jT-Cev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training loop with backbone using ArcFace\n",
        "\n",
        "# sanity check for attack, rasmus' suggestion\n",
        "\n",
        "\n",
        "epsilon = 8/255\n",
        "pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)  # instantiate the LinfPGDAttack\n",
        "training_mode = \"adv_train\"\n",
        "\n",
        "\n",
        "# note for us!! check the training loop and confirm whether it is similar to the previous one we had\n",
        "\n",
        "\n",
        "for proportion in proportions:\n",
        "    model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "    best_acc = 0.0\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize val_loader_f and val_loader_m to None\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    # Check if the subsets have any samples before creating DataLoaders\n",
        "    if len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=True)\n",
        "    if len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "    train(model, train_loader=train_loader, mode=training_mode, val_loader_f=val_loader_f, val_loader_m= val_loader_m, pgd_attack=pgd, learning_rate=0.001, checkpoint_path='model_adv.pt', epochs=70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "hmUtIMuSzm_r",
        "outputId": "bebf3e8f-3a5c-4599-eb2b-79038ae29702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 0.545271\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.505271\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.540523\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-1815b8fea78d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_m\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_loader_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_adv.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-e3fc131f7568>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader_f, val_loader_m, pgd_attack, mode, epochs, batch_size, learning_rate, momentum, weight_decay, checkpoint_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_ep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-d34fff666fa2>\u001b[0m in \u001b[0;36mtrain_ep\u001b[0;34m(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_ep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Selecting the first column of targets, assuming it represents the identity label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/celeba.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2296\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mResampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 )\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                     \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpulls_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                         \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check for attack, rasmus' suggestion\n",
        "\n",
        "epsilon = 8/255\n",
        "pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)  # instantiate the LinfPGDAttack\n",
        "training_mode = \"adv_train\"\n",
        "\n",
        "\n",
        "# note for us!! check the training loop and confirm whether it is similar to the previous one we had\n",
        "\n",
        "\n",
        "for proportion in proportions:\n",
        "    model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "    best_acc = 0.0\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize val_loader_f and val_loader_m to None\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    # Check if the subsets have any samples before creating DataLoaders\n",
        "    if len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=True)\n",
        "    if len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "    train(model, train_loader=train_loader, mode=training_mode, val_loader_f=val_loader_f, val_loader_m= val_loader_m, pgd_attack=pgd, learning_rate=0.001, checkpoint_path='model_adv.pt', epochs=70)\n"
      ],
      "metadata": {
        "id": "Z7dq1q1Xg37s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting this with a simpler model\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                loss = nn.CrossEntropyLoss()(output, y)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "iI3HBGKtzrWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "def make_dataloader(data_path, batch_size):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform_train)\n",
        "    val_dataset = datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def eval_test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n",
        "\n",
        "def mixup_data(x, y, mixup_alpha=1.0):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=0.003,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Lp-OlBD7pJmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        labels = targets[:, 0] # the first column is the identity label\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer)\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "id": "fKvyFN-QiJQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "        # evaluate clean accuracy\n",
        "        test_loss, test_acc = eval_test(model, val_loader, device)\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = test_acc > best_acc\n",
        "        best_acc = max(test_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print('================================================================')"
      ],
      "metadata": {
        "id": "H38K2cfuoASW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title resnet module\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n"
      ],
      "metadata": {
        "id": "4nLHgl8ekD7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting this with a simpler model\n",
        "\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = y[:, 0] # Assuming the first column is the identity label\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "                loss = nn.CrossEntropyLoss()(output, labels)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv\n",
        "\n",
        "# Modified train_ep function to handle multi-dimensional targets from CelebA\n",
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = targets[:, 0] # Assuming the first column is the identity label\n",
        "\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            # Pass the original multi-dimensional targets to the attack\n",
        "            adv_x = pgd_attack(inputs, targets) # The attack will extract labels internally\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            # Note: The original trades_loss function expects a 1D target tensor.\n",
        "            # You might need to adapt trades_loss similar to LinfPGDAttack if you plan to use this mode\n",
        "            # with CelebA's multi-dimensional targets. For now, using extracted labels might work,\n",
        "            # but the original TRADES formulation uses clean labels for the KL divergence part.\n",
        "            # This might require further adjustments based on the specific TRADES implementation\n",
        "            # you are using.\n",
        "            # Assuming for now that trades_loss can handle the extracted 1D labels.\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=labels, optimizer=optimizer)\n",
        "\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels.\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, labels)\n",
        "        #     adv_x = pgd_attack(inputs, targets) # Pass original targets to attack\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels from adv_x?\n",
        "        #     # This part of mixup with adversarial training might need careful consideration of how targets are handled.\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, labels) # Using extracted labels\n",
        "\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     # Use the extracted 1D labels for criterion\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "# Keep the rest of the training loop as is, ensuring the train function calls the modified train_ep\n",
        "# and evaluate functions (which already handle the identity label extraction).\n",
        "# Make sure the 'train' function signature matches how it's called in the loop:\n",
        "# train(model, train_loader=train_loader, mode=training_mode, val_loader_f=val_loader_f, val_loader_m= val_loader_m, pgd_attack=pgd, learning_rate=0.001, checkpoint_path='model_adv.pt', epochs=70)\n",
        "# The 'train' function you provided in the notebook takes 'val_loader', but your calling code\n",
        "# passes 'val_loader_f'. You should update the train function signature or the call site\n",
        "# to be consistent. Since you added logic for val_loader_f and val_loader_m in the loop\n",
        "# where the error occurred, it seems you intend to evaluate on female and male subsets\n",
        "# separately during training. You should adjust the 'train' function to accept both\n",
        "# val_loader_f and val_loader_m and call your 'evaluate' function twice for each epoch.\n",
        "\n",
        "\n",
        "# Here's the updated train function to accept female and male validation loaders\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # change this to adam!!!\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "    # Note: Your training loop defined the optimizer with arc_head parameters.\n",
        "    # The train function here needs to accept or define the optimizer appropriately.\n",
        "    # Let's assume the optimizer is defined outside and passed in, or redefined here\n",
        "    # to include arc_head parameters if using ArcFace. If using the simple ResNet18\n",
        "    # without ArcFace for this adversarial training part, the current optimizer definition\n",
        "    # (SGD or Adam on model.parameters() only) might be correct.\n",
        "    # Based on the trace, it seems you are using the simple ResNet18 for the PGD attack part.\n",
        "    # Let's keep the SGD optimizer as in the original train function definition.\n",
        "    # If you are using ArcFace, you need to adapt this train function as well.\n",
        "\n",
        "    # Based on the code in the failing cell block, the optimizer was defined *inside* the loop\n",
        "    # and included arc_head parameters. Let's adjust the train function to match that,\n",
        "    # assuming you are training with ArcFace in this section.\n",
        "    # **However**, the eval_test and eval_robust functions called within this 'train' function\n",
        "    # only take 'model', not 'arc_head'. You will need to adapt eval_test and eval_robust\n",
        "    # if they are intended to work with the ArcFace setup (which predicts identity classes\n",
        "    # using features from the backbone and the arc_head).\n",
        "    # Let's assume for now that the adversarial training is on the simpler ResNet18\n",
        "    # for a standard classification task (which seems to be what the PGD attack code expects),\n",
        "    # and the ArcFace training is a separate block of code. If you intend to do adversarial\n",
        "    # training *with* the ArcFace setup, you'll need more significant modifications to the\n",
        "    # attack and evaluation functions.\n",
        "\n",
        "    # Reverting to the optimizer definition as seen in the cell where the error occurred,\n",
        "    # which includes arc_head parameters, assuming you want to train the entire ArcFace model adversarially.\n",
        "    # **IMPORTANT:** This requires adapting eval_test and eval_robust to work with the ArcFace model.\n",
        "    # For simplicity and to fix the immediate error, let's assume the PGD attack is on the\n",
        "    # simple classification task and the ArcFace part is separate, or you need to pass both\n",
        "    # model and arc_head to the train_ep and evaluation functions.\n",
        "\n",
        "    # Let's assume the intention is to train the simple ResNet18 adversarially.\n",
        "    # In this case, the optimizer should only optimize model.parameters().\n",
        "    # If you want to adversarially train the ArcFace setup, you need to pass\n",
        "    # both the model (backbone) and arc_head to train_ep and the attack.\n",
        "\n",
        "    # Given the structure of the failing code block, it seems you are initializing a simple ResNet18\n",
        "    # *inside* the loop for each proportion and then calling this `train` function.\n",
        "    # This suggests adversarial training on the simple ResNet18.\n",
        "    # The optimizer should then be for the model's parameters only.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Using Adam as in your failing block, but only for model\n",
        "\n",
        "    best_acc = 0.0 # Keep track of best average accuracy across genders\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        # Pass the extracted labels in train_ep as modified above\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "\n",
        "        # Evaluate clean accuracy on both subsets using the eval_test function\n",
        "        # Note: eval_test currently calculates standard classification accuracy,\n",
        "        # not accuracy in an embedding space with ArcFace distance.\n",
        "        # If you want to evaluate with ArcFace, you need a different evaluation function.\n",
        "        # Assuming standard classification evaluation for now.\n",
        "        val_acc_f = 0.0\n",
        "        val_acc_m = 0.0\n",
        "        val_loss_f = 0.0\n",
        "        val_loss_m = 0.0\n",
        "\n",
        "        if val_loader_f and len(val_loader_f.dataset) > 0:\n",
        "            # eval_test is designed for 1D targets, but the DataLoader yields multi-dimensional targets.\n",
        "            # You need to adapt eval_test or the DataLoader to yield 1D targets for this evaluation.\n",
        "            # Or, pass the appropriate labels to eval_test.\n",
        "            # Let's modify eval_test slightly to extract the identity label.\n",
        "\n",
        "            # For consistency with the error source, let's assume eval_test needs 1D targets.\n",
        "            # We'll extract the labels before calling eval_test. This is not ideal as it\n",
        "            # involves iterating through the dataset again. A better approach would be to\n",
        "            # modify eval_test to accept multi-dimensional targets and extract the label internally.\n",
        "            # Given the constraint to fix the immediate error, let's assume eval_test needs 1D targets.\n",
        "            # However, eval_test takes a DataLoader, so modifying it is better.\n",
        "\n",
        "            # Let's adapt eval_test to extract the identity label.\n",
        "            # Need to redefine eval_test to handle the multi-dimensional target.\n",
        "            # **This requires modifying the eval_test function definition.**\n",
        "            # Assuming the `evaluate` function defined earlier (which takes arc_head and extracts labels)\n",
        "            # is the correct evaluation function for your setup, you should use that here.\n",
        "\n",
        "            # Replacing eval_test calls with the `evaluate` function used previously.\n",
        "            # This assumes `evaluate` is defined and available in this scope.\n",
        "            # The `evaluate` function also requires the arc_head, which is not passed to this `train` function.\n",
        "            # This highlights a mismatch in your code structure.\n",
        "            # Either the `train` function needs `arc_head` or the adversarial training part is meant for\n",
        "            # a model without ArcFace.\n",
        "\n",
        "            # Let's assume you want to adversarially train the simple ResNet18.\n",
        "            # In that case, the targets should be 1D class labels, and the model should output logits\n",
        "            # for the number of classes (identities). The ResNet18 is instantiated with num_classes=1000,\n",
        "            # which aligns with the idea of predicting identity.\n",
        "            # The `CelebA` dataset yields `identity_labels` which is already a tensor where the first column\n",
        "            # is the identity.\n",
        "\n",
        "            # Let's return to modifying `train_ep` and `LinfPGDAttack` to handle the multi-dimensional\n",
        "            # targets from the DataLoader and extract the first column for the loss calculation.\n",
        "            # This was done in the code block above this function definition.\n",
        "\n",
        "            # Now, for evaluation, we need a function that evaluates the model on the identity prediction task.\n",
        "            # The `evaluate` function defined earlier does this with the ArcFace setup.\n",
        "            # If you are using the simple ResNet18 directly for classification,\n",
        "            # you need an evaluation function that takes the model and a DataLoader,\n",
        "            # extracts the identity label, and calculates accuracy. Let's use `eval_test` but\n",
        "            # modify it to extract the target.\n",
        "\n",
        "            # Redefine eval_test here to handle CelebA targets\n",
        "            def eval_test_celeba(model, dataloader, device):\n",
        "                model.eval()\n",
        "                test_loss = 0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs, targets in dataloader:\n",
        "                        inputs, targets = inputs.to(device), targets.to(device)\n",
        "                        labels = targets[:, 0] # Extract identity label\n",
        "                        outputs = model(inputs)\n",
        "                        test_loss += F.cross_entropy(outputs, labels).item() * inputs.size(0)\n",
        "                        pred = outputs.max(1, keepdim=True)[1]\n",
        "                        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "                        total += inputs.size(0)\n",
        "                test_loss /= total if total > 0 else 1\n",
        "                accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "                print(f'Test: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)')\n",
        "                return test_loss, accuracy\n",
        "\n",
        "            val_loss_f, val_acc_f = eval_test_celeba(model, val_loader_f, device)\n",
        "\n",
        "        if val_loader_m and len(val_loader_m.dataset) > 0:\n",
        "            val_loss_m, val_acc_m = eval_test_celeba(model, val_loader_m, device)\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Average accuracy: {val_acc:.2f}, female: {val_acc_f:.2f}, male: {val_acc_m:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "fLKbosPFkqTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "batch_size = 64\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=1000).to(device) # ResNet for identity classification\n",
        "    # Note: number of classes (1000) should match the number of unique identities\n",
        "    # we filtered initially by top 1000 identitites but this might be limiting perhaps?\n",
        "    # it gives very few examples on the test set\n",
        "\n",
        "\n",
        "    num_identity_classes = 1000 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    if proportion in test_subsets_f and len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "    if proportion in test_subsets_m and len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "\n",
        "\n",
        "    # call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader_f=val_loader_f, val_loader_m=val_loader_m,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=70) # Save checkpoints with proportion"
      ],
      "metadata": {
        "id": "-THEtjHpv_T_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc77347-7f51-4dd7-b404-a3fb77e90a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 6.991986\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.536099\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.628707\n",
            "Train Epoch: 0 [09664/39936 (97%)]\t Loss: 0.667225\n",
            "Test: Average loss: 0.3262, Accuracy: 346/377 (92%)\n",
            "Test: Average loss: 0.3451, Accuracy: 341/377 (90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 1 [00064/39936 (1%)]\t Loss: 0.614658\n",
            "Train Epoch: 1 [03264/39936 (33%)]\t Loss: 0.583970\n",
            "Train Epoch: 1 [06464/39936 (65%)]\t Loss: 0.703626\n",
            "Train Epoch: 1 [09664/39936 (97%)]\t Loss: 0.613131\n",
            "Test: Average loss: 0.6440, Accuracy: 285/377 (76%)\n",
            "Test: Average loss: 0.6484, Accuracy: 280/377 (74%)\n",
            "Average accuracy: 74.93, female: 75.60, male: 74.27\n",
            "Train Epoch: 2 [00064/39936 (1%)]\t Loss: 0.688969\n",
            "Train Epoch: 2 [03264/39936 (33%)]\t Loss: 0.693812\n",
            "Train Epoch: 2 [06464/39936 (65%)]\t Loss: 0.629332\n",
            "Train Epoch: 2 [09664/39936 (97%)]\t Loss: 0.655707\n",
            "Test: Average loss: 0.4370, Accuracy: 344/377 (91%)\n",
            "Test: Average loss: 0.4509, Accuracy: 337/377 (89%)\n",
            "Average accuracy: 90.32, female: 91.25, male: 89.39\n",
            "Train Epoch: 3 [00064/39936 (1%)]\t Loss: 0.615830\n",
            "Train Epoch: 3 [03264/39936 (33%)]\t Loss: 0.599249\n",
            "Train Epoch: 3 [06464/39936 (65%)]\t Loss: 0.625908\n",
            "Train Epoch: 3 [09664/39936 (97%)]\t Loss: 0.647885\n",
            "Test: Average loss: 0.4701, Accuracy: 346/377 (92%)\n",
            "Test: Average loss: 0.4788, Accuracy: 341/377 (90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 4 [00064/39936 (1%)]\t Loss: 0.628772\n",
            "Train Epoch: 4 [03264/39936 (33%)]\t Loss: 0.725800\n",
            "Train Epoch: 4 [06464/39936 (65%)]\t Loss: 0.615096\n",
            "Train Epoch: 4 [09664/39936 (97%)]\t Loss: 0.591036\n",
            "Test: Average loss: 0.3856, Accuracy: 346/377 (92%)\n",
            "Test: Average loss: 0.3988, Accuracy: 341/377 (90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 5 [00064/39936 (1%)]\t Loss: 0.635007\n",
            "Train Epoch: 5 [03264/39936 (33%)]\t Loss: 0.651074\n",
            "Train Epoch: 5 [06464/39936 (65%)]\t Loss: 0.656349\n",
            "Train Epoch: 5 [09664/39936 (97%)]\t Loss: 0.686168\n",
            "Test: Average loss: 0.3699, Accuracy: 346/377 (92%)\n",
            "Test: Average loss: 0.3862, Accuracy: 341/377 (90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 6 [00064/39936 (1%)]\t Loss: 0.639416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R382yizsxyUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}