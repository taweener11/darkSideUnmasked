{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taweener11/darkSideUnmasked/blob/main/5_24_race_demogpairs_dataset_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYi8w2zpnKKE",
        "outputId": "39ebb719-ec2b-407a-9edc-e6f875b15698"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/My Drive/Datasets/demogpairs/DemogPairs.zip\" -d \"/content/demogpairs/\""
      ],
      "metadata": {
        "id": "P1VwdIEoaD6W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #@title defining filenames for downloading fairface --> DemogPairs\n",
        "\n",
        "\n",
        "# import os\n",
        "\n",
        "\n",
        "# ROOT_DIR = '/content/drive/My Drive'\n",
        "# os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "\n",
        "# # # Filenames\n",
        "# # LABEL_ZIP = os.path.join(ROOT_DIR, 'fairface_label_train.zip')\n",
        "# # IMG_ZIP = os.path.join(ROOT_DIR, 'fairface-img-margin050-trainval.zip')\n",
        "# # LABEL_CSV = os.path.join(ROOT_DIR, 'fairface_label_train.csv')\n",
        "# # IMG_FOLDER = os.path.join(ROOT_DIR, 'train')\n",
        "\n",
        "# DEMOGPAIRS_ZIP = os.path.join(ROOT_DIR, 'DemogPairs.zip')\n",
        "# DEMOGPAIRS_FOLDER = os.path.join(ROOT_DIR, 'demogpairs')"
      ],
      "metadata": {
        "id": "C8td5koAnUZg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Z9FD-BfakgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #@title this downloads fairface! but you can also edit for other datasets\n",
        "\n",
        "\n",
        "\n",
        "# import zipfile\n",
        "\n",
        "# if not os.path.exists(DEMOGPAIRS_FOLDER):\n",
        "#     with zipfile.ZipFile(DEMOGPAIRS_ZIP, 'r') as zip_ref:\n",
        "#         zip_ref.extractall(DEMOGPAIRS_FOLDER)\n",
        "#     print(f\"Extracted to {DEMOGPAIRS_FOLDER}\")\n",
        "# else:\n",
        "#     print(f\"Already extracted in {DEMOGPAIRS_FOLDER}\")"
      ],
      "metadata": {
        "id": "wSFp7yLq5f1w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "b2285541-9361-444b-d224-bb97e0aa9370"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/DemogPairs.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-23c20dfefa56>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEMOGPAIRS_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEMOGPAIRS_ZIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEMOGPAIRS_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extracted to {DEMOGPAIRS_FOLDER}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/DemogPairs.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_metadata_file(filepath, gender_label, race_label):\n",
        "    \"\"\"\n",
        "    Read a DemogPairs metadata txt file and collect image paths with labels.\n",
        "\n",
        "    Args:\n",
        "      filepath (str): path to the metadata txt file\n",
        "      gender_label (int): 0 for female, 1 for male\n",
        "      race_label (str): string label for race, e.g. 'black', 'white', 'asian'\n",
        "\n",
        "    Returns:\n",
        "      List of tuples: (image_relative_path, gender_label, race_label)\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line or line.lower().startswith('db_code'):\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            # parts[1] is the image path relative to DemogPairs folder\n",
        "            img_path = parts[1]\n",
        "            samples.append((img_path, gender_label, race_label))\n",
        "    return samples\n"
      ],
      "metadata": {
        "id": "_OPo_ERHpYU_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# metadata_dir = '/content/drive/My Drive/demogpairs/Metadata'  # adjust path\n",
        "metadata_dir = '/content/demogpairs/Metadata'  # edited this to run with the local environment\n",
        "\n",
        "\n",
        "\n",
        "# Map filenames to gender and race labels\n",
        "metadata_info = {\n",
        "    'Black_Females.txt': (0, 'black'),\n",
        "    'Black_Males.txt': (1, 'black'),\n",
        "    'White_Females.txt': (0, 'white'),\n",
        "    'White_Males.txt': (1, 'white'),\n",
        "    'Asian_Females.txt': (0, 'asian'),\n",
        "    'Asian_Males.txt': (1, 'asian')\n",
        "}\n",
        "\n",
        "all_samples = []\n",
        "\n",
        "for fname, (gender, race) in metadata_info.items():\n",
        "    full_path = os.path.join(metadata_dir, fname)\n",
        "    print(f\"Reading {full_path} ...\")\n",
        "    samples = read_metadata_file(full_path, gender, race)\n",
        "    all_samples.extend(samples)\n",
        "\n",
        "print(f\"Total samples loaded: {len(all_samples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uMJX7lHpaYx",
        "outputId": "eb490654-75b9-410a-c294-0c4c4aa77089"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/demogpairs/Metadata/Black_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/Black_Males.txt ...\n",
            "Reading /content/demogpairs/Metadata/White_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/White_Males.txt ...\n",
            "Reading /content/demogpairs/Metadata/Asian_Females.txt ...\n",
            "Reading /content/demogpairs/Metadata/Asian_Males.txt ...\n",
            "Total samples loaded: 10800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# demogpairs_root = '/content/drive/My Drive/demogpairs/DemogPairs'\n",
        "demogpairs_root = '/content/demogpairs/DemogPairs'\n",
        "\n",
        "\n",
        "\n",
        "# Build final dataset list with full paths\n",
        "dataset = []\n",
        "for rel_path, gender, race in all_samples:\n",
        "    img_full_path = os.path.join(demogpairs_root, rel_path)\n",
        "    if os.path.isfile(img_full_path):\n",
        "        dataset.append((img_full_path, gender, race))\n",
        "    else:\n",
        "        print(f\"Missing file: {img_full_path}\")\n",
        "\n",
        "print(f\"Final dataset size after filtering missing files: {len(dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih0pUXU9qU2o",
        "outputId": "565a1778-fafa-44a6-8bd4-0b57eeb38ab7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataset size after filtering missing files: 10800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# working gender-only code:\n",
        "# import numpy as np\n",
        "# gender_labels = np.array([s[1] for s in dataset])\n",
        "# female_indices = np.where(gender_labels == 0)[0]\n",
        "# male_indices = np.where(gender_labels == 1)[0]\n",
        "# print(f\"Females: {len(female_indices)}, Males: {len(male_indices)}\")\n",
        "# N = min(len(female_indices), len(male_indices))\n",
        "# rng = np.random.default_rng(seed=42)\n",
        "# rng.shuffle(female_indices)\n",
        "# rng.shuffle(male_indices)\n",
        "# female_subset = female_indices[:N]\n",
        "# male_subset = male_indices[:N]\n",
        "# balanced_indices = np.concatenate([female_subset, male_subset])\n",
        "# balanced_dataset = [dataset[i] for i in balanced_indices]\n",
        "# print(f\"Balanced dataset size: {len(balanced_dataset)}\")"
      ],
      "metadata": {
        "id": "PNMa-n4kra-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "gender_labels = np.array([s[1] for s in dataset])\n",
        "race_labels = np.array([s[2] for s in dataset])\n",
        "\n",
        "unique_races = np.unique(race_labels)\n",
        "balanced_indices = []\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# Find minimal count per race-gender group\n",
        "min_count = min(\n",
        "    np.sum((gender_labels == gender) & (race_labels == race))\n",
        "    for race in unique_races\n",
        "    for gender in [0,1]\n",
        ")\n",
        "print(f\"Balancing all race-gender groups to {min_count} samples each\")\n",
        "\n",
        "for race in unique_races:\n",
        "    for gender in [0,1]:\n",
        "        group_indices = np.where((race_labels == race) & (gender_labels == gender))[0]\n",
        "        rng.shuffle(group_indices)\n",
        "        balanced_indices.extend(group_indices[:min_count])\n",
        "\n",
        "balanced_dataset = [dataset[i] for i in balanced_indices]\n",
        "print(f\"Balanced dataset size (race+gender): {len(balanced_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsyPM0oEqhdV",
        "outputId": "c691d72e-9ce2-4416-cd02-3de5f1e54c71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balancing all race-gender groups to 1800 samples each\n",
            "Balanced dataset size (race+gender): 10800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example: training indices subset (can be full train, or a subset)\n",
        "train_indices = np.arange(len(dataset))  # or your train split indices\n",
        "\n",
        "# Extract gender and race array for those indices\n",
        "gender_labels = np.array([dataset[i][1] for i in train_indices])  # 0=female, 1=male\n",
        "race_labels = np.array([dataset[i][2] for i in train_indices])    # e.g. 'black', 'white', 'asian'\n",
        "\n",
        "unique_races = np.unique(race_labels)\n",
        "print(f\"Unique races in training data: {unique_races}\")\n",
        "\n",
        "# Group indices by race and gender\n",
        "indices_by_race_gender = {}\n",
        "for race in unique_races:\n",
        "    for gender in [0, 1]:\n",
        "        mask = (race_labels == race) & (gender_labels == gender)\n",
        "        group_indices = train_indices[mask]\n",
        "        indices_by_race_gender[(race, gender)] = group_indices\n",
        "        print(f\"Count for {race} {'female' if gender == 0 else 'male'}: {len(group_indices)}\")\n",
        "\n",
        "# Find minimal count (for balanced sampling across all race+gender groups)\n",
        "min_count = min(len(idxs) for idxs in indices_by_race_gender.values())\n",
        "print(f\"Balancing all race-gender groups to {min_count} samples each\")\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "train_subsets_f = {}\n",
        "train_subsets_m = {}\n",
        "train_subsets = {}\n",
        "\n",
        "subset_sizes = [100, 500, 1000]  # example subset sizes total (must be divisible by number of groups)\n",
        "\n",
        "# Number of race-gender groups (race groups * 2 genders)\n",
        "num_groups = len(unique_races) * 2\n",
        "\n",
        "for size in subset_sizes:\n",
        "    # Adjust size to nearest lower multiple of num_groups\n",
        "    if size % num_groups != 0:\n",
        "        adjusted_size = (size // num_groups) * num_groups\n",
        "        print(f\"Adjusting subset size {size} -> {adjusted_size} for divisibility by {num_groups}\")\n",
        "        size = adjusted_size\n",
        "\n",
        "    per_group_n = size // num_groups\n",
        "    balanced_indices = []\n",
        "\n",
        "    for (race, gender), group_indices in indices_by_race_gender.items():\n",
        "        if len(group_indices) < per_group_n:\n",
        "            raise ValueError(f\"Group {race} {gender} has fewer samples ({len(group_indices)}) than requested {per_group_n}\")\n",
        "        shuffled = np.copy(group_indices)\n",
        "        rng.shuffle(shuffled)\n",
        "        balanced_indices.extend(shuffled[:per_group_n])\n",
        "\n",
        "        if gender == 0:\n",
        "            train_subsets_f.setdefault(size, []).extend(shuffled[:per_group_n])\n",
        "        else:\n",
        "            train_subsets_m.setdefault(size, []).extend(shuffled[:per_group_n])\n",
        "\n",
        "    balanced_indices = np.array(balanced_indices)\n",
        "    rng.shuffle(balanced_indices)\n",
        "\n",
        "    train_subsets[size] = balanced_indices\n",
        "\n",
        "print(f\"Females: {len(train_subsets_f[size])}, Males: {len(train_subsets_m[size])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzEQOxm5rm8G",
        "outputId": "98bd7b00-14fb-4214-f83e-64b328c8c4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique races in training data: ['asian' 'black' 'white']\n",
            "Count for asian female: 1800\n",
            "Count for asian male: 1800\n",
            "Count for black female: 1800\n",
            "Count for black male: 1800\n",
            "Count for white female: 1800\n",
            "Count for white male: 1800\n",
            "Balancing all race-gender groups to 1800 samples each\n",
            "Adjusting subset size 100 -> 96 for divisibility by 6\n",
            "Adjusting subset size 500 -> 498 for divisibility by 6\n",
            "Adjusting subset size 1000 -> 996 for divisibility by 6\n",
            "Females: 498, Males: 498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # above working for just gender\n",
        "# import numpy as np\n",
        "\n",
        "# # Let's say these are your sample indices for training (could be all training indices or a subset)\n",
        "# train_indices = np.arange(len(dataset))  # or define a subset of indices here\n",
        "\n",
        "# # Extract gender labels for those indices\n",
        "# gender_labels = np.array([dataset[i][1] for i in train_indices])  # 0=female,1=male\n",
        "\n",
        "# # Find indices per gender within train_indices:\n",
        "# female_indices = train_indices[gender_labels == 0]\n",
        "# male_indices = train_indices[gender_labels == 1]\n",
        "\n",
        "# print(f\"Number females: {len(female_indices)}, males: {len(male_indices)}\")\n",
        "\n",
        "# # Determine max balanced subset size per gender:\n",
        "# N_train = min(len(female_indices), len(male_indices))\n",
        "# print(f\"Using balanced training subset size per gender: {N_train}\")\n",
        "\n",
        "# # Shuffle indices reproducibly:\n",
        "# rng = np.random.default_rng(seed=42)\n",
        "# rng.shuffle(female_indices)\n",
        "# rng.shuffle(male_indices)\n",
        "\n",
        "# # Choose balanced subsets\n",
        "# female_subset = female_indices[:N_train]\n",
        "# male_subset = male_indices[:N_train]\n",
        "\n",
        "# # Combine subsets for final balanced training set\n",
        "# balanced_train_indices = np.concatenate([female_subset, male_subset])\n",
        "# print(f\"Total balanced training subset size: {len(balanced_train_indices)}\")\n",
        "\n",
        "# # Optionally, shuffle the final combined indices again for mixing\n",
        "# rng.shuffle(balanced_train_indices)\n",
        "\n",
        "\n",
        "# # --- Create dictionary with splits if you want subsets of different sizes ---\n",
        "\n",
        "# subset_sizes = [100, 500, 1000]  # Example sizes\n",
        "# train_subsets_f = {}\n",
        "# train_subsets_m = {}\n",
        "# train_subsets = {}\n",
        "\n",
        "# for size in subset_sizes:\n",
        "#     n = size // 2  # half per gender\n",
        "#     train_subsets_f[size] = female_indices[:n]\n",
        "#     train_subsets_m[size] = male_indices[:n]\n",
        "#     train_subsets[size] = np.concatenate([train_subsets_f[size], train_subsets_m[size]])\n",
        "#     print(f\"Created train subset size {size} with {n} females and {n} males\")\n",
        "\n",
        "# # Now `train_subsets` dict contains balanced splits keyed by size, e.g. train_subsets[500]"
      ],
      "metadata": {
        "id": "4Y-O4EkQsVNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Workign code:\n",
        "# import numpy as np\n",
        "# from torch.utils.data import Subset\n",
        "\n",
        "# # Example dataset: list of (img_path, gender, race)\n",
        "# # Assuming your dataset list and labels are prepared as:\n",
        "# # dataset_gender_labels = np.array([sample[1] for sample in dataset])\n",
        "# # dataset_race_labels = np.array([sample[2] for sample in dataset])\n",
        "\n",
        "# # Setup\n",
        "# proportions = [100, 500, 1000]  # requested subset sizes\n",
        "# # Using dataset variables:\n",
        "# dataset_gender_labels = np.array([s[1] for s in dataset])\n",
        "# dataset_race_labels = np.array([s[2] for s in dataset])\n",
        "\n",
        "# unique_races = np.unique(dataset_race_labels)\n",
        "# train_indices = np.arange(len(dataset))\n",
        "\n",
        "# # Group indices by race and gender\n",
        "# indices_by_race_gender = {}\n",
        "# for race in unique_races:\n",
        "#     for gender in [0, 1]:\n",
        "#         mask = (dataset_race_labels == race) & (dataset_gender_labels == gender)\n",
        "#         group_indices = train_indices[mask]\n",
        "#         indices_by_race_gender[(race, gender)] = group_indices\n",
        "#         print(f\"Count for race {race} {'female' if gender == 0 else 'male'}: {len(group_indices)}\")\n",
        "\n",
        "# # Number of groups = #races * 2\n",
        "# num_groups = len(unique_races) * 2\n",
        "\n",
        "# # Find minimal count to ensure balanced sampling possible\n",
        "# min_count = min(len(idxs) for idxs in indices_by_race_gender.values())\n",
        "# print(f\"Minimal group size for balanced sampling: {min_count}\")\n",
        "\n",
        "# # RNG for repeatability\n",
        "# rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# # Containers for results\n",
        "# train_subsets = {}\n",
        "# train_subsets_f = {}\n",
        "# train_subsets_m = {}\n",
        "# adjusted_sizes_map = {}  # maps requested size -> adjusted size\n",
        "\n",
        "# for size in proportions:\n",
        "#     adjusted_size = size\n",
        "#     if size % num_groups != 0:\n",
        "#         adjusted_size = (size // num_groups) * num_groups\n",
        "#         print(f\"Adjusting subset size {size} -> {adjusted_size} for divisibility by {num_groups}\")\n",
        "#     adjusted_sizes_map[size] = adjusted_size\n",
        "\n",
        "#     per_group_n = adjusted_size // num_groups\n",
        "#     if per_group_n > min_count:\n",
        "#         raise ValueError(f\"Requested samples per group {per_group_n} exceeds minimum available group size {min_count}\")\n",
        "\n",
        "#     balanced_indices = []\n",
        "\n",
        "#     for (race, gender), group_indices in indices_by_race_gender.items():\n",
        "#         if len(group_indices) < per_group_n:\n",
        "#             raise ValueError(f\"Group {race} {gender} only has {len(group_indices)} samples but {per_group_n} requested.\")\n",
        "#         shuffled = np.copy(group_indices)\n",
        "#         rng.shuffle(shuffled)\n",
        "#         chosen = shuffled[:per_group_n]\n",
        "#         balanced_indices.extend(chosen)\n",
        "\n",
        "#         if gender == 0:\n",
        "#             train_subsets_f.setdefault(adjusted_size, []).extend(chosen)\n",
        "#         else:\n",
        "#             train_subsets_m.setdefault(adjusted_size, []).extend(chosen)\n",
        "\n",
        "#     balanced_indices = np.array(balanced_indices)\n",
        "#     rng.shuffle(balanced_indices)\n",
        "#     train_subsets[adjusted_size] = balanced_indices\n",
        "\n",
        "# print(\"Balanced subsets created:\")\n",
        "\n",
        "# # Example usage and verification:\n",
        "# for req_size in proportions:\n",
        "#     adj_size = adjusted_sizes_map[req_size]\n",
        "#     print(f\"\\nRequested size: {req_size}, adjusted size: {adj_size}\")\n",
        "#     fem_count = len(train_subsets_f[adj_size])\n",
        "#     male_count = len(train_subsets_m[adj_size])\n",
        "#     total = fem_count + male_count\n",
        "#     print(f\"Females: {fem_count}, Males: {male_count}, Total: {total}\")\n",
        "#     fem_ratio = fem_count / total if total > 0 else 0\n",
        "#     print(f\"Female ratio: {fem_ratio:.2%}\")\n",
        "\n",
        "#     # Create PyTorch Subset (if using PyTorch Dataset)\n",
        "#     train_subset_torch = Subset(dataset, train_subsets[adj_size])\n",
        "#     print(f\"PyTorch Subset with {len(train_subset_torch)} samples ready for training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcEsAbGTsV8M",
        "outputId": "0ab8ae93-b5a0-4929-f4d9-f5dd52879c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count for race asian female: 1800\n",
            "Count for race asian male: 1800\n",
            "Count for race black female: 1800\n",
            "Count for race black male: 1800\n",
            "Count for race white female: 1800\n",
            "Count for race white male: 1800\n",
            "Minimal group size for balanced sampling: 1800\n",
            "Adjusting subset size 100 -> 96 for divisibility by 6\n",
            "Adjusting subset size 500 -> 498 for divisibility by 6\n",
            "Adjusting subset size 1000 -> 996 for divisibility by 6\n",
            "Balanced subsets created:\n",
            "\n",
            "Requested size: 100, adjusted size: 96\n",
            "Females: 48, Males: 48, Total: 96\n",
            "Female ratio: 50.00%\n",
            "PyTorch Subset with 96 samples ready for training.\n",
            "\n",
            "Requested size: 500, adjusted size: 498\n",
            "Females: 249, Males: 249, Total: 498\n",
            "Female ratio: 50.00%\n",
            "PyTorch Subset with 498 samples ready for training.\n",
            "\n",
            "Requested size: 1000, adjusted size: 996\n",
            "Females: 498, Males: 498, Total: 996\n",
            "Female ratio: 50.00%\n",
            "PyTorch Subset with 996 samples ready for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2:\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from torch.utils.data import Subset\n",
        "\n",
        "# # Assume dataset: list of (img_path, gender, race)\n",
        "# dataset_gender_labels = np.array([s[1] for s in dataset])\n",
        "# dataset_race_labels = np.array([s[2] for s in dataset])\n",
        "\n",
        "# # Stratified split by gender (to preserve gender distribution in train/test)\n",
        "# full_indices = np.arange(len(dataset))\n",
        "# train_indices, test_indices = train_test_split(\n",
        "#     full_indices,\n",
        "#     test_size=0.2,\n",
        "#     random_state=42,\n",
        "#     stratify=dataset_gender_labels\n",
        "# )\n",
        "# print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
        "\n",
        "# proportions = [0.25, 0.5, 0.75]\n",
        "# unique_races = np.unique(dataset_race_labels)\n",
        "# rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# # ------ Group 1: Gender partitions (all races) ------\n",
        "\n",
        "# gender_train_subsets = {}\n",
        "# gender_test_subsets_f = {}\n",
        "# gender_test_subsets_m = {}\n",
        "\n",
        "# gender_train_labels = dataset_gender_labels[train_indices]\n",
        "# gender_test_labels = dataset_gender_labels[test_indices]\n",
        "\n",
        "# for p in proportions:\n",
        "#     female_train_idx = np.where(gender_train_labels == 0)[0]\n",
        "#     male_train_idx = np.where(gender_train_labels == 1)[0]\n",
        "#     N_train = min(len(female_train_idx), len(male_train_idx))\n",
        "#     rng.shuffle(female_train_idx)\n",
        "#     rng.shuffle(male_train_idx)\n",
        "#     num_female_train = int(N_train * p)\n",
        "#     num_male_train = N_train - num_female_train\n",
        "#     chosen_female_train_abs = train_indices[female_train_idx[:num_female_train]]\n",
        "#     chosen_male_train_abs = train_indices[male_train_idx[:num_male_train]]\n",
        "#     combined_train = np.concatenate([chosen_female_train_abs, chosen_male_train_abs])\n",
        "#     rng.shuffle(combined_train)\n",
        "#     gender_train_subsets[p] = Subset(dataset, combined_train)\n",
        "\n",
        "#     # Test balanced 50/50 female/male overall (not broken down by race)\n",
        "#     female_test_idx = np.where(gender_test_labels == 0)[0]\n",
        "#     male_test_idx = np.where(gender_test_labels == 1)[0]\n",
        "#     N_test = min(len(female_test_idx), len(male_test_idx))\n",
        "#     rng.shuffle(female_test_idx)\n",
        "#     rng.shuffle(male_test_idx)\n",
        "#     half_test = N_test // 2\n",
        "#     chosen_female_test = test_indices[female_test_idx[:half_test]]\n",
        "#     chosen_male_test = test_indices[male_test_idx[:half_test]]\n",
        "#     gender_test_subsets_f[p] = Subset(dataset, chosen_female_test)\n",
        "#     gender_test_subsets_m[p] = Subset(dataset, chosen_male_test)\n",
        "\n",
        "# # ------ Group 2: Race partitions (all genders, varying race proportion) ------\n",
        "\n",
        "# race_train_subsets = {}\n",
        "# # Test here is NOT race stratified, so a single test subset balanced 50/50 female/male from entire test set\n",
        "# # We'll build this test set once:\n",
        "# female_test_idx = np.where(gender_test_labels == 0)[0]\n",
        "# male_test_idx = np.where(gender_test_labels == 1)[0]\n",
        "# N_test = min(len(female_test_idx), len(male_test_idx))\n",
        "# rng.shuffle(female_test_idx)\n",
        "# rng.shuffle(male_test_idx)\n",
        "# half_test = N_test // 2\n",
        "# chosen_female_test = test_indices[female_test_idx[:half_test]]\n",
        "# chosen_male_test = test_indices[male_test_idx[:half_test]]\n",
        "# common_test_subset_f = Subset(dataset, chosen_female_test)\n",
        "# common_test_subset_m = Subset(dataset, chosen_male_test)\n",
        "\n",
        "# for race in unique_races:\n",
        "#     race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "#     race_train_indices = train_indices[race_mask_train]\n",
        "#     nonrace_train_indices = train_indices[~race_mask_train]\n",
        "\n",
        "#     total_race = len(race_train_indices)\n",
        "#     total_nonrace = len(nonrace_train_indices)\n",
        "\n",
        "#     for p in proportions:\n",
        "#         # Desired total subset size = len(train_indices) * some fraction, but here we just produce race proportion subsets\n",
        "#         # e.g. subset that is p portion race, (1 - p) portion non-race\n",
        "\n",
        "#         num_race_samples = int(total_race * p)\n",
        "#         num_nonrace_samples = min(total_nonrace, int(total_race * (1 - p)))  # keep size balanced-ish\n",
        "\n",
        "#         if num_race_samples == 0 or num_nonrace_samples == 0:\n",
        "#             print(f\"Warning: insufficient data for race {race} with proportion {p}, skipping.\")\n",
        "#             continue\n",
        "\n",
        "#         rng.shuffle(race_train_indices)\n",
        "#         rng.shuffle(nonrace_train_indices)\n",
        "\n",
        "#         chosen_race_abs = race_train_indices[:num_race_samples]\n",
        "#         chosen_nonrace_abs = nonrace_train_indices[:num_nonrace_samples]\n",
        "\n",
        "#         combined = np.concatenate([chosen_race_abs, chosen_nonrace_abs])\n",
        "#         rng.shuffle(combined)\n",
        "\n",
        "#         race_train_subsets[(race, p)] = Subset(dataset, combined)\n",
        "\n",
        "# # ------ Group 3: Intersectional partitions (race + gender) ------\n",
        "\n",
        "# intersection_train_subsets = {}\n",
        "# intersection_test_subsets_f = {}\n",
        "# intersection_test_subsets_m = {}\n",
        "\n",
        "# for race in unique_races:\n",
        "#     race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "#     race_train_indices = train_indices[race_mask_train]\n",
        "#     gender_train_race = dataset_gender_labels[race_train_indices]\n",
        "\n",
        "#     race_mask_test = (dataset_race_labels[test_indices] == race)\n",
        "#     race_test_indices = test_indices[race_mask_test]\n",
        "#     gender_test_race = dataset_gender_labels[race_test_indices]\n",
        "\n",
        "#     female_train_idx = np.where(gender_train_race == 0)[0]\n",
        "#     male_train_idx = np.where(gender_train_race == 1)[0]\n",
        "\n",
        "#     female_test_idx = np.where(gender_test_race == 0)[0]\n",
        "#     male_test_idx = np.where(gender_test_race == 1)[0]\n",
        "\n",
        "#     N_train = min(len(female_train_idx), len(male_train_idx))\n",
        "#     N_test = min(len(female_test_idx), len(male_test_idx))\n",
        "\n",
        "#     if N_train == 0 or N_test == 0:\n",
        "#         print(f\"Warning: insufficient samples for race {race}, skipping intersectional subsets.\")\n",
        "#         continue\n",
        "\n",
        "#     rng.shuffle(female_train_idx)\n",
        "#     rng.shuffle(male_train_idx)\n",
        "#     rng.shuffle(female_test_idx)\n",
        "#     rng.shuffle(male_test_idx)\n",
        "\n",
        "#     half_test = N_test // 2\n",
        "\n",
        "#     for p in proportions:\n",
        "#         num_female_train = int(N_train * p)\n",
        "#         num_male_train = N_train - num_female_train\n",
        "\n",
        "#         chosen_female_train_abs = race_train_indices[female_train_idx[:num_female_train]]\n",
        "#         chosen_male_train_abs = race_train_indices[male_train_idx[:num_male_train]]\n",
        "#         combined_train = np.concatenate([chosen_female_train_abs, chosen_male_train_abs])\n",
        "#         rng.shuffle(combined_train)\n",
        "\n",
        "#         chosen_female_test_abs = race_test_indices[female_test_idx[:half_test]]\n",
        "#         chosen_male_test_abs = race_test_indices[male_test_idx[:half_test]]\n",
        "\n",
        "#         intersection_train_subsets[(race, p)] = Subset(dataset, combined_train)\n",
        "#         intersection_test_subsets_f[(race, p)] = Subset(dataset, chosen_female_test_abs)\n",
        "#         intersection_test_subsets_m[(race, p)] = Subset(dataset, chosen_male_test_abs)\n",
        "\n",
        "# # -------- Printing summary --------\n",
        "\n",
        "# print(\"=== Gender partitions (all races combined) ===\")\n",
        "# for p in proportions:\n",
        "#     train_subset = gender_train_subsets[p]\n",
        "#     train_genders = [dataset[i][1] for i in train_subset.indices]\n",
        "#     total_train = len(train_genders)\n",
        "#     pct_female = np.mean(np.array(train_genders) == 0) if total_train > 0 else 0\n",
        "#     print(f\"Prop {int(p*100)}: Train samples = {total_train}, Female ratio = {pct_female*100:.2f}%\")\n",
        "#     print(f\"  Test Females: {len(gender_test_subsets_f[p].indices)}\")\n",
        "#     print(f\"  Test Males: {len(gender_test_subsets_m[p].indices)}\")\n",
        "\n",
        "# print(\"\\n=== Race partitions (all genders) ===\")\n",
        "# for (race, p), subset in sorted(race_train_subsets.items()):\n",
        "#     train_size = len(subset.indices)\n",
        "#     print(f\"Race {race} Proportion {int(p*100)}: Train samples = {train_size}\")\n",
        "# print(f\"Common test subset Females: {len(common_test_subset_f.indices)}\")\n",
        "# print(f\"Common test subset Males: {len(common_test_subset_m.indices)}\")\n",
        "\n",
        "# print(\"\\n=== Intersectional partitions (race + gender) ===\")\n",
        "# for (race, p), subset in sorted(intersection_train_subsets.items()):\n",
        "#     train_genders = [dataset[i][1] for i in subset.indices]\n",
        "#     total_train = len(train_genders)\n",
        "#     pct_female = np.mean(np.array(train_genders) == 0) if total_train > 0 else 0\n",
        "#     print(f\"Race {race} Prop {int(p*100)}: Train samples = {total_train}, Female ratio = {pct_female*100:.2f}%\")\n",
        "#     f_test = intersection_test_subsets_f.get((race, p), None)\n",
        "#     m_test = intersection_test_subsets_m.get((race, p), None)\n",
        "#     f_count = len(f_test.indices) if f_test else 0\n",
        "#     m_count = len(m_test.indices) if m_test else 0\n",
        "#     print(f\"  Test Females: {f_count}, Test Males: {m_count}\")\n",
        "\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from torch.utils.data import Subset\n",
        "\n",
        "# # Assume dataset = list of (img_path, gender, race)\n",
        "# dataset_race_labels = np.array([s[2] for s in dataset])\n",
        "# dataset_gender_labels = np.array([s[1] for s in dataset])\n",
        "\n",
        "# # Stratified split by gender (to preserve gender distribution)\n",
        "# full_indices = np.arange(len(dataset))\n",
        "# train_indices, test_indices = train_test_split(\n",
        "#     full_indices,\n",
        "#     test_size=0.2,\n",
        "#     random_state=42,\n",
        "#     stratify=dataset_gender_labels\n",
        "# )\n",
        "# print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
        "\n",
        "# proportions = [0.25, 0.5, 0.75]\n",
        "# unique_races = np.unique(dataset_race_labels)\n",
        "# rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# race_train_subsets = {}\n",
        "# race_test_subsets = {}\n",
        "\n",
        "# # Precompute test race indices & their counts for reporting\n",
        "# test_race_indices = {race: test_indices[dataset_race_labels[test_indices] == race] for race in unique_races}\n",
        "\n",
        "# for race in unique_races:\n",
        "#     race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "#     race_train_indices = train_indices[race_mask_train]\n",
        "\n",
        "#     nonrace_mask_train = (dataset_race_labels[train_indices] != race)\n",
        "#     nonrace_train_indices = train_indices[nonrace_mask_train]\n",
        "\n",
        "#     total_race = len(race_train_indices)\n",
        "#     total_nonrace = len(nonrace_train_indices)\n",
        "\n",
        "#     if total_race == 0 or total_nonrace == 0:\n",
        "#         print(f\"Warning: insufficient samples for race {race}, skipping.\")\n",
        "#         continue\n",
        "\n",
        "#     for p in proportions:\n",
        "#         num_race_samples = int(total_race * p)\n",
        "#         # To keep size balanced, pick equal number of nonrace samples proportional to race samples, or all nonrace if smaller\n",
        "#         num_nonrace_samples = min(total_nonrace, int(num_race_samples * (1 - p) / p)) if p > 0 else total_nonrace\n",
        "\n",
        "#         rng.shuffle(race_train_indices)\n",
        "#         rng.shuffle(nonrace_train_indices)\n",
        "\n",
        "#         chosen_race = race_train_indices[:num_race_samples]\n",
        "#         chosen_nonrace = nonrace_train_indices[:num_nonrace_samples]\n",
        "\n",
        "#         combined_train_indices = np.concatenate([chosen_race, chosen_nonrace])\n",
        "#         rng.shuffle(combined_train_indices)\n",
        "\n",
        "#         race_train_subsets[(race, p)] = Subset(dataset, combined_train_indices)\n",
        "#         race_test_subsets[race] = Subset(dataset, test_race_indices[race])\n",
        "\n",
        "#     # Print stats for race partitions\n",
        "#     print(f\"\\nRace '{race}' partitions:\")\n",
        "#     for p in proportions:\n",
        "#         subset = race_train_subsets[(race, p)]\n",
        "#         # Calculate actual race %:\n",
        "#         race_labels_in_subset = [dataset[i][2] for i in subset.indices]\n",
        "#         total = len(race_labels_in_subset)\n",
        "#         race_count = race_labels_in_subset.count(race)\n",
        "#         race_pct = race_count / total * 100 if total > 0 else 0\n",
        "\n",
        "#         print(f\"Train Subset (Prop {int(p*100)}%): Target {int(p*100)}% -- Actual {race_pct:.2f}% {race}, {total} samples\")\n",
        "\n",
        "#         # Test counts of race and other\n",
        "#         test_subset = race_test_subsets[race]\n",
        "#         test_labels = [dataset[i][2] for i in test_subset.indices]\n",
        "#         test_race_count = test_labels.count(race)\n",
        "#         test_other_count = len(test_labels) - test_race_count\n",
        "#         print(f\"Number of {race} test samples: {test_race_count}\")\n",
        "#         print(f\"Number of other test samples: {test_other_count}\")"
      ],
      "metadata": {
        "id": "g6jc0D6Q34-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Assume dataset is list of (img_path, gender, race)\n",
        "dataset_gender_labels = np.array([s[1] for s in dataset])\n",
        "dataset_race_labels = np.array([s[2] for s in dataset])\n",
        "\n",
        "# Train/test split stratified by gender (common for all groups)\n",
        "full_indices = np.arange(len(dataset))\n",
        "train_indices, test_indices = train_test_split(\n",
        "    full_indices, test_size=0.2, random_state=42, stratify=dataset_gender_labels\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "unique_races = np.unique(dataset_race_labels)\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# ========== Group 1: Gender partitions (all races combined) ==========\n",
        "gender_train_subsets = {}\n",
        "gender_test_subsets_f = {}\n",
        "gender_test_subsets_m = {}\n",
        "\n",
        "gender_train_labels = dataset_gender_labels[train_indices]\n",
        "gender_test_labels = dataset_gender_labels[test_indices]\n",
        "\n",
        "for p in proportions:\n",
        "    female_idx_train = np.where(gender_train_labels == 0)[0]\n",
        "    male_idx_train = np.where(gender_train_labels == 1)[0]\n",
        "\n",
        "    N_train = min(len(female_idx_train), len(male_idx_train))\n",
        "    rng.shuffle(female_idx_train)\n",
        "    rng.shuffle(male_idx_train)\n",
        "    num_female_train = int(N_train * p)\n",
        "    num_male_train = N_train - num_female_train\n",
        "\n",
        "    female_chosen = train_indices[female_idx_train[:num_female_train]]\n",
        "    male_chosen = train_indices[male_idx_train[:num_male_train]]\n",
        "\n",
        "    combined_train = np.concatenate([female_chosen, male_chosen])\n",
        "    rng.shuffle(combined_train)\n",
        "    gender_train_subsets[p] = Subset(dataset, combined_train)\n",
        "\n",
        "    # Test balanced 50/50 female/male overall\n",
        "    female_idx_test = np.where(gender_test_labels == 0)[0]\n",
        "    male_idx_test = np.where(gender_test_labels == 1)[0]\n",
        "    N_test = min(len(female_idx_test), len(male_idx_test))\n",
        "\n",
        "    rng.shuffle(female_idx_test)\n",
        "    rng.shuffle(male_idx_test)\n",
        "    half_test = N_test // 2\n",
        "\n",
        "    female_test_chosen = test_indices[female_idx_test[:half_test]]\n",
        "    male_test_chosen = test_indices[male_idx_test[:half_test]]\n",
        "\n",
        "    gender_test_subsets_f[p] = Subset(dataset, female_test_chosen)\n",
        "    gender_test_subsets_m[p] = Subset(dataset, male_test_chosen)\n",
        "\n",
        "# ========== Group 2: Race partitions (all genders, varying race proportion) ==========\n",
        "race_train_subsets = {}\n",
        "race_test_subsets = {}\n",
        "\n",
        "# Precompute test indices per race for reporting later\n",
        "test_race_indices = {race: test_indices[dataset_race_labels[test_indices] == race] for race in unique_races}\n",
        "\n",
        "for race in unique_races:\n",
        "    race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "    race_train_indices = train_indices[race_mask_train]\n",
        "\n",
        "    nonrace_mask_train = ~race_mask_train\n",
        "    nonrace_train_indices = train_indices[nonrace_mask_train]\n",
        "\n",
        "    total_race = len(race_train_indices)\n",
        "    total_nonrace = len(nonrace_train_indices)\n",
        "\n",
        "    if total_race == 0 or total_nonrace == 0:\n",
        "        print(f\"Warning: insufficient samples for race {race}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    for p in proportions:\n",
        "        num_race_samples = int(total_race * p)\n",
        "        # Balance size by sampling nonrace proportional to race samples\n",
        "        num_nonrace_samples = min(total_nonrace, int(num_race_samples * (1 - p) / p)) if p > 0 else total_nonrace\n",
        "\n",
        "        rng.shuffle(race_train_indices)\n",
        "        rng.shuffle(nonrace_train_indices)\n",
        "        chosen_race = race_train_indices[:num_race_samples]\n",
        "        chosen_nonrace = nonrace_train_indices[:num_nonrace_samples]\n",
        "\n",
        "        combined = np.concatenate([chosen_race, chosen_nonrace])\n",
        "        rng.shuffle(combined)\n",
        "\n",
        "        race_train_subsets[(race, p)] = Subset(dataset, combined)\n",
        "        race_test_subsets[race] = Subset(dataset, test_race_indices[race])\n",
        "\n",
        "# ========== Group 3: Intersectional partitions (race + gender) ==========\n",
        "intersection_train_subsets = {}\n",
        "intersection_test_subsets_f = {}\n",
        "intersection_test_subsets_m = {}\n",
        "\n",
        "for race in unique_races:\n",
        "    race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "    race_train_indices = train_indices[race_mask_train]\n",
        "    gender_train_race = dataset_gender_labels[race_train_indices]\n",
        "\n",
        "    race_mask_test = (dataset_race_labels[test_indices] == race)\n",
        "    race_test_indices = test_indices[race_mask_test]\n",
        "    gender_test_race = dataset_gender_labels[race_test_indices]\n",
        "\n",
        "    female_train_idx = np.where(gender_train_race == 0)[0]\n",
        "    male_train_idx = np.where(gender_train_race == 1)[0]\n",
        "    female_test_idx = np.where(gender_test_race == 0)[0]\n",
        "    male_test_idx = np.where(gender_test_race == 1)[0]\n",
        "\n",
        "    N_train = min(len(female_train_idx), len(male_train_idx))\n",
        "    N_test = min(len(female_test_idx), len(male_test_idx))\n",
        "\n",
        "    if N_train == 0 or N_test == 0:\n",
        "        print(f\"Warning: insufficient samples for race {race}, skipping intersectional subsets.\")\n",
        "        continue\n",
        "\n",
        "    rng.shuffle(female_train_idx)\n",
        "    rng.shuffle(male_train_idx)\n",
        "    rng.shuffle(female_test_idx)\n",
        "    rng.shuffle(male_test_idx)\n",
        "\n",
        "    half_test = N_test // 2\n",
        "\n",
        "    for p in proportions:\n",
        "        num_female_train = int(N_train * p)\n",
        "        num_male_train = N_train - num_female_train\n",
        "\n",
        "        chosen_female_train = race_train_indices[female_train_idx[:num_female_train]]\n",
        "        chosen_male_train = race_train_indices[male_train_idx[:num_male_train]]\n",
        "        combined_train = np.concatenate([chosen_female_train, chosen_male_train])\n",
        "        rng.shuffle(combined_train)\n",
        "\n",
        "        chosen_female_test = race_test_indices[female_test_idx[:half_test]]\n",
        "        chosen_male_test = race_test_indices[male_test_idx[:half_test]]\n",
        "\n",
        "        intersection_train_subsets[(race, p)] = Subset(dataset, combined_train)\n",
        "        intersection_test_subsets_f[(race, p)] = Subset(dataset, chosen_female_test)\n",
        "        intersection_test_subsets_m[(race, p)] = Subset(dataset, chosen_male_test)\n",
        "\n",
        "# -------- Printing summary --------\n",
        "\n",
        "print(\"=== Gender partitions (all races combined) ===\")\n",
        "for p in proportions:\n",
        "    train_subset = gender_train_subsets[p]\n",
        "    train_genders = [dataset[i][1] for i in train_subset.indices]\n",
        "    total_train = len(train_genders)\n",
        "    pct_female = np.mean(np.array(train_genders) == 0) if total_train > 0 else 0\n",
        "    print(f\"Prop {int(p*100)}: Train samples = {total_train}, Female ratio = {pct_female*100:.2f}%\")\n",
        "    print(f\"  Test Females: {len(gender_test_subsets_f[p].indices)}\")\n",
        "    print(f\"  Test Males: {len(gender_test_subsets_m[p].indices)}\")\n",
        "\n",
        "print(\"\\n=== Race partitions (all genders) ===\")\n",
        "for (race, p), subset in sorted(race_train_subsets.items()):\n",
        "    train_size = len(subset.indices)\n",
        "    print(f\"Race {race} Proportion {int(p*100)}: Train samples = {train_size}\")\n",
        "for race in unique_races:\n",
        "    test_sub = race_test_subsets.get(race, None)\n",
        "    if test_sub:\n",
        "        print(f\"Race {race} Test samples: {len(test_sub.indices)}\")\n",
        "\n",
        "print(\"\\n=== Intersectional partitions (race + gender) ===\")\n",
        "for (race, p), subset in sorted(intersection_train_subsets.items()):\n",
        "    train_genders = [dataset[i][1] for i in subset.indices]\n",
        "    total_train = len(train_genders)\n",
        "    pct_female = np.mean(np.array(train_genders) == 0) if total_train > 0 else 0\n",
        "    print(f\"Race {race} Prop {int(p*100)}: Train samples = {total_train}, Female ratio = {pct_female*100:.2f}%\")\n",
        "    f_test = intersection_test_subsets_f.get((race, p), None)\n",
        "    m_test = intersection_test_subsets_m.get((race, p), None)\n",
        "    f_count = len(f_test.indices) if f_test else 0\n",
        "    m_count = len(m_test.indices) if m_test else 0\n",
        "    print(f\"  Test Females: {f_count}, Test Males: {m_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COzW4jk8zn4Y",
        "outputId": "e3841604-a21f-4acf-9f29-1c2d83fa2d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 8640, Test samples: 2160\n",
            "=== Gender partitions (all races combined) ===\n",
            "Prop 25: Train samples = 4320, Female ratio = 25.00%\n",
            "  Test Females: 540\n",
            "  Test Males: 540\n",
            "Prop 50: Train samples = 4320, Female ratio = 50.00%\n",
            "  Test Females: 540\n",
            "  Test Males: 540\n",
            "Prop 75: Train samples = 4320, Female ratio = 75.00%\n",
            "  Test Females: 540\n",
            "  Test Males: 540\n",
            "\n",
            "=== Race partitions (all genders) ===\n",
            "Race asian Proportion 25: Train samples = 2876\n",
            "Race asian Proportion 50: Train samples = 2876\n",
            "Race asian Proportion 75: Train samples = 2876\n",
            "Race black Proportion 25: Train samples = 2912\n",
            "Race black Proportion 50: Train samples = 2912\n",
            "Race black Proportion 75: Train samples = 2912\n",
            "Race white Proportion 25: Train samples = 2848\n",
            "Race white Proportion 50: Train samples = 2850\n",
            "Race white Proportion 75: Train samples = 2850\n",
            "Race asian Test samples: 724\n",
            "Race black Test samples: 687\n",
            "Race white Test samples: 749\n",
            "\n",
            "=== Intersectional partitions (race + gender) ===\n",
            "Race asian Prop 25: Train samples = 1428, Female ratio = 25.00%\n",
            "  Test Females: 176, Test Males: 176\n",
            "Race asian Prop 50: Train samples = 1428, Female ratio = 50.00%\n",
            "  Test Females: 176, Test Males: 176\n",
            "Race asian Prop 75: Train samples = 1428, Female ratio = 75.00%\n",
            "  Test Females: 176, Test Males: 176\n",
            "Race black Prop 25: Train samples = 1438, Female ratio = 24.97%\n",
            "  Test Females: 162, Test Males: 162\n",
            "Race black Prop 50: Train samples = 1438, Female ratio = 50.00%\n",
            "  Test Females: 162, Test Males: 162\n",
            "Race black Prop 75: Train samples = 1438, Female ratio = 74.97%\n",
            "  Test Females: 162, Test Males: 162\n",
            "Race white Prop 25: Train samples = 1397, Female ratio = 24.98%\n",
            "  Test Females: 173, Test Males: 173\n",
            "Race white Prop 50: Train samples = 1397, Female ratio = 49.96%\n",
            "  Test Females: 173, Test Males: 173\n",
            "Race white Prop 75: Train samples = 1397, Female ratio = 74.95%\n",
            "  Test Females: 173, Test Males: 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# ----------------------------- #\n",
        "# Assume dataset is a list of (img_path, gender, race)\n",
        "dataset_gender_labels = np.array([s[1] for s in dataset])\n",
        "dataset_race_labels = np.array([s[2] for s in dataset])\n",
        "\n",
        "# Stratified train/test split by gender (to preserve gender distribution)\n",
        "full_indices = np.arange(len(dataset))\n",
        "train_indices, test_indices = train_test_split(\n",
        "    full_indices,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=dataset_gender_labels\n",
        ")\n",
        "print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]  # proportions to create subsets on\n",
        "unique_races = np.unique(dataset_race_labels)\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# --------------------------------- #\n",
        "# Group 1: Gender partitions (all races combined)\n",
        "gender_train_subsets = {}\n",
        "gender_test_subsets_f = {}\n",
        "gender_test_subsets_m = {}\n",
        "\n",
        "gender_train_labels = dataset_gender_labels[train_indices]\n",
        "gender_test_labels = dataset_gender_labels[test_indices]\n",
        "\n",
        "for p in proportions:\n",
        "    female_idx_train = np.where(gender_train_labels == 0)[0]\n",
        "    male_idx_train = np.where(gender_train_labels == 1)[0]\n",
        "    N_train = min(len(female_idx_train), len(male_idx_train))\n",
        "\n",
        "    rng.shuffle(female_idx_train)\n",
        "    rng.shuffle(male_idx_train)\n",
        "\n",
        "    num_female_train = int(N_train * p)\n",
        "    num_male_train = N_train - num_female_train\n",
        "\n",
        "    chosen_female_train_abs = train_indices[female_idx_train[:num_female_train]]\n",
        "    chosen_male_train_abs = train_indices[male_idx_train[:num_male_train]]\n",
        "\n",
        "    combined_train = np.concatenate([chosen_female_train_abs, chosen_male_train_abs])\n",
        "    rng.shuffle(combined_train)\n",
        "\n",
        "    gender_train_subsets[p] = Subset(dataset, combined_train)\n",
        "\n",
        "    # Balanced test sets (50% female, 50% male) overall\n",
        "    female_idx_test = np.where(gender_test_labels == 0)[0]\n",
        "    male_idx_test = np.where(gender_test_labels == 1)[0]\n",
        "    N_test = min(len(female_idx_test), len(male_idx_test))\n",
        "\n",
        "    rng.shuffle(female_idx_test)\n",
        "    rng.shuffle(male_idx_test)\n",
        "\n",
        "    half_test = N_test // 2\n",
        "\n",
        "    chosen_female_test = test_indices[female_idx_test[:half_test]]\n",
        "    chosen_male_test = test_indices[male_idx_test[:half_test]]\n",
        "\n",
        "    gender_test_subsets_f[p] = Subset(dataset, chosen_female_test)\n",
        "    gender_test_subsets_m[p] = Subset(dataset, chosen_male_test)\n",
        "\n",
        "# --------------------------------- #\n",
        "# Group 2: Race partitions (all genders; varying race proportions)\n",
        "\n",
        "race_train_subsets = {}\n",
        "race_test_subsets = {}\n",
        "\n",
        "# Precompute test indices by race for reporting + subset creation\n",
        "test_race_indices = {race: test_indices[dataset_race_labels[test_indices] == race] for race in unique_races}\n",
        "\n",
        "# Balanced test set for all races combined (50% female/male)\n",
        "female_test_idx = np.where(gender_test_labels == 0)[0]\n",
        "male_test_idx = np.where(gender_test_labels == 1)[0]\n",
        "N_test = min(len(female_test_idx), len(male_test_idx))\n",
        "rng.shuffle(female_test_idx)\n",
        "rng.shuffle(male_test_idx)\n",
        "half_test = N_test // 2\n",
        "global_test_females = test_indices[female_test_idx[:half_test]]\n",
        "global_test_males = test_indices[male_test_idx[:half_test]]\n",
        "common_test_subset_f = Subset(dataset, global_test_females)\n",
        "common_test_subset_m = Subset(dataset, global_test_males)\n",
        "\n",
        "for race in unique_races:\n",
        "    race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "    race_train_indices = train_indices[race_mask_train]\n",
        "    nonrace_train_indices = train_indices[~race_mask_train]\n",
        "\n",
        "    total_race = len(race_train_indices)\n",
        "    total_nonrace = len(nonrace_train_indices)\n",
        "\n",
        "    for p in proportions:\n",
        "        num_race_samples = int(total_race * p)\n",
        "        # Pick nonrace samples proportional to race samples to roughly balance size\n",
        "        num_nonrace_samples = min(total_nonrace, int(num_race_samples * (1 - p) / p)) if p > 0 else total_nonrace\n",
        "\n",
        "        if num_race_samples == 0 or num_nonrace_samples == 0:\n",
        "            print(f\"Warning: insufficient data for race {race} with proportion {p}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        rng.shuffle(race_train_indices)\n",
        "        rng.shuffle(nonrace_train_indices)\n",
        "\n",
        "        chosen_race = race_train_indices[:num_race_samples]\n",
        "        chosen_nonrace = nonrace_train_indices[:num_nonrace_samples]\n",
        "\n",
        "        combined = np.concatenate([chosen_race, chosen_nonrace])\n",
        "        rng.shuffle(combined)\n",
        "\n",
        "        race_train_subsets[(race, p)] = Subset(dataset, combined)\n",
        "        race_test_subsets[race] = Subset(dataset, test_race_indices[race])\n",
        "\n",
        "# --------------------------------- #\n",
        "# Group 3: Intersectional partitions (race + gender combined)\n",
        "\n",
        "intersection_train_subsets = {}\n",
        "intersection_test_subsets_f = {}\n",
        "intersection_test_subsets_m = {}\n",
        "\n",
        "for race in unique_races:\n",
        "    race_mask_train = (dataset_race_labels[train_indices] == race)\n",
        "    race_train_indices = train_indices[race_mask_train]\n",
        "    gender_train_race = dataset_gender_labels[race_train_indices]\n",
        "\n",
        "    race_mask_test = (dataset_race_labels[test_indices] == race)\n",
        "    race_test_indices = test_indices[race_mask_test]\n",
        "    gender_test_race = dataset_gender_labels[race_test_indices]\n",
        "\n",
        "    female_train_idx = np.where(gender_train_race == 0)[0]\n",
        "    male_train_idx = np.where(gender_train_race == 1)[0]\n",
        "    female_test_idx = np.where(gender_test_race == 0)[0]\n",
        "    male_test_idx = np.where(gender_test_race == 1)[0]\n",
        "\n",
        "    N_train = min(len(female_train_idx), len(male_train_idx))\n",
        "    N_test = min(len(female_test_idx), len(male_test_idx))\n",
        "\n",
        "    if N_train == 0 or N_test == 0:\n",
        "        print(f\"Warning: insufficient data for race {race}, skipping intersectional subsets.\")\n",
        "        continue\n",
        "\n",
        "    rng.shuffle(female_train_idx)\n",
        "    rng.shuffle(male_train_idx)\n",
        "    rng.shuffle(female_test_idx)\n",
        "    rng.shuffle(male_test_idx)\n",
        "\n",
        "    half_test = N_test // 2\n",
        "\n",
        "    for p in proportions:\n",
        "        num_female_train = int(N_train * p)\n",
        "        num_male_train = N_train - num_female_train\n",
        "\n",
        "        chosen_female_train_abs = race_train_indices[female_train_idx[:num_female_train]]\n",
        "        chosen_male_train_abs = race_train_indices[male_train_idx[:num_male_train]]\n",
        "        combined_train = np.concatenate([chosen_female_train_abs, chosen_male_train_abs])\n",
        "        rng.shuffle(combined_train)\n",
        "\n",
        "        chosen_female_test_abs = race_test_indices[female_test_idx[:half_test]]\n",
        "        chosen_male_test_abs = race_test_indices[male_test_idx[:half_test]]\n",
        "\n",
        "        intersection_train_subsets[(race, p)] = Subset(dataset, combined_train)\n",
        "        intersection_test_subsets_f[(race, p)] = Subset(dataset, chosen_female_test_abs)\n",
        "        intersection_test_subsets_m[(race, p)] = Subset(dataset, chosen_male_test_abs)\n",
        "\n",
        "# --------------- Helpers for DataLoaders ----------------\n",
        "\n",
        "def get_train_loader(group, key, batch_size=64):\n",
        "    if group == 'gender':\n",
        "        dataset_subset = gender_train_subsets[key]\n",
        "    elif group == 'race':\n",
        "        dataset_subset = race_train_subsets[key]\n",
        "    elif group == 'intersectional':\n",
        "        dataset_subset = intersection_train_subsets[key]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown group '{group}'\")\n",
        "    return DataLoader(dataset_subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def get_test_loaders(group, key, batch_size=64):\n",
        "    if group == 'gender':\n",
        "        return (\n",
        "            DataLoader(gender_test_subsets_f[key], batch_size=batch_size, shuffle=False),\n",
        "            DataLoader(gender_test_subsets_m[key], batch_size=batch_size, shuffle=False),\n",
        "        )\n",
        "    elif group == 'race':\n",
        "        # common test subsets for race group (all genders) - female and male split combined, so just one loader each\n",
        "        race = key[0] if isinstance(key, tuple) else key\n",
        "        return (\n",
        "            DataLoader(race_test_subsets[race], batch_size=batch_size, shuffle=False),\n",
        "            None,\n",
        "        )\n",
        "    elif group == 'intersectional':\n",
        "        return (\n",
        "            DataLoader(intersection_test_subsets_f[key], batch_size=batch_size, shuffle=False),\n",
        "            DataLoader(intersection_test_subsets_m[key], batch_size=batch_size, shuffle=False),\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown group '{group}'\")\n",
        "\n",
        "\n",
        "# --------------- Example usage ----------------\n",
        "batch_size = 64\n",
        "\n",
        "# Gender group loader example (50% females)\n",
        "train_loader_gender = get_train_loader('gender', 0.5, batch_size)\n",
        "\n",
        "# Race group loader example (50% proportion of 'black')\n",
        "train_loader_race = get_train_loader('race', ('black', 0.5), batch_size)\n",
        "\n",
        "# Intersectional group loader example ('white', 75% females)\n",
        "train_loader_intersection = get_train_loader('intersectional', ('white', 0.75), batch_size)\n",
        "\n",
        "print(f\"Gender group train loader batches: {len(train_loader_gender)}\")\n",
        "print(f\"Race group train loader batches: {len(train_loader_race)}\")\n",
        "print(f\"Intersectional group train loader batches: {len(train_loader_intersection)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBiKz7G9yyFb",
        "outputId": "45da4174-c10c-4965-a52b-a13aefccc1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 8640, Test samples: 2160\n",
            "Gender group train loader batches: 68\n",
            "Race group train loader batches: 46\n",
            "Intersectional group train loader batches: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# import numpy as np\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# batch_size = 16\n",
        "# num_epochs = 5  # small number for quick run\n",
        "\n",
        "\n",
        "# # --- Dummy dataset class to simulate data and labels ----\n",
        "# class DummyDataset(Dataset):\n",
        "#     def __init__(self, num_samples, num_classes=10):\n",
        "#         # Random images 3x64x64, labels in num_classes\n",
        "#         self.images = torch.randn(num_samples, 3, 64, 64)\n",
        "#         self.labels = torch.randint(0, num_classes, (num_samples,))\n",
        "#     def __len__(self):\n",
        "#         return len(self.images)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.images[idx], self.labels[idx].unsqueeze(0)  # labels shaped (1,)\n",
        "\n",
        "\n",
        "# # --- Dummy ArcHead as used in ArcFace etc. ---\n",
        "# class ArcHead(nn.Module):\n",
        "#     def __init__(self, in_features, out_features):\n",
        "#         super().__init__()\n",
        "#         self.fc = nn.Linear(in_features, out_features)\n",
        "#     def forward(self, features, labels):\n",
        "#         # labels unused here for simplicity\n",
        "#         return self.fc(features)\n",
        "\n",
        "\n",
        "# # --- Dummy backbone (ResNet-like) ---\n",
        "# backbone = nn.Sequential(\n",
        "#     nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
        "#     nn.ReLU(),\n",
        "#     nn.AdaptiveAvgPool2d((1,1)),\n",
        "#     nn.Flatten()\n",
        "# )\n",
        "# num_features = 16  # output channels from backbone\n",
        "\n",
        "# # Dummy arc_head for 10 identities\n",
        "# arc_head = ArcHead(num_features, 10)\n",
        "\n",
        "# # Dummy train/test subsets (normally from your pre-split Subsets)\n",
        "# train_set = DummyDataset(200, num_classes=10)\n",
        "# test_set_f = DummyDataset(40, num_classes=10)\n",
        "# test_set_m = DummyDataset(40, num_classes=10)\n",
        "\n",
        "# # Define a dummy evaluate function\n",
        "# def evaluate(model, arc_head, loader, device):\n",
        "#     model.eval()\n",
        "#     arc_head.eval()\n",
        "#     total, correct = 0, 0\n",
        "#     running_loss = 0\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in loader:\n",
        "#             images, labels = images.to(device), labels[:,0].to(device)\n",
        "#             features = model(images)\n",
        "#             logits = arc_head(features, labels)\n",
        "#             loss = criterion(logits, labels)\n",
        "#             running_loss += loss.item() * images.size(0)\n",
        "#             preds = torch.argmax(logits, dim=1)\n",
        "#             correct += (preds == labels).sum().item()\n",
        "#             total += images.size(0)\n",
        "#     return running_loss / total, correct / total\n",
        "\n",
        "\n",
        "# # Example proportions to test\n",
        "# proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "# for proportion in proportions:\n",
        "#     print(f\"\\n=== Training with female proportion {proportion} ===\")\n",
        "#     model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "#     arc_head = arc_head.to(device)   # <---- MOVE arc_head to device here\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "#     best_acc = 0.0\n",
        "\n",
        "#     # For testing/demo, use dummy loaders (replace with your real Subsets/DataLoaders)\n",
        "#     train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "#     val_loader_f = DataLoader(test_set_f, batch_size=batch_size, shuffle=False)\n",
        "#     val_loader_m = DataLoader(test_set_m, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         arc_head.train()\n",
        "#         total, correct, running_loss = 0, 0, 0.0\n",
        "\n",
        "#         for i, (images, labels) in enumerate(train_loader):\n",
        "#             images, labels = images.to(device), labels[:,0].to(device)\n",
        "#             features = model(images)\n",
        "#             logits = arc_head(features, labels)\n",
        "#             loss = criterion(logits, labels)\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item() * images.size(0)\n",
        "#             preds = logits.argmax(1)\n",
        "#             correct += (preds == labels).sum().item()\n",
        "#             total += images.size(0)\n",
        "\n",
        "#             if (i+1) % 10 == 0 or (i+1) == len(train_loader):\n",
        "#                 print(f\"Epoch {epoch+1} Batch {i+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "#         epoch_loss = running_loss / total\n",
        "#         epoch_acc = correct / total * 100\n",
        "#         print(f\"Epoch {epoch+1} completed: Loss={epoch_loss:.4f}, Acc={epoch_acc:.2f}%\")\n",
        "\n",
        "#         val_loss_f, val_acc_f = evaluate(model, arc_head, val_loader_f, device)\n",
        "#         val_loss_m, val_acc_m = evaluate(model, arc_head, val_loader_m, device)\n",
        "\n",
        "#         avg_val_acc = (val_acc_f + val_acc_m) / 2\n",
        "#         if avg_val_acc > best_acc:\n",
        "#             best_acc = avg_val_acc\n",
        "#             print(f\"New best model at epoch {epoch+1} with avg val acc: {avg_val_acc*100:.2f}%\")\n",
        "\n",
        "# print(\"Sample run complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03x8QWJi4fOJ",
        "outputId": "824287ef-5dd0-424c-e2bc-2a766956fffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training with female proportion 0.25 ===\n",
            "Epoch 1 Batch 10/13 - Loss: 2.3580\n",
            "Epoch 1 Batch 13/13 - Loss: 2.3095\n",
            "Epoch 1 completed: Loss=2.3184, Acc=12.50%\n",
            "New best model at epoch 1 with avg val acc: 10.00%\n",
            "Epoch 2 Batch 10/13 - Loss: 2.2662\n",
            "Epoch 2 Batch 13/13 - Loss: 2.2603\n",
            "Epoch 2 completed: Loss=2.3117, Acc=13.00%\n",
            "Epoch 3 Batch 10/13 - Loss: 2.3123\n",
            "Epoch 3 Batch 13/13 - Loss: 2.4038\n",
            "Epoch 3 completed: Loss=2.3063, Acc=13.00%\n",
            "Epoch 4 Batch 10/13 - Loss: 2.3808\n",
            "Epoch 4 Batch 13/13 - Loss: 2.3197\n",
            "Epoch 4 completed: Loss=2.3009, Acc=13.00%\n",
            "Epoch 5 Batch 10/13 - Loss: 2.2887\n",
            "Epoch 5 Batch 13/13 - Loss: 2.3316\n",
            "Epoch 5 completed: Loss=2.2980, Acc=13.00%\n",
            "\n",
            "=== Training with female proportion 0.5 ===\n",
            "Epoch 1 Batch 10/13 - Loss: 2.2538\n",
            "Epoch 1 Batch 13/13 - Loss: 2.3424\n",
            "Epoch 1 completed: Loss=2.2953, Acc=13.00%\n",
            "New best model at epoch 1 with avg val acc: 10.00%\n",
            "Epoch 2 Batch 10/13 - Loss: 2.2470\n",
            "Epoch 2 Batch 13/13 - Loss: 2.3527\n",
            "Epoch 2 completed: Loss=2.2923, Acc=13.00%\n",
            "Epoch 3 Batch 10/13 - Loss: 2.2709\n",
            "Epoch 3 Batch 13/13 - Loss: 2.2561\n",
            "Epoch 3 completed: Loss=2.2898, Acc=13.00%\n",
            "Epoch 4 Batch 10/13 - Loss: 2.3811\n",
            "Epoch 4 Batch 13/13 - Loss: 2.3230\n",
            "Epoch 4 completed: Loss=2.2876, Acc=13.00%\n",
            "Epoch 5 Batch 10/13 - Loss: 2.2276\n",
            "Epoch 5 Batch 13/13 - Loss: 2.3318\n",
            "Epoch 5 completed: Loss=2.2868, Acc=13.00%\n",
            "\n",
            "=== Training with female proportion 0.75 ===\n",
            "Epoch 1 Batch 10/13 - Loss: 2.2745\n",
            "Epoch 1 Batch 13/13 - Loss: 2.4078\n",
            "Epoch 1 completed: Loss=2.2863, Acc=13.00%\n",
            "New best model at epoch 1 with avg val acc: 10.00%\n",
            "Epoch 2 Batch 10/13 - Loss: 2.2441\n",
            "Epoch 2 Batch 13/13 - Loss: 2.2943\n",
            "Epoch 2 completed: Loss=2.2844, Acc=13.00%\n",
            "Epoch 3 Batch 10/13 - Loss: 2.2879\n",
            "Epoch 3 Batch 13/13 - Loss: 2.2829\n",
            "Epoch 3 completed: Loss=2.2836, Acc=13.00%\n",
            "Epoch 4 Batch 10/13 - Loss: 2.2135\n",
            "Epoch 4 Batch 13/13 - Loss: 2.2724\n",
            "Epoch 4 completed: Loss=2.2828, Acc=13.00%\n",
            "Epoch 5 Batch 10/13 - Loss: 2.3317\n",
            "Epoch 5 Batch 13/13 - Loss: 2.2464\n",
            "Epoch 5 completed: Loss=2.2822, Acc=13.00%\n",
            "Sample run complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 64\n",
        "num_epochs = 5  # Adjust for your needs\n",
        "\n",
        "def evaluate(model, arc_head, loader, device):\n",
        "    model.eval()\n",
        "    arc_head.eval()\n",
        "    running_corrects = 0\n",
        "    running_loss = 0\n",
        "    total_samples = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels[:, 0].to(device)\n",
        "            features = model(images)\n",
        "            logits = arc_head(features, labels)\n",
        "            loss = criterion(logits, labels)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            total_samples += images.size(0)\n",
        "    return running_loss / total_samples, running_corrects / total_samples\n",
        "\n",
        "def train_and_eval_group(name, train_subsets, test_subsets_f, test_subsets_m, proportions,\n",
        "                         backbone, arc_head, batch_size=64, epochs=5):\n",
        "    for p in proportions:\n",
        "        print(f\"\\n== Group: {name} - Training with proportion {p} ==\")\n",
        "        model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "        arc_head = arc_head.to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "        best_acc = 0\n",
        "        train_loader = DataLoader(train_subsets[p], batch_size=batch_size, shuffle=True)\n",
        "        val_loader_f = DataLoader(test_subsets_f[p], batch_size=batch_size, shuffle=False)\n",
        "        val_loader_m = DataLoader(test_subsets_m[p], batch_size=batch_size, shuffle=False)\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            arc_head.train()\n",
        "            total_loss, total_correct, total_samples = 0.0, 0, 0\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels[:, 0].to(device)\n",
        "                optimizer.zero_grad()\n",
        "                features = model(images)\n",
        "                logits = arc_head(features, labels)\n",
        "                loss = criterion(logits, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                total_correct += (preds == labels).sum().item()\n",
        "                total_loss += loss.item() * images.size(0)\n",
        "                total_samples += images.size(0)\n",
        "            train_loss = total_loss / total_samples\n",
        "            train_acc = total_correct / total_samples\n",
        "            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}%\")\n",
        "            val_loss_f, val_acc_f = evaluate(model, arc_head, val_loader_f, device)\n",
        "            val_loss_m, val_acc_m = evaluate(model, arc_head, val_loader_m, device)\n",
        "            avg_val_acc = (val_acc_f + val_acc_m) / 2\n",
        "            print(f\"Val Acc Female: {val_acc_f*100:.2f}%, Male: {val_acc_m*100:.2f}%, Avg: {avg_val_acc*100:.2f}%\")\n",
        "            if avg_val_acc > best_acc:\n",
        "                best_acc = avg_val_acc\n",
        "                print(f\"New best avg val acc at epoch {epoch+1}: {best_acc*100:.2f}%\")\n",
        "    print(f\"== Completed training group {name} ==\\n\")\n",
        "\n",
        "# Dummy backbone and ArcHead for example\n",
        "class DummyArcHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "    def forward(self, x, y):\n",
        "        return self.fc(x)\n",
        "\n",
        "backbone = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),\n",
        "    nn.Flatten()\n",
        ")\n",
        "arc_head = DummyArcHead()\n",
        "\n",
        "# Dummy datasets & subsets — replace with real ones\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.data = torch.randn(size, 3, 64, 64)\n",
        "        self.labels = torch.randint(0, 10, (size, 1))\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "class DummySubset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.data = DummyDataset(size)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "unique_races = ['asian']  # Example, replace with your real races\n",
        "\n",
        "# Dummy subset dicts for illustration\n",
        "gender_train_subsets = {p: DummySubset(80) for p in proportions}\n",
        "gender_test_subsets_f = {p: DummySubset(10) for p in proportions}\n",
        "gender_test_subsets_m = {p: DummySubset(10) for p in proportions}\n",
        "\n",
        "race_train_subsets = {('asian', p): DummySubset(80) for p in proportions}\n",
        "race_test_subsets_f = {p: DummySubset(10) for p in proportions}  # reuse gender test subsets for simplicity\n",
        "race_test_subsets_m = {p: DummySubset(10) for p in proportions}\n",
        "\n",
        "intersection_train_subsets = {('asian', p): DummySubset(80) for p in proportions}\n",
        "intersection_test_subsets_f = {('asian', p): DummySubset(10) for p in proportions}\n",
        "intersection_test_subsets_m = {('asian', p): DummySubset(10) for p in proportions}\n",
        "\n",
        "print(\"Starting training on gender partitions...\")\n",
        "train_and_eval_group(\"Gender\", gender_train_subsets, gender_test_subsets_f, gender_test_subsets_m,\n",
        "                     proportions, backbone, arc_head, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "print(\"Starting training on race partitions...\")\n",
        "for race in unique_races:\n",
        "    # Filter race test subsets dict for proper keys (p only)\n",
        "    test_f_filtered = {p: race_test_subsets_f[p] for p in proportions}\n",
        "    test_m_filtered = {p: race_test_subsets_m[p] for p in proportions}\n",
        "    train_filt = {p: race_train_subsets[(race, p)] for p in proportions}\n",
        "    train_and_eval_group(f\"Race-{race}\", train_filt, test_f_filtered, test_m_filtered,\n",
        "                         proportions, backbone, arc_head, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "print(\"Starting training on intersectional partitions...\")\n",
        "for race in unique_races:\n",
        "    test_f_filtered = {p: intersection_test_subsets_f[(race, p)] for p in proportions}\n",
        "    test_m_filtered = {p: intersection_test_subsets_m[(race, p)] for p in proportions}\n",
        "    train_filt = {p: intersection_train_subsets[(race, p)] for p in proportions}\n",
        "    train_and_eval_group(f\"Intersection-{race}\", train_filt, test_f_filtered, test_m_filtered,\n",
        "                         proportions, backbone, arc_head, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "print(\"All training runs complete.\")"
      ],
      "metadata": {
        "id": "Bqjbt__q4rDY",
        "outputId": "8b9d035b-713f-4be3-aeb9-fd9a474e7d37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on gender partitions...\n",
            "\n",
            "== Group: Gender - Training with proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.2887, Train Acc=16.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 2: Train Loss=2.2871, Train Acc=16.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 3: Train Loss=2.2861, Train Acc=16.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 4: Train Loss=2.2847, Train Acc=16.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 5: Train Loss=2.2843, Train Acc=16.25%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "\n",
            "== Group: Gender - Training with proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3076, Train Acc=10.00%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "New best avg val acc at epoch 1: 5.00%\n",
            "Epoch 2: Train Loss=2.3041, Train Acc=10.00%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "Epoch 3: Train Loss=2.3010, Train Acc=10.00%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "Epoch 4: Train Loss=2.2989, Train Acc=10.00%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "Epoch 5: Train Loss=2.2961, Train Acc=10.00%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "\n",
            "== Group: Gender - Training with proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3289, Train Acc=8.75%\n",
            "Val Acc Female: 0.00%, Male: 10.00%, Avg: 5.00%\n",
            "New best avg val acc at epoch 1: 5.00%\n",
            "Epoch 2: Train Loss=2.3248, Train Acc=8.75%\n",
            "Val Acc Female: 0.00%, Male: 10.00%, Avg: 5.00%\n",
            "Epoch 3: Train Loss=2.3210, Train Acc=8.75%\n",
            "Val Acc Female: 0.00%, Male: 10.00%, Avg: 5.00%\n",
            "Epoch 4: Train Loss=2.3177, Train Acc=8.75%\n",
            "Val Acc Female: 0.00%, Male: 10.00%, Avg: 5.00%\n",
            "Epoch 5: Train Loss=2.3143, Train Acc=8.75%\n",
            "Val Acc Female: 0.00%, Male: 10.00%, Avg: 5.00%\n",
            "== Completed training group Gender ==\n",
            "\n",
            "Starting training on race partitions...\n",
            "\n",
            "== Group: Race-asian - Training with proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.3162, Train Acc=12.50%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "New best avg val acc at epoch 1: 5.00%\n",
            "Epoch 2: Train Loss=2.3139, Train Acc=12.50%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "Epoch 3: Train Loss=2.3120, Train Acc=12.50%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "Epoch 4: Train Loss=2.3103, Train Acc=12.50%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "Epoch 5: Train Loss=2.3084, Train Acc=12.50%\n",
            "Val Acc Female: 10.00%, Male: 0.00%, Avg: 5.00%\n",
            "\n",
            "== Group: Race-asian - Training with proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3042, Train Acc=12.50%\n",
            "Val Acc Female: 20.00%, Male: 0.00%, Avg: 10.00%\n",
            "New best avg val acc at epoch 1: 10.00%\n",
            "Epoch 2: Train Loss=2.3013, Train Acc=12.50%\n",
            "Val Acc Female: 20.00%, Male: 0.00%, Avg: 10.00%\n",
            "Epoch 3: Train Loss=2.2991, Train Acc=12.50%\n",
            "Val Acc Female: 20.00%, Male: 0.00%, Avg: 10.00%\n",
            "Epoch 4: Train Loss=2.2969, Train Acc=12.50%\n",
            "Val Acc Female: 20.00%, Male: 0.00%, Avg: 10.00%\n",
            "Epoch 5: Train Loss=2.2946, Train Acc=12.50%\n",
            "Val Acc Female: 20.00%, Male: 0.00%, Avg: 10.00%\n",
            "\n",
            "== Group: Race-asian - Training with proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3079, Train Acc=12.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 2: Train Loss=2.3051, Train Acc=12.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 3: Train Loss=2.3030, Train Acc=12.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 4: Train Loss=2.3009, Train Acc=12.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "Epoch 5: Train Loss=2.2994, Train Acc=12.50%\n",
            "Val Acc Female: 0.00%, Male: 0.00%, Avg: 0.00%\n",
            "== Completed training group Race-asian ==\n",
            "\n",
            "Starting training on intersectional partitions...\n",
            "\n",
            "== Group: Intersection-asian - Training with proportion 0.25 ==\n",
            "Epoch 1: Train Loss=2.3021, Train Acc=13.75%\n",
            "Val Acc Female: 20.00%, Male: 10.00%, Avg: 15.00%\n",
            "New best avg val acc at epoch 1: 15.00%\n",
            "Epoch 2: Train Loss=2.2986, Train Acc=13.75%\n",
            "Val Acc Female: 20.00%, Male: 10.00%, Avg: 15.00%\n",
            "Epoch 3: Train Loss=2.2955, Train Acc=13.75%\n",
            "Val Acc Female: 20.00%, Male: 10.00%, Avg: 15.00%\n",
            "Epoch 4: Train Loss=2.2930, Train Acc=13.75%\n",
            "Val Acc Female: 20.00%, Male: 10.00%, Avg: 15.00%\n",
            "Epoch 5: Train Loss=2.2904, Train Acc=13.75%\n",
            "Val Acc Female: 20.00%, Male: 10.00%, Avg: 15.00%\n",
            "\n",
            "== Group: Intersection-asian - Training with proportion 0.5 ==\n",
            "Epoch 1: Train Loss=2.3124, Train Acc=7.50%\n",
            "Val Acc Female: 10.00%, Male: 20.00%, Avg: 15.00%\n",
            "New best avg val acc at epoch 1: 15.00%\n",
            "Epoch 2: Train Loss=2.3098, Train Acc=7.50%\n",
            "Val Acc Female: 10.00%, Male: 20.00%, Avg: 15.00%\n",
            "Epoch 3: Train Loss=2.3076, Train Acc=7.50%\n",
            "Val Acc Female: 10.00%, Male: 20.00%, Avg: 15.00%\n",
            "Epoch 4: Train Loss=2.3057, Train Acc=7.50%\n",
            "Val Acc Female: 10.00%, Male: 20.00%, Avg: 15.00%\n",
            "Epoch 5: Train Loss=2.3035, Train Acc=7.50%\n",
            "Val Acc Female: 10.00%, Male: 20.00%, Avg: 15.00%\n",
            "\n",
            "== Group: Intersection-asian - Training with proportion 0.75 ==\n",
            "Epoch 1: Train Loss=2.3285, Train Acc=10.00%\n",
            "Val Acc Female: 30.00%, Male: 0.00%, Avg: 15.00%\n",
            "New best avg val acc at epoch 1: 15.00%\n",
            "Epoch 2: Train Loss=2.3259, Train Acc=10.00%\n",
            "Val Acc Female: 30.00%, Male: 0.00%, Avg: 15.00%\n",
            "Epoch 3: Train Loss=2.3238, Train Acc=10.00%\n",
            "Val Acc Female: 30.00%, Male: 0.00%, Avg: 15.00%\n",
            "Epoch 4: Train Loss=2.3222, Train Acc=10.00%\n",
            "Val Acc Female: 30.00%, Male: 0.00%, Avg: 15.00%\n",
            "Epoch 5: Train Loss=2.3201, Train Acc=10.00%\n",
            "Val Acc Female: 30.00%, Male: 0.00%, Avg: 15.00%\n",
            "== Completed training group Intersection-asian ==\n",
            "\n",
            "All training runs complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8TM7bpCk3kNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title shell pipeline for unzipping! this needs to run every time\n",
        "\n",
        "!unzip -q \"/content/drive/My Drive/Datasets/celeba/img_align_celeba.zip\" -d \"/content/celeba/\""
      ],
      "metadata": {
        "id": "SS8uNZYunxqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content' # setting it to the local environment"
      ],
      "metadata": {
        "id": "4uA1b3RswNS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "cn679gCpx6pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a transform that is smaller per suggestion of rasmus\n",
        "\n",
        "image_size = 64\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                          std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "oLYdzNAHyveC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfering files from gdrive to here so that they would work without us uploading manually all the time\n",
        "# import module\n",
        "import shutil\n",
        "\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/identity_CelebA.txt', '/content/celeba/identity_CelebA.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_attr_celeba.txt', '/content/celeba/list_attr_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_bbox_celeba.txt', '/content/celeba/list_bbox_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_landmarks_align_celeba.txt', '/content/celeba/list_landmarks_align_celeba.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/Datasets/celeba/list_eval_partition.txt', '/content/celeba/list_eval_partition.txt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6CEeXlqPztTx",
        "outputId": "66df1422-b442-4df6-905c-fb7838d1afb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/celeba/list_eval_partition.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CelebA\n",
        "\n",
        "\n",
        "# it creates a folder on the go!\n",
        "\n",
        "try:\n",
        "    dataset = CelebA(\n",
        "        root='/content',\n",
        "        split='train',\n",
        "        target_type='attr',\n",
        "        transform=transform,\n",
        "        download=False # this works now!!!! its just important that it is in the root folder\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"CelebA error:\", e)"
      ],
      "metadata": {
        "id": "iyQji5uMx2yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = '/content/celeba'\n",
        "\n",
        "print(\"Root contents:\", os.listdir(data_dir))\n",
        "print(\"Images folder exists:\", os.path.isdir(os.path.join(data_dir, 'img_align_celeba')))\n",
        "print(\"Sample images:\", os.listdir(os.path.join(data_dir, 'img_align_celeba'))[:3])\n",
        "print(\"Has attribute file:\", os.path.isfile(os.path.join(data_dir, 'list_attr_celeba.txt')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jz1k1xJybHu",
        "outputId": "82549f4c-0c60-48db-b5a1-1f89825a4510",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root contents: ['identity_CelebA.txt', 'list_bbox_celeba.txt', 'list_attr_celeba.txt', 'list_eval_partition.txt', 'img_align_celeba', 'list_landmarks_align_celeba.txt']\n",
            "Images folder exists: True\n",
            "Sample images: ['085474.jpg', '129511.jpg', '100524.jpg']\n",
            "Has attribute file: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check 2 & the moment of truth!!\n",
        "\n",
        "# adding a dataloader and a basic model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "tw8aIHtsyj0Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting the training data, different distributions\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "test_dataset = CelebA(\n",
        "    root='/content',\n",
        "    split='test',\n",
        "    target_type='attr',\n",
        "    transform=transform,\n",
        "    download=False # i set it to true in case there is some secret metadata?? it is looking for\n",
        ")\n"
      ],
      "metadata": {
        "id": "ABpA6NKeQYRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Get the identity information from the training dataset\n",
        "identity_labels = dataset.identity\n",
        "# Convert to a pandas Series for easier counting\n",
        "identity_series = pd.Series(identity_labels.squeeze().numpy())\n",
        "identity_counts = identity_series.value_counts()\n",
        "top_1000_identities = identity_counts.nlargest(1000)\n",
        "# Get the indices corresponding to the top 1000 identities\n",
        "top_1000_indices = identity_series[identity_series.isin(top_1000_identities.index)].index\n",
        "# Create a subset of the dataset containing only the top 1000 identities\n",
        "dataset_top_1000 = Subset(dataset, top_1000_indices)\n",
        "\n",
        "\n",
        "min_samples = top_1000_identities.min()\n",
        "max_samples = top_1000_identities.max()\n",
        "\n",
        "print(f\"Minimum samples per identity: {min_samples}\")\n",
        "print(f\"Maximum samples per identity: {max_samples}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIYrBM44Fd_0",
        "outputId": "c866a686-325f-4b70-934c-a3dce3f2ae77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum samples per identity: 30\n",
            "Maximum samples per identity: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Make sure numpy is imported if it hasn't been already\n",
        "\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "gender_labels_test_subset = []\n",
        "for i in top_1000_indices:\n",
        "  # Note: As discussed before, using training indices on the test dataset\n",
        "  # might lead to issues or misalignment. Assuming this is intended for now.\n",
        "  if i < len(test_dataset):\n",
        "    gender_labels_test_subset.append(test_dataset.attr[i, male_idx])\n",
        "\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "gender_labels_test_subset_np = np.array(gender_labels_test_subset)\n",
        "\n",
        "\n",
        "# Now use np.where on the NumPy array\n",
        "# This is the part that fixes the DeprecationWarning\n",
        "female_test_subset_indices = np.where(gender_labels_test_subset_np == 0)[0]\n",
        "male_test_subset_indices   = np.where(gender_labels_test_subset_np ==  1)[0]\n",
        "\n",
        "\n",
        "print(len(female_test_subset_indices))\n",
        "print(len(male_test_subset_indices))\n",
        "\n",
        "\n",
        "N_test = min(len(female_test_subset_indices), len(male_test_subset_indices))\n",
        "\n",
        "rng_test = np.random.default_rng(seed=42)\n",
        "shuffled_female_test_subset_indices = np.copy(female_test_subset_indices)\n",
        "shuffled_male_test_subset_indices   = np.copy(male_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_female_test_subset_indices)\n",
        "rng_test.shuffle(shuffled_male_test_subset_indices)\n",
        "\n",
        "\n",
        "test_subsets = {}\n",
        "\n",
        "# Create training subsets\n",
        "test_subsets_f = {}\n",
        "test_subsets_m = {}\n",
        "# even split for all examples. we can change this later but we want to be able to generalize... we want there to be the same number of examples for men and women and for these to be in the same set...\n",
        "# we will put this to the loop.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkTrbz2Az2Zy",
        "outputId": "35b89be6-3ea4-46a5-e252-c75fc19616ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300\n",
            "1510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "# choose smallest n\n",
        "# proportions = [0, 0.1, 0.25, 0.5, 0.75, 1.0] # changed this bc it doesn't make sense\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "male_idx = test_dataset.attr_names.index('Male')\n",
        "\n",
        "# You need to create subsets from the test_dataset using test_dataset-specific indices\n",
        "# The previous code was creating subsets of the training dataset.\n",
        "# It seems like you want to create training subsets with varying gender proportions\n",
        "# and test subsets for evaluation (separated by gender).\n",
        "\n",
        "# For the training subsets (assuming you still want to use indices from the training dataset,\n",
        "# but with the identity filtering from before):\n",
        "# You will need to re-calculate the gender labels for the *training* dataset based on top_1000_indices.\n",
        "male_idx_train = dataset.attr_names.index('Male')\n",
        "gender_labels_train_subset = dataset.attr[top_1000_indices, male_idx_train] # Use gender from training dataset\n",
        "female_train_subset_indices = np.where(gender_labels_train_subset == 0)[0]\n",
        "male_train_subset_indices   = np.where(gender_labels_train_subset ==  1)[0]\n",
        "\n",
        "N_train = min(len(female_train_subset_indices), len(male_train_subset_indices))\n",
        "\n",
        "rng_train = np.random.default_rng(seed=42)\n",
        "shuffled_female_train_subset_indices = np.copy(female_train_subset_indices)\n",
        "shuffled_male_train_subset_indices   = np.copy(male_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_female_train_subset_indices)\n",
        "rng_train.shuffle(shuffled_male_train_subset_indices)\n",
        "\n",
        "\n",
        "# Create training subsets\n",
        "train_subsets = {}\n",
        "for p in proportions:\n",
        "    num_females_train = int(N_train * p)\n",
        "    num_males_train = N_train - num_females_train\n",
        "\n",
        "    q = min(p, 1-p)\n",
        "    num_females_test = int(N_test * q) # even split for testing\n",
        "    num_males_test = num_females_test\n",
        "\n",
        "    chosen_female_train = shuffled_female_train_subset_indices[:num_females_train] if num_females_train > 0 else np.array([], dtype=int)\n",
        "    chosen_male_train   = shuffled_male_train_subset_indices[:num_males_train]   if num_males_train > 0   else np.array([], dtype=int)\n",
        "\n",
        "    chosen_female_test = shuffled_female_test_subset_indices[:num_females_test]\n",
        "    chosen_male_test   = shuffled_male_test_subset_indices[:num_males_test]\n",
        "\n",
        "    # These indices are relative to the 'dataset_top_1000' subset,\n",
        "    # so you need to map them back to the original 'dataset' indices if Subset requires it.\n",
        "    # Since top_1000_indices is the mapping, we can directly use that:\n",
        "    original_indices_train = np.concatenate([\n",
        "        top_1000_indices[chosen_female_train],\n",
        "        top_1000_indices[chosen_male_train]\n",
        "    ]).astype(int)\n",
        "    rng_train.shuffle(original_indices_train)\n",
        "    train_subsets[p] = Subset(dataset, original_indices_train)\n",
        "    test_subsets_f[p] = Subset(test_dataset, chosen_female_test)\n",
        "    test_subsets_m[p] = Subset(test_dataset, chosen_male_test)\n",
        "\n",
        "\n",
        "\n",
        "# Verification as before\n",
        "for p in proportions:\n",
        "    # Verification for the training subset\n",
        "    indices_train = train_subsets[p].indices\n",
        "    # Need to get genders for these original training indices from the *full* training dataset\n",
        "    genders_train = dataset.attr[indices_train, male_idx_train]\n",
        "    percent_female_train = (genders_train == 0).sum()/len(indices_train) if len(indices_train) > 0 else 0\n",
        "    print(f\"Train Subset (Prop {int(p*100)}%): Target {int(p*100)}% -- Actual {percent_female_train*100:.2f}% females, {(genders_train == 0).sum()} samples\")\n",
        "\n",
        "\n",
        "    number_female_test = len(test_subsets_f[p].indices)\n",
        "    number_male_test = len(test_subsets_m[p].indices)\n",
        "    print(f\"Number of female test samples: {number_female_test}\")\n",
        "    print(f\"Number of male test samples: {number_male_test}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2x4LIJezqg1",
        "outputId": "057d03ad-080d-4408-ecd0-5e88541ea02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Subset (Prop 25%): Target 25% -- Actual 24.99% females, 2480 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n",
            "Train Subset (Prop 50%): Target 50% -- Actual 50.00% females, 4961 samples\n",
            "Number of female test samples: 755\n",
            "Number of male test samples: 755\n",
            "Train Subset (Prop 75%): Target 75% -- Actual 74.99% females, 7441 samples\n",
            "Number of female test samples: 377\n",
            "Number of male test samples: 377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_subsets[0.5], batch_size=batch_size, shuffle=True)\n",
        "# val_loader = DataLoader(test_subsets[0.5], batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "V4HgisJbFAtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "goy8JCnuN5y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#title this is a more complicated model but used more commonly in FR\n",
        "\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.5, easy_margin=False):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = torch.cos(torch.tensor(self.m))\n",
        "        self.sin_m = torch.sin(torch.tensor(self.m))\n",
        "        self.th = torch.cos(torch.tensor(3.14159265 - self.m))\n",
        "        self.mm = torch.sin(torch.tensor(3.14159265 - self.m)) * self.m\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - torch.clamp(cosine ** 2, 0, 1))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        one_hot = torch.zeros_like(cosine)\n",
        "        one_hot.scatter_(1, label.view(-1, 1), 1)\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "_wFThr7cNaP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, arc_head, dataloader, device):\n",
        "    model.eval()\n",
        "    arc_head.eval()\n",
        "    total, correct, running_loss = 0, 0, 0.0\n",
        "    for i, (images, identity_labels) in enumerate(dataloader):\n",
        "        images, labels = images.to(device), identity_labels[:,0].to(device) # Selecting the first column of identity_labels\n",
        "        features = model(images)\n",
        "        logits = arc_head(features, labels) # Using the modified labels for arc_head\n",
        "        loss = criterion(logits, labels)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += images.size(0)\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    print(f\"Test set: loss={avg_loss:.4f}, accuracy={acc*100:.2f}, data loader{dataloader}%\")\n",
        "    return avg_loss, acc\n",
        "\n"
      ],
      "metadata": {
        "id": "EARq75acrB5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "backbone = models.resnet18(weights=None)\n",
        "feature_dim = backbone.fc.in_features\n",
        "backbone.fc = nn.Identity()\n",
        "\n",
        "n_classes=(dataset.identity.unique())\n",
        "print(n_classes)\n",
        "# arc_head = ArcMarginProduct(feature_dim, out_features=n_classes).to(device)\n",
        "arc_head = ArcMarginProduct(feature_dim, 10177).to(device) # Tracy update\n",
        "\n"
      ],
      "metadata": {
        "id": "Vr67fLnNNXWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fde6e0-2fa9-4912-f5c5-bbc8d43f4c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    1,     2,     3,  ..., 10175, 10176, 10177])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sanity check for the eval code\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "\n",
        "\n",
        "\n",
        "# for epoch in range(1):\n",
        "#   avg_loss, acc = evaluate(model, arc_head, val_loader, device)"
      ],
      "metadata": {
        "id": "_oK13oicDSxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "best_acc = 0.0\n",
        "\n",
        "\n",
        "for proportion in proportions:\n",
        "    model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "    best_acc = 0.0\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize val_loader_f and val_loader_m to None\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    # Check if the subsets have any samples before creating DataLoaders\n",
        "    if len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=True)\n",
        "    if len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        arc_head.train()\n",
        "        total, correct, running_loss = 0, 0, 0.0\n",
        "        for i, (images, identity_labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), identity_labels[:,0].to(device) # Selecting the first column of identity_labels\n",
        "            features = model(images)\n",
        "            logits = arc_head(features, labels) # Using the modified labels for arc_head\n",
        "            loss = criterion(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += images.size(0)\n",
        "            if (i+1) % 50 == 0: print(f\"Batch {i+1}/{len(train_loader)} - Loss {loss.item():.4f}\")\n",
        "        print(f\"Epoch {epoch+1}: Loss={running_loss/total:.4f}  Accuracy={correct/total*100:.2f}%\")\n",
        "\n",
        "        # Check if the validation loaders are not empty before evaluating\n",
        "\n",
        "        if val_loader_f:\n",
        "            val_loss_f, val_acc_f = evaluate(model, arc_head, val_loader_f, device)\n",
        "        if val_loader_m:\n",
        "            val_loss_m, val_acc_m = evaluate(model, arc_head, val_loader_m, device)\n",
        "\n",
        "        if val_loader_m is not None and val_loader_f is not None:\n",
        "          val_acc = (val_acc_f + val_acc_m) / 2\n",
        "        elif val_loader_m:\n",
        "          val_acc = val_acc_m\n",
        "        elif val_loader_f:\n",
        "          val_acc = val_acc_f\n",
        "        else:\n",
        "          val_acc = 0.0\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "          best_acc = val_acc\n",
        "          torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'arc_head_state_dict': arc_head.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'val_acc': val_acc,\n",
        "          }, f'model{proportion}_checkpoint.pth')\n",
        "\n",
        "          if val_loader_m and val_loader_f:\n",
        "            print(f\"New best model saved at epoch {epoch+1} with average acc {val_acc_f*100:.2f}, female acc {val_acc_f*100:.2f}, male acc {val_acc_m*100:.2f}%\")\n",
        "          elif val_loader_m:\n",
        "            print(f\"New best model saved at epoch {epoch+1} with male acc {val_acc_m*100:.2f}%\")\n",
        "          elif val_loader_f:\n",
        "            print(f\"New best model saved at epoch {epoch+1} with female acc {val_acc_f*100:.2f}%\")"
      ],
      "metadata": {
        "id": "MvwwRTTpMt5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715662f3-fb15-469d-a1ff-831cb3dfcc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 50/156 - Loss 3.8559\n",
            "Batch 100/156 - Loss 2.8617\n",
            "Batch 150/156 - Loss 3.2255\n",
            "Epoch 1: Loss=2.9338  Accuracy=71.79%\n",
            "Test set: loss=1.5582, accuracy=88.59%\n",
            "Test set: loss=1.5568, accuracy=89.12%\n",
            "New best model saved at epoch 1 with average acc 88.59, female acc 88.59, male acc 89.12%\n",
            "Batch 50/156 - Loss 3.2611\n",
            "Batch 100/156 - Loss 3.1904\n",
            "Batch 150/156 - Loss 4.8022\n",
            "Epoch 2: Loss=2.9162  Accuracy=72.49%\n",
            "Test set: loss=1.6898, accuracy=86.47%\n",
            "Test set: loss=1.6631, accuracy=88.06%\n",
            "Batch 50/156 - Loss 2.4079\n",
            "Batch 100/156 - Loss 1.6515\n",
            "Batch 150/156 - Loss 2.6151\n",
            "Epoch 3: Loss=2.7828  Accuracy=75.84%\n",
            "Test set: loss=1.3345, accuracy=89.39%\n",
            "Test set: loss=1.4649, accuracy=89.66%\n",
            "New best model saved at epoch 3 with average acc 89.39, female acc 89.39, male acc 89.66%\n",
            "Batch 50/156 - Loss 3.0508\n",
            "Batch 100/156 - Loss 3.8725\n",
            "Batch 150/156 - Loss 3.3005\n",
            "Epoch 4: Loss=2.6610  Accuracy=77.30%\n",
            "Test set: loss=1.6540, accuracy=87.53%\n",
            "Test set: loss=1.6887, accuracy=87.53%\n",
            "Batch 50/156 - Loss 2.2793\n",
            "Batch 100/156 - Loss 2.3232\n",
            "Batch 150/156 - Loss 2.8048\n",
            "Epoch 5: Loss=2.5710  Accuracy=77.87%\n",
            "Test set: loss=1.9922, accuracy=85.15%\n",
            "Test set: loss=2.1485, accuracy=83.55%\n",
            "Batch 50/156 - Loss 2.2392\n",
            "Batch 100/156 - Loss 3.1714\n",
            "Batch 150/156 - Loss 3.3365\n",
            "Epoch 6: Loss=2.5334  Accuracy=78.95%\n",
            "Test set: loss=2.1802, accuracy=85.15%\n",
            "Test set: loss=1.9369, accuracy=84.08%\n",
            "Batch 50/156 - Loss 2.9705\n",
            "Batch 100/156 - Loss 2.3699\n",
            "Batch 150/156 - Loss 2.5338\n",
            "Epoch 7: Loss=2.4184  Accuracy=79.88%\n",
            "Test set: loss=1.9715, accuracy=84.62%\n",
            "Test set: loss=1.8327, accuracy=86.21%\n",
            "Batch 50/156 - Loss 2.6519\n",
            "Batch 100/156 - Loss 2.9730\n",
            "Batch 150/156 - Loss 3.0006\n",
            "Epoch 8: Loss=2.3980  Accuracy=79.70%\n",
            "Test set: loss=1.7038, accuracy=87.27%\n",
            "Test set: loss=1.5941, accuracy=87.80%\n",
            "Batch 50/156 - Loss 2.3581\n",
            "Batch 100/156 - Loss 1.6399\n",
            "Batch 150/156 - Loss 1.6215\n",
            "Epoch 9: Loss=2.3004  Accuracy=82.00%\n",
            "Test set: loss=1.7635, accuracy=88.86%\n",
            "Test set: loss=1.5062, accuracy=90.19%\n",
            "Batch 50/156 - Loss 3.0578\n",
            "Batch 100/156 - Loss 2.2911\n",
            "Batch 150/156 - Loss 2.1420\n",
            "Epoch 10: Loss=2.4469  Accuracy=79.66%\n",
            "Test set: loss=1.8454, accuracy=86.47%\n",
            "Test set: loss=1.7646, accuracy=86.47%\n",
            "Batch 50/156 - Loss 2.2696\n",
            "Batch 100/156 - Loss 2.7688\n",
            "Batch 150/156 - Loss 2.4821\n",
            "Epoch 11: Loss=2.2333  Accuracy=82.14%\n",
            "Test set: loss=1.8538, accuracy=88.06%\n",
            "Test set: loss=1.7581, accuracy=88.59%\n",
            "Batch 50/156 - Loss 2.7043\n",
            "Batch 100/156 - Loss 2.0212\n",
            "Batch 150/156 - Loss 3.0883\n",
            "Epoch 12: Loss=2.3737  Accuracy=80.06%\n",
            "Test set: loss=1.6973, accuracy=84.35%\n",
            "Test set: loss=1.7874, accuracy=84.62%\n",
            "Batch 50/156 - Loss 3.6117\n",
            "Batch 100/156 - Loss 2.7268\n",
            "Batch 150/156 - Loss 1.7609\n",
            "Epoch 13: Loss=2.2245  Accuracy=82.72%\n",
            "Test set: loss=1.5689, accuracy=89.39%\n",
            "Test set: loss=1.4835, accuracy=90.45%\n",
            "New best model saved at epoch 13 with average acc 89.39, female acc 89.39, male acc 90.45%\n",
            "Batch 50/156 - Loss 2.3794\n",
            "Batch 100/156 - Loss 2.7526\n",
            "Batch 150/156 - Loss 1.7128\n",
            "Epoch 14: Loss=2.2582  Accuracy=81.36%\n",
            "Test set: loss=1.9449, accuracy=85.94%\n",
            "Test set: loss=1.8919, accuracy=87.80%\n",
            "Batch 50/156 - Loss 1.8017\n",
            "Batch 100/156 - Loss 1.0283\n",
            "Batch 150/156 - Loss 1.1855\n",
            "Epoch 15: Loss=2.0215  Accuracy=83.92%\n",
            "Test set: loss=1.9797, accuracy=84.62%\n",
            "Test set: loss=1.7920, accuracy=87.27%\n",
            "Batch 50/156 - Loss 1.3447\n",
            "Batch 100/156 - Loss 1.1883\n",
            "Batch 150/156 - Loss 2.2036\n",
            "Epoch 16: Loss=1.9031  Accuracy=85.46%\n",
            "Test set: loss=1.6665, accuracy=88.33%\n",
            "Test set: loss=1.5151, accuracy=90.19%\n",
            "Batch 50/156 - Loss 1.2374\n",
            "Batch 100/156 - Loss 2.0339\n",
            "Batch 150/156 - Loss 1.7311\n",
            "Epoch 17: Loss=1.8966  Accuracy=85.79%\n",
            "Test set: loss=1.4030, accuracy=90.19%\n",
            "Test set: loss=1.4672, accuracy=90.72%\n",
            "New best model saved at epoch 17 with average acc 90.19, female acc 90.19, male acc 90.72%\n",
            "Batch 50/156 - Loss 1.4565\n",
            "Batch 100/156 - Loss 2.0115\n",
            "Batch 150/156 - Loss 3.7686\n",
            "Epoch 18: Loss=1.9369  Accuracy=85.80%\n",
            "Test set: loss=1.6738, accuracy=86.47%\n",
            "Test set: loss=1.7592, accuracy=87.80%\n",
            "Batch 50/156 - Loss 2.8077\n",
            "Batch 100/156 - Loss 1.9869\n",
            "Batch 150/156 - Loss 2.3501\n",
            "Epoch 19: Loss=1.9898  Accuracy=84.73%\n",
            "Test set: loss=1.8698, accuracy=82.23%\n",
            "Test set: loss=1.4661, accuracy=85.94%\n",
            "Batch 50/156 - Loss 2.1550\n",
            "Batch 100/156 - Loss 2.8228\n",
            "Batch 150/156 - Loss 1.9950\n",
            "Epoch 20: Loss=1.9201  Accuracy=84.71%\n",
            "Test set: loss=2.0295, accuracy=87.27%\n",
            "Test set: loss=1.7257, accuracy=88.59%\n",
            "Batch 50/156 - Loss 0.6522\n",
            "Batch 100/156 - Loss 2.0989\n",
            "Batch 150/156 - Loss 0.9018\n",
            "Epoch 21: Loss=1.7672  Accuracy=87.20%\n",
            "Test set: loss=1.8625, accuracy=88.06%\n",
            "Test set: loss=1.7978, accuracy=88.59%\n",
            "Batch 50/156 - Loss 1.7873\n",
            "Batch 100/156 - Loss 2.3635\n",
            "Batch 150/156 - Loss 1.7340\n",
            "Epoch 22: Loss=1.8387  Accuracy=86.75%\n",
            "Test set: loss=1.7869, accuracy=87.53%\n",
            "Test set: loss=1.6383, accuracy=88.86%\n",
            "Batch 50/156 - Loss 2.1695\n",
            "Batch 100/156 - Loss 1.4251\n",
            "Batch 150/156 - Loss 2.1413\n",
            "Epoch 23: Loss=1.7943  Accuracy=86.90%\n",
            "Test set: loss=1.7159, accuracy=87.53%\n",
            "Test set: loss=1.6400, accuracy=89.39%\n",
            "Batch 50/156 - Loss 1.8163\n",
            "Batch 100/156 - Loss 1.2724\n",
            "Batch 150/156 - Loss 0.9733\n",
            "Epoch 24: Loss=1.6398  Accuracy=87.62%\n",
            "Test set: loss=1.7461, accuracy=87.27%\n",
            "Test set: loss=1.7348, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.9311\n",
            "Batch 100/156 - Loss 2.5431\n",
            "Batch 150/156 - Loss 1.9614\n",
            "Epoch 25: Loss=1.6295  Accuracy=88.57%\n",
            "Test set: loss=2.2562, accuracy=83.29%\n",
            "Test set: loss=1.7259, accuracy=87.00%\n",
            "Batch 50/156 - Loss 1.8519\n",
            "Batch 100/156 - Loss 2.0782\n",
            "Batch 150/156 - Loss 1.3863\n",
            "Epoch 26: Loss=1.6556  Accuracy=87.93%\n",
            "Test set: loss=2.0563, accuracy=87.53%\n",
            "Test set: loss=1.5868, accuracy=89.39%\n",
            "Batch 50/156 - Loss 1.1575\n",
            "Batch 100/156 - Loss 1.4725\n",
            "Batch 150/156 - Loss 2.0535\n",
            "Epoch 27: Loss=1.6778  Accuracy=87.68%\n",
            "Test set: loss=1.8798, accuracy=87.00%\n",
            "Test set: loss=1.6534, accuracy=88.86%\n",
            "Batch 50/156 - Loss 2.3757\n",
            "Batch 100/156 - Loss 2.0114\n",
            "Batch 150/156 - Loss 1.6852\n",
            "Epoch 28: Loss=1.6253  Accuracy=87.86%\n",
            "Test set: loss=1.9009, accuracy=88.33%\n",
            "Test set: loss=1.2956, accuracy=92.04%\n",
            "Batch 50/156 - Loss 1.0992\n",
            "Batch 100/156 - Loss 0.9941\n",
            "Batch 150/156 - Loss 1.4703\n",
            "Epoch 29: Loss=1.5822  Accuracy=88.41%\n",
            "Test set: loss=1.9132, accuracy=86.74%\n",
            "Test set: loss=1.6010, accuracy=88.59%\n",
            "Batch 50/156 - Loss 1.3508\n",
            "Batch 100/156 - Loss 0.4023\n",
            "Batch 150/156 - Loss 1.2520\n",
            "Epoch 30: Loss=1.4514  Accuracy=89.49%\n",
            "Test set: loss=1.8551, accuracy=88.06%\n",
            "Test set: loss=1.6077, accuracy=89.92%\n",
            "Batch 50/156 - Loss 1.3457\n",
            "Batch 100/156 - Loss 1.5573\n",
            "Batch 150/156 - Loss 1.5485\n",
            "Epoch 31: Loss=1.3953  Accuracy=90.53%\n",
            "Test set: loss=2.2866, accuracy=85.15%\n",
            "Test set: loss=1.7476, accuracy=88.06%\n",
            "Batch 50/156 - Loss 1.4258\n",
            "Batch 100/156 - Loss 1.2823\n",
            "Batch 150/156 - Loss 1.1726\n",
            "Epoch 32: Loss=1.3590  Accuracy=90.42%\n",
            "Test set: loss=1.7485, accuracy=87.53%\n",
            "Test set: loss=1.5958, accuracy=89.39%\n",
            "Batch 50/156 - Loss 1.2291\n",
            "Batch 100/156 - Loss 1.7426\n",
            "Batch 150/156 - Loss 1.2728\n",
            "Epoch 33: Loss=1.2971  Accuracy=90.58%\n",
            "Test set: loss=1.9192, accuracy=84.88%\n",
            "Test set: loss=1.7015, accuracy=85.68%\n",
            "Batch 50/156 - Loss 1.3935\n",
            "Batch 100/156 - Loss 1.0985\n",
            "Batch 150/156 - Loss 1.3844\n",
            "Epoch 34: Loss=1.3846  Accuracy=89.53%\n",
            "Test set: loss=2.1369, accuracy=86.47%\n",
            "Test set: loss=1.9147, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.3464\n",
            "Batch 100/156 - Loss 2.1463\n",
            "Batch 150/156 - Loss 1.0348\n",
            "Epoch 35: Loss=1.2120  Accuracy=91.17%\n",
            "Test set: loss=1.7826, accuracy=87.53%\n",
            "Test set: loss=1.5638, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.4277\n",
            "Batch 100/156 - Loss 2.0862\n",
            "Batch 150/156 - Loss 1.1250\n",
            "Epoch 36: Loss=1.0932  Accuracy=92.14%\n",
            "Test set: loss=2.0063, accuracy=86.74%\n",
            "Test set: loss=1.5004, accuracy=89.92%\n",
            "Batch 50/156 - Loss 1.1218\n",
            "Batch 100/156 - Loss 0.7460\n",
            "Batch 150/156 - Loss 1.2018\n",
            "Epoch 37: Loss=1.0764  Accuracy=92.03%\n",
            "Test set: loss=2.1965, accuracy=86.47%\n",
            "Test set: loss=1.8011, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.5673\n",
            "Batch 100/156 - Loss 1.3052\n",
            "Batch 150/156 - Loss 1.1549\n",
            "Epoch 38: Loss=1.0848  Accuracy=91.85%\n",
            "Test set: loss=2.3447, accuracy=85.68%\n",
            "Test set: loss=1.7338, accuracy=88.33%\n",
            "Batch 50/156 - Loss 1.7830\n",
            "Batch 100/156 - Loss 0.8696\n",
            "Batch 150/156 - Loss 1.4607\n",
            "Epoch 39: Loss=1.0793  Accuracy=91.64%\n",
            "Test set: loss=2.3368, accuracy=84.88%\n",
            "Test set: loss=1.7726, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.6110\n",
            "Batch 100/156 - Loss 1.2437\n",
            "Batch 150/156 - Loss 1.8866\n",
            "Epoch 40: Loss=1.0538  Accuracy=91.46%\n",
            "Test set: loss=2.1441, accuracy=84.35%\n",
            "Test set: loss=1.8838, accuracy=84.62%\n",
            "Batch 50/156 - Loss 1.4729\n",
            "Batch 100/156 - Loss 1.1698\n",
            "Batch 150/156 - Loss 0.6696\n",
            "Epoch 41: Loss=1.0200  Accuracy=91.82%\n",
            "Test set: loss=2.1485, accuracy=87.53%\n",
            "Test set: loss=1.6865, accuracy=89.12%\n",
            "Batch 50/156 - Loss 0.5616\n",
            "Batch 100/156 - Loss 0.8103\n",
            "Batch 150/156 - Loss 1.6546\n",
            "Epoch 42: Loss=0.9899  Accuracy=92.23%\n",
            "Test set: loss=1.8246, accuracy=88.59%\n",
            "Test set: loss=1.6849, accuracy=89.12%\n",
            "Batch 50/156 - Loss 1.1151\n",
            "Batch 100/156 - Loss 0.6701\n",
            "Batch 150/156 - Loss 1.3904\n",
            "Epoch 43: Loss=0.8814  Accuracy=93.17%\n",
            "Test set: loss=2.2548, accuracy=84.35%\n",
            "Test set: loss=1.5468, accuracy=88.86%\n",
            "Batch 50/156 - Loss 1.0742\n",
            "Batch 100/156 - Loss 0.8789\n",
            "Batch 150/156 - Loss 0.5621\n",
            "Epoch 44: Loss=0.9791  Accuracy=92.79%\n",
            "Test set: loss=2.1902, accuracy=85.68%\n",
            "Test set: loss=1.8446, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.1745\n",
            "Batch 100/156 - Loss 0.3578\n",
            "Batch 150/156 - Loss 0.4328\n",
            "Epoch 45: Loss=0.8762  Accuracy=93.54%\n",
            "Test set: loss=2.1406, accuracy=85.15%\n",
            "Test set: loss=1.9306, accuracy=87.27%\n",
            "Batch 50/156 - Loss 1.3497\n",
            "Batch 100/156 - Loss 0.9585\n",
            "Batch 150/156 - Loss 1.0204\n",
            "Epoch 46: Loss=0.8982  Accuracy=93.46%\n",
            "Test set: loss=1.9541, accuracy=87.27%\n",
            "Test set: loss=1.8944, accuracy=87.00%\n",
            "Batch 50/156 - Loss 0.0947\n",
            "Batch 100/156 - Loss 0.6634\n",
            "Batch 150/156 - Loss 0.2727\n",
            "Epoch 47: Loss=0.7800  Accuracy=94.34%\n",
            "Test set: loss=2.0977, accuracy=86.21%\n",
            "Test set: loss=2.1113, accuracy=86.47%\n",
            "Batch 50/156 - Loss 0.1317\n",
            "Batch 100/156 - Loss 0.5130\n",
            "Batch 150/156 - Loss 0.3223\n",
            "Epoch 48: Loss=0.7146  Accuracy=95.00%\n",
            "Test set: loss=2.7290, accuracy=83.55%\n",
            "Test set: loss=2.0480, accuracy=86.74%\n",
            "Batch 50/156 - Loss 0.7380\n",
            "Batch 100/156 - Loss 0.5863\n",
            "Batch 150/156 - Loss 1.1145\n",
            "Epoch 49: Loss=0.7676  Accuracy=94.54%\n",
            "Test set: loss=2.0321, accuracy=88.06%\n",
            "Test set: loss=1.7424, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.7167\n",
            "Batch 100/156 - Loss 0.6091\n",
            "Batch 150/156 - Loss 0.9314\n",
            "Epoch 50: Loss=0.6913  Accuracy=95.09%\n",
            "Test set: loss=2.3866, accuracy=86.21%\n",
            "Test set: loss=1.8849, accuracy=88.06%\n",
            "Batch 50/156 - Loss 1.1670\n",
            "Batch 100/156 - Loss 0.3536\n",
            "Batch 150/156 - Loss 0.9636\n",
            "Epoch 1: Loss=0.5764  Accuracy=95.89%\n",
            "Test set: loss=2.1380, accuracy=86.89%\n",
            "Test set: loss=2.0595, accuracy=87.68%\n",
            "New best model saved at epoch 1 with average acc 86.89, female acc 86.89, male acc 87.68%\n",
            "Batch 50/156 - Loss 0.5710\n",
            "Batch 100/156 - Loss 0.0199\n",
            "Batch 150/156 - Loss 0.6425\n",
            "Epoch 2: Loss=0.4924  Accuracy=96.47%\n",
            "Test set: loss=2.2454, accuracy=86.49%\n",
            "Test set: loss=1.9013, accuracy=88.34%\n",
            "New best model saved at epoch 2 with average acc 86.49, female acc 86.49, male acc 88.34%\n",
            "Batch 50/156 - Loss 0.3174\n",
            "Batch 100/156 - Loss 0.3233\n",
            "Batch 150/156 - Loss 0.5009\n",
            "Epoch 3: Loss=0.4560  Accuracy=96.83%\n",
            "Test set: loss=2.2595, accuracy=85.56%\n",
            "Test set: loss=1.9430, accuracy=86.36%\n",
            "Batch 50/156 - Loss 0.3757\n",
            "Batch 100/156 - Loss 0.6585\n",
            "Batch 150/156 - Loss 0.3352\n",
            "Epoch 4: Loss=0.5044  Accuracy=96.41%\n",
            "Test set: loss=2.5096, accuracy=84.64%\n",
            "Test set: loss=2.3027, accuracy=85.17%\n",
            "Batch 50/156 - Loss 0.3580\n",
            "Batch 100/156 - Loss 0.0389\n",
            "Batch 150/156 - Loss 0.6382\n",
            "Epoch 5: Loss=0.4792  Accuracy=96.57%\n",
            "Test set: loss=2.0307, accuracy=87.55%\n",
            "Test set: loss=1.7366, accuracy=89.54%\n",
            "New best model saved at epoch 5 with average acc 87.55, female acc 87.55, male acc 89.54%\n",
            "Batch 50/156 - Loss 0.3786\n",
            "Batch 100/156 - Loss 0.5571\n",
            "Batch 150/156 - Loss 1.1735\n",
            "Epoch 6: Loss=0.4052  Accuracy=97.18%\n",
            "Test set: loss=2.0490, accuracy=87.02%\n",
            "Test set: loss=1.7061, accuracy=88.34%\n",
            "Batch 50/156 - Loss 0.6419\n",
            "Batch 100/156 - Loss 0.0347\n",
            "Batch 150/156 - Loss 0.3214\n",
            "Epoch 7: Loss=0.4084  Accuracy=97.11%\n",
            "Test set: loss=2.3192, accuracy=87.02%\n",
            "Test set: loss=1.8950, accuracy=88.34%\n",
            "Batch 50/156 - Loss 0.3370\n",
            "Batch 100/156 - Loss 0.5378\n",
            "Batch 150/156 - Loss 0.3584\n",
            "Epoch 8: Loss=0.4211  Accuracy=96.90%\n",
            "Test set: loss=2.5854, accuracy=84.11%\n",
            "Test set: loss=2.0109, accuracy=86.75%\n",
            "Batch 50/156 - Loss 0.2825\n",
            "Batch 100/156 - Loss 0.6378\n",
            "Batch 150/156 - Loss 0.0107\n",
            "Epoch 9: Loss=0.4422  Accuracy=96.84%\n",
            "Test set: loss=2.1652, accuracy=87.55%\n",
            "Test set: loss=1.9907, accuracy=89.01%\n",
            "Batch 50/156 - Loss 0.3244\n",
            "Batch 100/156 - Loss 0.4865\n",
            "Batch 150/156 - Loss 0.5545\n",
            "Epoch 10: Loss=0.4166  Accuracy=97.07%\n",
            "Test set: loss=2.0513, accuracy=85.56%\n",
            "Test set: loss=1.7648, accuracy=87.15%\n",
            "Batch 50/156 - Loss 0.7088\n",
            "Batch 100/156 - Loss 0.0324\n",
            "Batch 150/156 - Loss 0.9173\n",
            "Epoch 11: Loss=0.3110  Accuracy=97.78%\n",
            "Test set: loss=2.1837, accuracy=87.95%\n",
            "Test set: loss=1.9462, accuracy=89.14%\n",
            "Batch 50/156 - Loss 0.7608\n",
            "Batch 100/156 - Loss 0.6118\n",
            "Batch 150/156 - Loss 0.0214\n",
            "Epoch 12: Loss=0.3516  Accuracy=97.52%\n",
            "Test set: loss=2.1148, accuracy=86.49%\n",
            "Test set: loss=1.6769, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.3072\n",
            "Batch 100/156 - Loss 0.3469\n",
            "Batch 150/156 - Loss 0.5696\n",
            "Epoch 13: Loss=0.3555  Accuracy=97.49%\n",
            "Test set: loss=2.3579, accuracy=85.83%\n",
            "Test set: loss=1.8972, accuracy=89.14%\n",
            "Batch 50/156 - Loss 0.0160\n",
            "Batch 100/156 - Loss 0.3213\n",
            "Batch 150/156 - Loss 0.9403\n",
            "Epoch 14: Loss=0.3761  Accuracy=97.34%\n",
            "Test set: loss=2.5326, accuracy=85.56%\n",
            "Test set: loss=1.9722, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.7373\n",
            "Batch 100/156 - Loss 0.3273\n",
            "Batch 150/156 - Loss 0.6268\n",
            "Epoch 15: Loss=0.4017  Accuracy=97.17%\n",
            "Test set: loss=2.2966, accuracy=86.09%\n",
            "Test set: loss=1.6667, accuracy=89.80%\n",
            "Batch 50/156 - Loss 0.6305\n",
            "Batch 100/156 - Loss 0.1694\n",
            "Batch 150/156 - Loss 0.0173\n",
            "Epoch 16: Loss=0.3372  Accuracy=97.69%\n",
            "Test set: loss=2.2362, accuracy=87.55%\n",
            "Test set: loss=1.8808, accuracy=88.74%\n",
            "Batch 50/156 - Loss 0.3352\n",
            "Batch 100/156 - Loss 0.0160\n",
            "Batch 150/156 - Loss 0.0205\n",
            "Epoch 17: Loss=0.3206  Accuracy=97.77%\n",
            "Test set: loss=2.2832, accuracy=86.75%\n",
            "Test set: loss=1.8856, accuracy=89.01%\n",
            "Batch 50/156 - Loss 0.3256\n",
            "Batch 100/156 - Loss 0.0622\n",
            "Batch 150/156 - Loss 0.5052\n",
            "Epoch 18: Loss=0.2950  Accuracy=97.97%\n",
            "Test set: loss=2.0178, accuracy=87.68%\n",
            "Test set: loss=1.7140, accuracy=90.20%\n",
            "New best model saved at epoch 18 with average acc 87.68, female acc 87.68, male acc 90.20%\n",
            "Batch 50/156 - Loss 0.7507\n",
            "Batch 100/156 - Loss 0.0188\n",
            "Batch 150/156 - Loss 0.0388\n",
            "Epoch 19: Loss=0.3132  Accuracy=97.86%\n",
            "Test set: loss=1.9139, accuracy=89.01%\n",
            "Test set: loss=1.7270, accuracy=90.33%\n",
            "New best model saved at epoch 19 with average acc 89.01, female acc 89.01, male acc 90.33%\n",
            "Batch 50/156 - Loss 0.0218\n",
            "Batch 100/156 - Loss 0.0350\n",
            "Batch 150/156 - Loss 0.0080\n",
            "Epoch 20: Loss=0.2757  Accuracy=98.16%\n",
            "Test set: loss=2.2744, accuracy=88.08%\n",
            "Test set: loss=1.9601, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.3537\n",
            "Batch 100/156 - Loss 0.3196\n",
            "Batch 150/156 - Loss 0.4466\n",
            "Epoch 21: Loss=0.3326  Accuracy=97.67%\n",
            "Test set: loss=1.7783, accuracy=88.87%\n",
            "Test set: loss=1.8894, accuracy=87.81%\n",
            "Batch 50/156 - Loss 1.1862\n",
            "Batch 100/156 - Loss 0.8764\n",
            "Batch 150/156 - Loss 0.7925\n",
            "Epoch 22: Loss=0.2951  Accuracy=97.91%\n",
            "Test set: loss=2.5704, accuracy=85.56%\n",
            "Test set: loss=1.9449, accuracy=88.48%\n",
            "Batch 50/156 - Loss 0.3622\n",
            "Batch 100/156 - Loss 0.0181\n",
            "Batch 150/156 - Loss 0.0950\n",
            "Epoch 23: Loss=0.2805  Accuracy=98.19%\n",
            "Test set: loss=2.2572, accuracy=85.96%\n",
            "Test set: loss=1.6663, accuracy=89.80%\n",
            "Batch 50/156 - Loss 0.1455\n",
            "Batch 100/156 - Loss 0.3059\n",
            "Batch 150/156 - Loss 1.2332\n",
            "Epoch 24: Loss=0.2617  Accuracy=98.30%\n",
            "Test set: loss=2.0774, accuracy=87.95%\n",
            "Test set: loss=1.7428, accuracy=89.93%\n",
            "Batch 50/156 - Loss 0.0251\n",
            "Batch 100/156 - Loss 0.3014\n",
            "Batch 150/156 - Loss 0.6138\n",
            "Epoch 25: Loss=0.2795  Accuracy=97.96%\n",
            "Test set: loss=2.0693, accuracy=88.34%\n",
            "Test set: loss=2.0019, accuracy=89.27%\n",
            "Batch 50/156 - Loss 0.0133\n",
            "Batch 100/156 - Loss 0.3159\n",
            "Batch 150/156 - Loss 0.0189\n",
            "Epoch 26: Loss=0.2629  Accuracy=98.22%\n",
            "Test set: loss=2.1601, accuracy=88.34%\n",
            "Test set: loss=1.9096, accuracy=89.54%\n",
            "Batch 50/156 - Loss 0.3276\n",
            "Batch 100/156 - Loss 0.3132\n",
            "Batch 150/156 - Loss 0.0279\n",
            "Epoch 27: Loss=0.2629  Accuracy=98.21%\n",
            "Test set: loss=1.9779, accuracy=89.27%\n",
            "Test set: loss=1.8847, accuracy=89.67%\n",
            "Batch 50/156 - Loss 0.0172\n",
            "Batch 100/156 - Loss 0.0350\n",
            "Batch 150/156 - Loss 0.0160\n",
            "Epoch 28: Loss=0.2256  Accuracy=98.63%\n",
            "Test set: loss=2.1756, accuracy=85.30%\n",
            "Test set: loss=1.8810, accuracy=87.55%\n",
            "Batch 50/156 - Loss 0.3212\n",
            "Batch 100/156 - Loss 0.6579\n",
            "Batch 150/156 - Loss 0.0152\n",
            "Epoch 29: Loss=0.2068  Accuracy=98.65%\n",
            "Test set: loss=2.1787, accuracy=87.81%\n",
            "Test set: loss=1.7724, accuracy=89.67%\n",
            "Batch 50/156 - Loss 0.5696\n",
            "Batch 100/156 - Loss 0.0159\n",
            "Batch 150/156 - Loss 0.4125\n",
            "Epoch 30: Loss=0.2476  Accuracy=98.35%\n",
            "Test set: loss=2.1225, accuracy=88.61%\n",
            "Test set: loss=1.9799, accuracy=88.74%\n",
            "Batch 50/156 - Loss 0.0207\n",
            "Batch 100/156 - Loss 0.3935\n",
            "Batch 150/156 - Loss 0.6234\n",
            "Epoch 31: Loss=0.2594  Accuracy=98.17%\n",
            "Test set: loss=2.3282, accuracy=87.42%\n",
            "Test set: loss=2.0406, accuracy=88.74%\n",
            "Batch 50/156 - Loss 0.0997\n",
            "Batch 100/156 - Loss 0.2686\n",
            "Batch 150/156 - Loss 0.0244\n",
            "Epoch 32: Loss=0.2453  Accuracy=98.39%\n",
            "Test set: loss=2.0912, accuracy=88.61%\n",
            "Test set: loss=1.9262, accuracy=88.34%\n",
            "Batch 50/156 - Loss 0.0218\n",
            "Batch 100/156 - Loss 0.5057\n",
            "Batch 150/156 - Loss 1.6053\n",
            "Epoch 33: Loss=0.3082  Accuracy=97.82%\n",
            "Test set: loss=2.1974, accuracy=87.68%\n",
            "Test set: loss=1.8170, accuracy=89.93%\n",
            "Batch 50/156 - Loss 0.6189\n",
            "Batch 100/156 - Loss 0.0124\n",
            "Batch 150/156 - Loss 0.5552\n",
            "Epoch 34: Loss=0.2526  Accuracy=98.28%\n",
            "Test set: loss=2.6841, accuracy=84.77%\n",
            "Test set: loss=2.1534, accuracy=87.15%\n",
            "Batch 50/156 - Loss 0.8086\n",
            "Batch 100/156 - Loss 0.3344\n",
            "Batch 150/156 - Loss 0.3541\n",
            "Epoch 35: Loss=0.3234  Accuracy=97.85%\n",
            "Test set: loss=2.1253, accuracy=87.81%\n",
            "Test set: loss=1.9954, accuracy=88.48%\n",
            "Batch 50/156 - Loss 0.1017\n",
            "Batch 100/156 - Loss 0.0181\n",
            "Batch 150/156 - Loss 0.3273\n",
            "Epoch 36: Loss=0.2938  Accuracy=98.01%\n",
            "Test set: loss=2.3976, accuracy=85.70%\n",
            "Test set: loss=2.0512, accuracy=87.68%\n",
            "Batch 50/156 - Loss 0.3266\n",
            "Batch 100/156 - Loss 0.0147\n",
            "Batch 150/156 - Loss 0.0124\n",
            "Epoch 37: Loss=0.2487  Accuracy=98.33%\n",
            "Test set: loss=2.4034, accuracy=86.49%\n",
            "Test set: loss=2.0155, accuracy=89.14%\n",
            "Batch 50/156 - Loss 0.0149\n",
            "Batch 100/156 - Loss 0.6383\n",
            "Batch 150/156 - Loss 0.3141\n",
            "Epoch 38: Loss=0.2459  Accuracy=98.36%\n",
            "Test set: loss=2.4953, accuracy=85.83%\n",
            "Test set: loss=2.2044, accuracy=87.95%\n",
            "Batch 50/156 - Loss 0.6133\n",
            "Batch 100/156 - Loss 0.0156\n",
            "Batch 150/156 - Loss 0.5935\n",
            "Epoch 39: Loss=0.2776  Accuracy=98.02%\n",
            "Test set: loss=2.4646, accuracy=84.77%\n",
            "Test set: loss=1.9970, accuracy=87.42%\n",
            "Batch 50/156 - Loss 0.2364\n",
            "Batch 100/156 - Loss 0.0205\n",
            "Batch 150/156 - Loss 0.0243\n",
            "Epoch 40: Loss=0.2509  Accuracy=98.26%\n",
            "Test set: loss=2.4132, accuracy=86.23%\n",
            "Test set: loss=1.9556, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.0239\n",
            "Batch 100/156 - Loss 0.3165\n",
            "Batch 150/156 - Loss 0.0095\n",
            "Epoch 41: Loss=0.2062  Accuracy=98.65%\n",
            "Test set: loss=2.4917, accuracy=87.02%\n",
            "Test set: loss=2.1561, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.0075\n",
            "Batch 100/156 - Loss 0.1391\n",
            "Batch 150/156 - Loss 0.2105\n",
            "Epoch 42: Loss=0.1886  Accuracy=98.80%\n",
            "Test set: loss=2.4309, accuracy=86.62%\n",
            "Test set: loss=1.9446, accuracy=88.87%\n",
            "Batch 50/156 - Loss 0.3107\n",
            "Batch 100/156 - Loss 0.3242\n",
            "Batch 150/156 - Loss 1.1930\n",
            "Epoch 43: Loss=0.2121  Accuracy=98.67%\n",
            "Test set: loss=2.2855, accuracy=86.49%\n",
            "Test set: loss=2.0610, accuracy=87.68%\n",
            "Batch 50/156 - Loss 0.0583\n",
            "Batch 100/156 - Loss 0.0322\n",
            "Batch 150/156 - Loss 0.6262\n",
            "Epoch 44: Loss=0.2146  Accuracy=98.48%\n",
            "Test set: loss=2.5372, accuracy=86.49%\n",
            "Test set: loss=2.1812, accuracy=87.95%\n",
            "Batch 50/156 - Loss 0.2627\n",
            "Batch 100/156 - Loss 0.3266\n",
            "Batch 150/156 - Loss 0.4139\n",
            "Epoch 45: Loss=0.2633  Accuracy=98.15%\n",
            "Test set: loss=2.6515, accuracy=84.37%\n",
            "Test set: loss=2.0417, accuracy=87.68%\n",
            "Batch 50/156 - Loss 0.0144\n",
            "Batch 100/156 - Loss 0.0181\n",
            "Batch 150/156 - Loss 0.7915\n",
            "Epoch 46: Loss=0.2274  Accuracy=98.29%\n",
            "Test set: loss=1.9791, accuracy=88.08%\n",
            "Test set: loss=1.9807, accuracy=88.48%\n",
            "Batch 50/156 - Loss 0.0146\n",
            "Batch 100/156 - Loss 0.3466\n",
            "Batch 150/156 - Loss 0.1005\n",
            "Epoch 47: Loss=0.2779  Accuracy=98.01%\n",
            "Test set: loss=2.5020, accuracy=84.64%\n",
            "Test set: loss=2.1573, accuracy=86.89%\n",
            "Batch 50/156 - Loss 0.3107\n",
            "Batch 100/156 - Loss 0.5338\n",
            "Batch 150/156 - Loss 0.0083\n",
            "Epoch 48: Loss=0.1867  Accuracy=98.85%\n",
            "Test set: loss=2.2289, accuracy=88.74%\n",
            "Test set: loss=1.8621, accuracy=90.46%\n",
            "Batch 50/156 - Loss 0.0345\n",
            "Batch 100/156 - Loss 0.0357\n",
            "Batch 150/156 - Loss 0.0119\n",
            "Epoch 49: Loss=0.3242  Accuracy=97.69%\n",
            "Test set: loss=2.4333, accuracy=86.89%\n",
            "Test set: loss=2.0804, accuracy=88.61%\n",
            "Batch 50/156 - Loss 0.3199\n",
            "Batch 100/156 - Loss 0.0129\n",
            "Batch 150/156 - Loss 0.3036\n",
            "Epoch 50: Loss=0.1729  Accuracy=98.89%\n",
            "Test set: loss=2.3130, accuracy=86.36%\n",
            "Test set: loss=2.3674, accuracy=86.75%\n",
            "Batch 50/156 - Loss 0.0064\n",
            "Batch 100/156 - Loss 0.0047\n",
            "Batch 150/156 - Loss 0.3226\n",
            "Epoch 1: Loss=0.1439  Accuracy=99.00%\n",
            "Test set: loss=2.1067, accuracy=86.74%\n",
            "Test set: loss=2.2546, accuracy=87.00%\n",
            "New best model saved at epoch 1 with average acc 86.74, female acc 86.74, male acc 87.00%\n",
            "Batch 50/156 - Loss 0.4325\n",
            "Batch 100/156 - Loss 0.2875\n",
            "Batch 150/156 - Loss 0.1824\n",
            "Epoch 2: Loss=0.1360  Accuracy=99.02%\n",
            "Test set: loss=1.9990, accuracy=87.00%\n",
            "Test set: loss=1.9651, accuracy=87.27%\n",
            "New best model saved at epoch 2 with average acc 87.00, female acc 87.00, male acc 87.27%\n",
            "Batch 50/156 - Loss 0.3218\n",
            "Batch 100/156 - Loss 0.0048\n",
            "Batch 150/156 - Loss 0.6162\n",
            "Epoch 3: Loss=0.1421  Accuracy=99.05%\n",
            "Test set: loss=2.1407, accuracy=87.80%\n",
            "Test set: loss=1.9038, accuracy=89.66%\n",
            "New best model saved at epoch 3 with average acc 87.80, female acc 87.80, male acc 89.66%\n",
            "Batch 50/156 - Loss 0.0419\n",
            "Batch 100/156 - Loss 0.0112\n",
            "Batch 150/156 - Loss 0.3210\n",
            "Epoch 4: Loss=0.1711  Accuracy=98.83%\n",
            "Test set: loss=2.9310, accuracy=81.96%\n",
            "Test set: loss=2.3912, accuracy=84.88%\n",
            "Batch 50/156 - Loss 0.2816\n",
            "Batch 100/156 - Loss 0.0223\n",
            "Batch 150/156 - Loss 0.0053\n",
            "Epoch 5: Loss=0.1695  Accuracy=98.73%\n",
            "Test set: loss=2.4235, accuracy=87.27%\n",
            "Test set: loss=1.8903, accuracy=89.92%\n",
            "Batch 50/156 - Loss 0.0153\n",
            "Batch 100/156 - Loss 0.0036\n",
            "Batch 150/156 - Loss 0.0048\n",
            "Epoch 6: Loss=0.1368  Accuracy=99.08%\n",
            "Test set: loss=1.9738, accuracy=88.86%\n",
            "Test set: loss=1.8377, accuracy=88.33%\n",
            "Batch 50/156 - Loss 0.0062\n",
            "Batch 100/156 - Loss 0.0248\n",
            "Batch 150/156 - Loss 0.0038\n",
            "Epoch 7: Loss=0.0909  Accuracy=99.41%\n",
            "Test set: loss=2.1501, accuracy=88.86%\n",
            "Test set: loss=1.7508, accuracy=90.72%\n",
            "New best model saved at epoch 7 with average acc 88.86, female acc 88.86, male acc 90.72%\n",
            "Batch 50/156 - Loss 0.0028\n",
            "Batch 100/156 - Loss 0.3064\n",
            "Batch 150/156 - Loss 0.2555\n",
            "Epoch 8: Loss=0.0998  Accuracy=99.42%\n",
            "Test set: loss=1.8658, accuracy=88.86%\n",
            "Test set: loss=1.8192, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0075\n",
            "Batch 100/156 - Loss 0.0019\n",
            "Batch 150/156 - Loss 0.2286\n",
            "Epoch 9: Loss=0.1319  Accuracy=99.07%\n",
            "Test set: loss=2.2120, accuracy=87.00%\n",
            "Test set: loss=1.9329, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.2065\n",
            "Batch 100/156 - Loss 0.0065\n",
            "Batch 150/156 - Loss 0.5751\n",
            "Epoch 10: Loss=0.1650  Accuracy=98.83%\n",
            "Test set: loss=2.3400, accuracy=85.94%\n",
            "Test set: loss=1.7834, accuracy=89.12%\n",
            "Batch 50/156 - Loss 0.0108\n",
            "Batch 100/156 - Loss 0.2893\n",
            "Batch 150/156 - Loss 0.0044\n",
            "Epoch 11: Loss=0.1498  Accuracy=98.91%\n",
            "Test set: loss=2.4639, accuracy=85.68%\n",
            "Test set: loss=2.0492, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.0054\n",
            "Batch 100/156 - Loss 0.0017\n",
            "Batch 150/156 - Loss 0.2873\n",
            "Epoch 12: Loss=0.0863  Accuracy=99.48%\n",
            "Test set: loss=2.1969, accuracy=85.94%\n",
            "Test set: loss=1.9093, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.2883\n",
            "Batch 100/156 - Loss 0.1738\n",
            "Batch 150/156 - Loss 0.3423\n",
            "Epoch 13: Loss=0.1435  Accuracy=98.91%\n",
            "Test set: loss=2.0232, accuracy=85.15%\n",
            "Test set: loss=1.9110, accuracy=86.74%\n",
            "Batch 50/156 - Loss 0.0044\n",
            "Batch 100/156 - Loss 0.0056\n",
            "Batch 150/156 - Loss 0.0070\n",
            "Epoch 14: Loss=0.1311  Accuracy=99.09%\n",
            "Test set: loss=2.0613, accuracy=89.39%\n",
            "Test set: loss=1.9778, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.0021\n",
            "Batch 100/156 - Loss 0.0184\n",
            "Batch 150/156 - Loss 0.1763\n",
            "Epoch 15: Loss=0.1257  Accuracy=99.14%\n",
            "Test set: loss=2.0912, accuracy=86.74%\n",
            "Test set: loss=1.8475, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0070\n",
            "Batch 100/156 - Loss 0.5098\n",
            "Batch 150/156 - Loss 0.0084\n",
            "Epoch 16: Loss=0.1368  Accuracy=98.93%\n",
            "Test set: loss=2.6434, accuracy=86.21%\n",
            "Test set: loss=1.9280, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.0126\n",
            "Batch 100/156 - Loss 0.2848\n",
            "Batch 150/156 - Loss 0.0020\n",
            "Epoch 17: Loss=0.1280  Accuracy=99.15%\n",
            "Test set: loss=2.5649, accuracy=86.74%\n",
            "Test set: loss=2.0929, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.2741\n",
            "Batch 100/156 - Loss 0.0086\n",
            "Batch 150/156 - Loss 0.0038\n",
            "Epoch 18: Loss=0.1354  Accuracy=99.01%\n",
            "Test set: loss=2.5400, accuracy=84.08%\n",
            "Test set: loss=2.0080, accuracy=87.53%\n",
            "Batch 50/156 - Loss 0.0250\n",
            "Batch 100/156 - Loss 0.0031\n",
            "Batch 150/156 - Loss 0.3082\n",
            "Epoch 19: Loss=0.1146  Accuracy=99.17%\n",
            "Test set: loss=2.1630, accuracy=88.06%\n",
            "Test set: loss=1.8272, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.0031\n",
            "Batch 100/156 - Loss 0.0092\n",
            "Batch 150/156 - Loss 0.3137\n",
            "Epoch 20: Loss=0.0904  Accuracy=99.41%\n",
            "Test set: loss=2.2988, accuracy=85.94%\n",
            "Test set: loss=1.8033, accuracy=88.86%\n",
            "Batch 50/156 - Loss 0.0029\n",
            "Batch 100/156 - Loss 0.0040\n",
            "Batch 150/156 - Loss 0.6206\n",
            "Epoch 21: Loss=0.0949  Accuracy=99.33%\n",
            "Test set: loss=2.8285, accuracy=84.35%\n",
            "Test set: loss=1.9250, accuracy=87.53%\n",
            "Batch 50/156 - Loss 0.0049\n",
            "Batch 100/156 - Loss 0.0031\n",
            "Batch 150/156 - Loss 0.0102\n",
            "Epoch 22: Loss=0.1415  Accuracy=98.93%\n",
            "Test set: loss=2.0217, accuracy=87.00%\n",
            "Test set: loss=1.8310, accuracy=88.06%\n",
            "Batch 50/156 - Loss 0.0182\n",
            "Batch 100/156 - Loss 0.0098\n",
            "Batch 150/156 - Loss 0.1764\n",
            "Epoch 23: Loss=0.1613  Accuracy=98.78%\n",
            "Test set: loss=2.1578, accuracy=88.33%\n",
            "Test set: loss=1.8557, accuracy=89.66%\n",
            "Batch 50/156 - Loss 0.2997\n",
            "Batch 100/156 - Loss 0.1194\n",
            "Batch 150/156 - Loss 0.0053\n",
            "Epoch 24: Loss=0.1225  Accuracy=99.01%\n",
            "Test set: loss=2.6468, accuracy=86.74%\n",
            "Test set: loss=1.9254, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.3149\n",
            "Batch 100/156 - Loss 0.2996\n",
            "Batch 150/156 - Loss 0.0071\n",
            "Epoch 25: Loss=0.0874  Accuracy=99.39%\n",
            "Test set: loss=2.5449, accuracy=86.47%\n",
            "Test set: loss=2.1001, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0025\n",
            "Batch 100/156 - Loss 0.2315\n",
            "Batch 150/156 - Loss 0.0037\n",
            "Epoch 26: Loss=0.0489  Accuracy=99.72%\n",
            "Test set: loss=2.0085, accuracy=89.39%\n",
            "Test set: loss=1.9221, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.3178\n",
            "Batch 100/156 - Loss 0.0099\n",
            "Batch 150/156 - Loss 0.0089\n",
            "Epoch 27: Loss=0.0912  Accuracy=99.29%\n",
            "Test set: loss=2.5285, accuracy=87.27%\n",
            "Test set: loss=2.0261, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0046\n",
            "Batch 100/156 - Loss 0.0336\n",
            "Batch 150/156 - Loss 0.6507\n",
            "Epoch 28: Loss=0.1369  Accuracy=99.06%\n",
            "Test set: loss=2.2890, accuracy=85.41%\n",
            "Test set: loss=1.5584, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0020\n",
            "Batch 100/156 - Loss 0.0328\n",
            "Batch 150/156 - Loss 0.0040\n",
            "Epoch 29: Loss=0.0673  Accuracy=99.52%\n",
            "Test set: loss=2.4090, accuracy=88.06%\n",
            "Test set: loss=1.9723, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.2155\n",
            "Batch 100/156 - Loss 0.0033\n",
            "Batch 150/156 - Loss 0.0063\n",
            "Epoch 30: Loss=0.0929  Accuracy=99.37%\n",
            "Test set: loss=2.2951, accuracy=87.80%\n",
            "Test set: loss=1.6842, accuracy=91.51%\n",
            "Batch 50/156 - Loss 0.0047\n",
            "Batch 100/156 - Loss 0.0075\n",
            "Batch 150/156 - Loss 0.0091\n",
            "Epoch 31: Loss=0.1747  Accuracy=98.76%\n",
            "Test set: loss=2.2005, accuracy=87.53%\n",
            "Test set: loss=1.8195, accuracy=89.12%\n",
            "Batch 50/156 - Loss 0.0072\n",
            "Batch 100/156 - Loss 0.0072\n",
            "Batch 150/156 - Loss 0.2900\n",
            "Epoch 32: Loss=0.1185  Accuracy=99.18%\n",
            "Test set: loss=1.9659, accuracy=89.92%\n",
            "Test set: loss=1.7010, accuracy=90.98%\n",
            "New best model saved at epoch 32 with average acc 89.92, female acc 89.92, male acc 90.98%\n",
            "Batch 50/156 - Loss 0.0054\n",
            "Batch 100/156 - Loss 0.3328\n",
            "Batch 150/156 - Loss 0.0035\n",
            "Epoch 33: Loss=0.1162  Accuracy=99.13%\n",
            "Test set: loss=1.6976, accuracy=89.66%\n",
            "Test set: loss=1.7782, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.3090\n",
            "Batch 100/156 - Loss 0.0043\n",
            "Batch 150/156 - Loss 0.0099\n",
            "Epoch 34: Loss=0.1001  Accuracy=99.29%\n",
            "Test set: loss=1.8867, accuracy=89.12%\n",
            "Test set: loss=1.7994, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.2581\n",
            "Batch 100/156 - Loss 0.0038\n",
            "Batch 150/156 - Loss 0.0035\n",
            "Epoch 35: Loss=0.0819  Accuracy=99.45%\n",
            "Test set: loss=2.5858, accuracy=86.47%\n",
            "Test set: loss=1.6631, accuracy=92.04%\n",
            "Batch 50/156 - Loss 0.0017\n",
            "Batch 100/156 - Loss 0.0060\n",
            "Batch 150/156 - Loss 0.0022\n",
            "Epoch 36: Loss=0.0546  Accuracy=99.65%\n",
            "Test set: loss=2.1714, accuracy=88.86%\n",
            "Test set: loss=1.8665, accuracy=90.19%\n",
            "Batch 50/156 - Loss 0.0020\n",
            "Batch 100/156 - Loss 0.0018\n",
            "Batch 150/156 - Loss 0.0025\n",
            "Epoch 37: Loss=0.0610  Accuracy=99.60%\n",
            "Test set: loss=1.9829, accuracy=90.45%\n",
            "Test set: loss=1.9347, accuracy=90.98%\n",
            "New best model saved at epoch 37 with average acc 90.45, female acc 90.45, male acc 90.98%\n",
            "Batch 50/156 - Loss 0.0026\n",
            "Batch 100/156 - Loss 0.6546\n",
            "Batch 150/156 - Loss 0.0033\n",
            "Epoch 38: Loss=0.0487  Accuracy=99.68%\n",
            "Test set: loss=2.1949, accuracy=89.39%\n",
            "Test set: loss=1.8651, accuracy=90.98%\n",
            "Batch 50/156 - Loss 0.0013\n",
            "Batch 100/156 - Loss 0.0024\n",
            "Batch 150/156 - Loss 0.0079\n",
            "Epoch 39: Loss=0.0601  Accuracy=99.62%\n",
            "Test set: loss=2.3052, accuracy=88.59%\n",
            "Test set: loss=1.7652, accuracy=91.25%\n",
            "Batch 50/156 - Loss 0.2069\n",
            "Batch 100/156 - Loss 0.0027\n",
            "Batch 150/156 - Loss 0.0044\n",
            "Epoch 40: Loss=0.0809  Accuracy=99.50%\n",
            "Test set: loss=2.1818, accuracy=87.00%\n",
            "Test set: loss=1.5646, accuracy=92.04%\n",
            "Batch 50/156 - Loss 0.3034\n",
            "Batch 100/156 - Loss 0.0096\n",
            "Batch 150/156 - Loss 0.3612\n",
            "Epoch 41: Loss=0.0887  Accuracy=99.39%\n",
            "Test set: loss=2.1637, accuracy=89.39%\n",
            "Test set: loss=1.7152, accuracy=91.51%\n",
            "Batch 50/156 - Loss 0.4698\n",
            "Batch 100/156 - Loss 0.0032\n",
            "Batch 150/156 - Loss 0.2834\n",
            "Epoch 42: Loss=0.0816  Accuracy=99.40%\n",
            "Test set: loss=2.4956, accuracy=86.21%\n",
            "Test set: loss=1.8142, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.3399\n",
            "Batch 100/156 - Loss 0.0792\n",
            "Batch 150/156 - Loss 0.2757\n",
            "Epoch 43: Loss=0.0697  Accuracy=99.49%\n",
            "Test set: loss=1.9769, accuracy=90.19%\n",
            "Test set: loss=1.7491, accuracy=91.51%\n",
            "New best model saved at epoch 43 with average acc 90.19, female acc 90.19, male acc 91.51%\n",
            "Batch 50/156 - Loss 0.0010\n",
            "Batch 100/156 - Loss 0.0029\n",
            "Batch 150/156 - Loss 0.0028\n",
            "Epoch 44: Loss=0.0526  Accuracy=99.63%\n",
            "Test set: loss=2.4992, accuracy=87.80%\n",
            "Test set: loss=1.8009, accuracy=90.98%\n",
            "Batch 50/156 - Loss 0.0014\n",
            "Batch 100/156 - Loss 0.0018\n",
            "Batch 150/156 - Loss 0.0560\n",
            "Epoch 45: Loss=0.0430  Accuracy=99.72%\n",
            "Test set: loss=2.1413, accuracy=88.06%\n",
            "Test set: loss=1.6879, accuracy=90.45%\n",
            "Batch 50/156 - Loss 0.0018\n",
            "Batch 100/156 - Loss 0.3272\n",
            "Batch 150/156 - Loss 0.0031\n",
            "Epoch 46: Loss=0.0501  Accuracy=99.67%\n",
            "Test set: loss=2.2511, accuracy=87.80%\n",
            "Test set: loss=1.9196, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0040\n",
            "Batch 100/156 - Loss 0.0018\n",
            "Batch 150/156 - Loss 0.0018\n",
            "Epoch 47: Loss=0.0732  Accuracy=99.43%\n",
            "Test set: loss=2.5632, accuracy=87.27%\n",
            "Test set: loss=2.0237, accuracy=89.92%\n",
            "Batch 50/156 - Loss 0.0018\n",
            "Batch 100/156 - Loss 0.0034\n",
            "Batch 150/156 - Loss 0.3226\n",
            "Epoch 48: Loss=0.0709  Accuracy=99.55%\n",
            "Test set: loss=2.0786, accuracy=87.53%\n",
            "Test set: loss=1.8171, accuracy=89.39%\n",
            "Batch 50/156 - Loss 0.0132\n",
            "Batch 100/156 - Loss 0.0036\n",
            "Batch 150/156 - Loss 0.0012\n",
            "Epoch 49: Loss=0.0925  Accuracy=99.27%\n",
            "Test set: loss=2.2174, accuracy=88.86%\n",
            "Test set: loss=2.2210, accuracy=88.59%\n",
            "Batch 50/156 - Loss 0.0051\n",
            "Batch 100/156 - Loss 0.0014\n",
            "Batch 150/156 - Loss 0.0493\n",
            "Epoch 50: Loss=0.0923  Accuracy=99.31%\n",
            "Test set: loss=2.2610, accuracy=85.94%\n",
            "Test set: loss=1.9600, accuracy=87.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title putting in the utils here for easier dev\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "V5RyGgI-xDUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=8/2550,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ozdMyUbKyike"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Selecting the first column of y (assuming it's the identity label)\n",
        "                loss = nn.CrossEntropyLoss()(output, y[:, 0])\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "P0OJ1YNLy6DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Selecting the first column of targets, assuming it represents the identity label\n",
        "        labels = targets[:, 0]\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train':  # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, labels)  # Use labels instead of targets\n",
        "\n",
        "        elif mode == 'adv_train_trades':  # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "BPenGmbpy_C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # change this to adam!!!\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "\n",
        "        val_loss_f, val_acc_f = evaluate(model, arc_head, val_loader_f, device)\n",
        "        val_loss_m, val_acc_m = evaluate(model, arc_head, val_loader_m, device)\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Avergae accuracy: {val_acc}, female: {val_acc_f}, male: {val_acc_m}')\n",
        "        print('================================================================')\n",
        "\n"
      ],
      "metadata": {
        "id": "toQXbD13z_VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# def train(model, train_loader, val_loader, pgd_attack,\n",
        "#           mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "#           checkpoint_path='model1.pt'):\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     # change this to adam!!!\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "#     best_acc = 0\n",
        "#     for epoch in range(epochs):\n",
        "#         # training\n",
        "#         train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "#         # evaluate clean accuracy\n",
        "#         test_loss, test_acc = evaluate(model, arc_head, val_loader, device)\n",
        "\n",
        "#         # remember best acc@1 and save checkpoint\n",
        "#         is_best = test_acc > best_acc\n",
        "#         best_acc = max(test_acc, best_acc)\n",
        "\n",
        "#         # save checkpoint if is a new best\n",
        "#         if is_best:\n",
        "#             torch.save(model.state_dict(), checkpoint_path)\n",
        "#         print(f'Accuracy: {test_acc}')\n",
        "#         print('================================================================')"
      ],
      "metadata": {
        "id": "EtEYn5jT-Cev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training loop with backbone using ArcFace\n",
        "\n",
        "# sanity check for attack, rasmus' suggestion\n",
        "\n",
        "\n",
        "epsilon = 8/255\n",
        "pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)  # instantiate the LinfPGDAttack\n",
        "training_mode = \"adv_train\"\n",
        "\n",
        "\n",
        "# note for us!! check the training loop and confirm whether it is similar to the previous one we had\n",
        "\n",
        "\n",
        "for proportion in proportions:\n",
        "    model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "    best_acc = 0.0\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize val_loader_f and val_loader_m to None\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    # Check if the subsets have any samples before creating DataLoaders\n",
        "    if len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=True)\n",
        "    if len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "    train(model, train_loader=train_loader, mode=training_mode, val_loader_f=val_loader_f, val_loader_m= val_loader_m, pgd_attack=pgd, learning_rate=0.001, checkpoint_path='model_adv.pt', epochs=70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "hmUtIMuSzm_r",
        "outputId": "bebf3e8f-3a5c-4599-eb2b-79038ae29702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 0.545271\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.505271\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.540523\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-1815b8fea78d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_m\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_loader_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_adv.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-e3fc131f7568>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader_f, val_loader_m, pgd_attack, mode, epochs, batch_size, learning_rate, momentum, weight_decay, checkpoint_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_ep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-d34fff666fa2>\u001b[0m in \u001b[0;36mtrain_ep\u001b[0;34m(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_ep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Selecting the first column of targets, assuming it represents the identity label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/celeba.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2296\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mResampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 )\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                     \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpulls_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                         \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check for attack, rasmus' suggestion\n",
        "\n",
        "epsilon = 8/255\n",
        "pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)  # instantiate the LinfPGDAttack\n",
        "training_mode = \"adv_train\"\n",
        "\n",
        "\n",
        "# note for us!! check the training loop and confirm whether it is similar to the previous one we had\n",
        "\n",
        "\n",
        "for proportion in proportions:\n",
        "    model = nn.Sequential(backbone, nn.Flatten(start_dim=1)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(arc_head.parameters()), lr=1e-3)\n",
        "    best_acc = 0.0\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize val_loader_f and val_loader_m to None\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    # Check if the subsets have any samples before creating DataLoaders\n",
        "    if len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=True)\n",
        "    if len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "    train(model, train_loader=train_loader, mode=training_mode, val_loader_f=val_loader_f, val_loader_m= val_loader_m, pgd_attack=pgd, learning_rate=0.001, checkpoint_path='model_adv.pt', epochs=70)\n"
      ],
      "metadata": {
        "id": "Z7dq1q1Xg37s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting this with a simpler model\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                loss = nn.CrossEntropyLoss()(output, y)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "iI3HBGKtzrWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "def make_dataloader(data_path, batch_size):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform_train)\n",
        "    val_dataset = datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def eval_test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            test_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "def eval_robust(model, test_loader, pgd_attack, device):\n",
        "    model.eval()\n",
        "    robust_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv = pgd_attack(inputs, targets)\n",
        "            outputs = model(adv)\n",
        "            robust_loss += F.cross_entropy(outputs, targets).item()\n",
        "            pred = outputs.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
        "    robust_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('LinfPGD Attack: Average loss: {:.4f}, Robust Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        robust_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    robust_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return robust_loss, robust_accuracy\n",
        "\n",
        "\n",
        "def mixup_data(x, y, mixup_alpha=1.0):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    '''\n",
        "    Source https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n",
        "    '''\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def trades_loss(model,\n",
        "                x_natural,\n",
        "                y,\n",
        "                optimizer,\n",
        "                step_size=0.003,\n",
        "                epsilon=8/255,\n",
        "                perturb_steps=10,\n",
        "                beta=1.0):\n",
        "    '''\n",
        "    Source https://github.com/yaodongyu/TRADES/blob/master/trades.py\n",
        "    '''\n",
        "    # define KL-loss\n",
        "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
        "    model.eval()\n",
        "    batch_size = len(x_natural)\n",
        "\n",
        "    # generate adversarial example\n",
        "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
        "    for _ in range(perturb_steps):\n",
        "        x_adv.requires_grad_()\n",
        "        with torch.enable_grad():\n",
        "            loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                   F.softmax(model(x_natural), dim=1))\n",
        "        grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
        "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
        "        x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
        "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
        "\n",
        "    # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculate robust loss\n",
        "    logits = model(x_natural)\n",
        "    loss_natural = F.cross_entropy(logits, y)\n",
        "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
        "                                                    F.softmax(model(x_natural), dim=1))\n",
        "    loss = loss_natural + beta * loss_robust\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Lp-OlBD7pJmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        labels = targets[:, 0] # the first column is the identity label\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            adv_x = pgd_attack(inputs, targets)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer)\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
        "        #     adv_x = pgd_attack(inputs, targets)\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "id": "fKvyFN-QiJQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "        # evaluate clean accuracy\n",
        "        test_loss, test_acc = eval_test(model, val_loader, device)\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = test_acc > best_acc\n",
        "        best_acc = max(test_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print('================================================================')"
      ],
      "metadata": {
        "id": "H38K2cfuoASW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title resnet module\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n"
      ],
      "metadata": {
        "id": "4nLHgl8ekD7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title adjusting this with a simpler model\n",
        "\n",
        "\n",
        "class LinfPGDAttack(nn.Module):\n",
        "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.steps = steps\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def perturb(self, x_natural, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cross-entropy loss with respect to the input\n",
        "        image `x_adv` and updates the image based on the gradient direction. The\n",
        "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
        "        and is finally clamped to ensure pixel values are valid.\n",
        "\n",
        "        The resulting perturbed image is returned.\n",
        "        \"\"\"\n",
        "        # *********** Your code starts here ***********\n",
        "        x_adv = x_natural.clone().requires_grad_(True)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = y[:, 0] # Assuming the first column is the identity label\n",
        "        with torch.enable_grad():\n",
        "            for i in range(self.steps):\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                # calculate loss\n",
        "                output = self.model(x_adv)\n",
        "                # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "                loss = nn.CrossEntropyLoss()(output, labels)\n",
        "\n",
        "\n",
        "                # gradient\n",
        "                grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "\n",
        "                # clipping\n",
        "                perturbation = torch.clamp(self.step_size * torch.sign(grad), -self.epsilon, self.epsilon)\n",
        "\n",
        "                # clamping\n",
        "                x_adv = torch.clamp(x_adv + perturbation, 0, 1)\n",
        "\n",
        "        # *********** Your code ends here *************\n",
        "\n",
        "        return x_adv\n",
        "\n",
        "    def forward(self, x_natural, y):\n",
        "        x_adv = self.perturb(x_natural, y)\n",
        "        return x_adv\n",
        "\n",
        "# Modified train_ep function to handle multi-dimensional targets from CelebA\n",
        "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Extract the identity label from the multi-dimensional target tensor\n",
        "        labels = targets[:, 0] # Assuming the first column is the identity label\n",
        "\n",
        "\n",
        "        if mode == 'natural':\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
        "            model.eval()\n",
        "            # Pass the original multi-dimensional targets to the attack\n",
        "            adv_x = pgd_attack(inputs, targets) # The attack will extract labels internally\n",
        "            model.train()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(adv_x)\n",
        "            # Use the extracted identity labels as the target for CrossEntropyLoss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
        "            optimizer.zero_grad()\n",
        "            # Note: The original trades_loss function expects a 1D target tensor.\n",
        "            # You might need to adapt trades_loss similar to LinfPGDAttack if you plan to use this mode\n",
        "            # with CelebA's multi-dimensional targets. For now, using extracted labels might work,\n",
        "            # but the original TRADES formulation uses clean labels for the KL divergence part.\n",
        "            # This might require further adjustments based on the specific TRADES implementation\n",
        "            # you are using.\n",
        "            # Assuming for now that trades_loss can handle the extracted 1D labels.\n",
        "            loss = trades_loss(model=model, x_natural=inputs, y=labels, optimizer=optimizer)\n",
        "\n",
        "\n",
        "        # elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
        "        #     model.eval()\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels.\n",
        "        #     benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, labels)\n",
        "        #     adv_x = pgd_attack(inputs, targets) # Pass original targets to attack\n",
        "        #     # Mixup needs 1D targets. You would need to modify mixup_data to work with the extracted labels from adv_x?\n",
        "        #     # This part of mixup with adversarial training might need careful consideration of how targets are handled.\n",
        "        #     adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, labels) # Using extracted labels\n",
        "\n",
        "\n",
        "        #     model.train()\n",
        "        #     optimizer.zero_grad()\n",
        "\n",
        "        #     benign_outputs = model(benign_inputs)\n",
        "        #     adv_outputs = model(adv_inputs)\n",
        "        #     # Use the extracted 1D labels for criterion\n",
        "        #     loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
        "        #     loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
        "\n",
        "        #     loss = (loss_1 + loss_2) / 2\n",
        "\n",
        "        else:\n",
        "            print(\"No training mode specified.\")\n",
        "            raise ValueError()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
        "\n",
        "# Keep the rest of the training loop as is, ensuring the train function calls the modified train_ep\n",
        "# and evaluate functions (which already handle the identity label extraction).\n",
        "# Make sure the 'train' function signature matches how it's called in the loop:\n",
        "# train(model, train_loader=train_loader, mode=training_mode, val_loader_f=val_loader_f, val_loader_m= val_loader_m, pgd_attack=pgd, learning_rate=0.001, checkpoint_path='model_adv.pt', epochs=70)\n",
        "# The 'train' function you provided in the notebook takes 'val_loader', but your calling code\n",
        "# passes 'val_loader_f'. You should update the train function signature or the call site\n",
        "# to be consistent. Since you added logic for val_loader_f and val_loader_m in the loop\n",
        "# where the error occurred, it seems you intend to evaluate on female and male subsets\n",
        "# separately during training. You should adjust the 'train' function to accept both\n",
        "# val_loader_f and val_loader_m and call your 'evaluate' function twice for each epoch.\n",
        "\n",
        "\n",
        "# Here's the updated train function to accept female and male validation loaders\n",
        "def train(model, train_loader, val_loader_f, val_loader_m, pgd_attack,\n",
        "          mode='natural', epochs=25, batch_size=256, learning_rate=0.001, momentum=0.9, weight_decay=2e-4,\n",
        "          checkpoint_path='model1.pt'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # change this to adam!!!\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "    # Note: Your training loop defined the optimizer with arc_head parameters.\n",
        "    # The train function here needs to accept or define the optimizer appropriately.\n",
        "    # Let's assume the optimizer is defined outside and passed in, or redefined here\n",
        "    # to include arc_head parameters if using ArcFace. If using the simple ResNet18\n",
        "    # without ArcFace for this adversarial training part, the current optimizer definition\n",
        "    # (SGD or Adam on model.parameters() only) might be correct.\n",
        "    # Based on the trace, it seems you are using the simple ResNet18 for the PGD attack part.\n",
        "    # Let's keep the SGD optimizer as in the original train function definition.\n",
        "    # If you are using ArcFace, you need to adapt this train function as well.\n",
        "\n",
        "    # Based on the code in the failing cell block, the optimizer was defined *inside* the loop\n",
        "    # and included arc_head parameters. Let's adjust the train function to match that,\n",
        "    # assuming you are training with ArcFace in this section.\n",
        "    # **However**, the eval_test and eval_robust functions called within this 'train' function\n",
        "    # only take 'model', not 'arc_head'. You will need to adapt eval_test and eval_robust\n",
        "    # if they are intended to work with the ArcFace setup (which predicts identity classes\n",
        "    # using features from the backbone and the arc_head).\n",
        "    # Let's assume for now that the adversarial training is on the simpler ResNet18\n",
        "    # for a standard classification task (which seems to be what the PGD attack code expects),\n",
        "    # and the ArcFace training is a separate block of code. If you intend to do adversarial\n",
        "    # training *with* the ArcFace setup, you'll need more significant modifications to the\n",
        "    # attack and evaluation functions.\n",
        "\n",
        "    # Reverting to the optimizer definition as seen in the cell where the error occurred,\n",
        "    # which includes arc_head parameters, assuming you want to train the entire ArcFace model adversarially.\n",
        "    # **IMPORTANT:** This requires adapting eval_test and eval_robust to work with the ArcFace model.\n",
        "    # For simplicity and to fix the immediate error, let's assume the PGD attack is on the\n",
        "    # simple classification task and the ArcFace part is separate, or you need to pass both\n",
        "    # model and arc_head to the train_ep and evaluation functions.\n",
        "\n",
        "    # Let's assume the intention is to train the simple ResNet18 adversarially.\n",
        "    # In this case, the optimizer should only optimize model.parameters().\n",
        "    # If you want to adversarially train the ArcFace setup, you need to pass\n",
        "    # both the model (backbone) and arc_head to train_ep and the attack.\n",
        "\n",
        "    # Given the structure of the failing code block, it seems you are initializing a simple ResNet18\n",
        "    # *inside* the loop for each proportion and then calling this `train` function.\n",
        "    # This suggests adversarial training on the simple ResNet18.\n",
        "    # The optimizer should then be for the model's parameters only.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Using Adam as in your failing block, but only for model\n",
        "\n",
        "    best_acc = 0.0 # Keep track of best average accuracy across genders\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        # Pass the extracted labels in train_ep as modified above\n",
        "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
        "\n",
        "\n",
        "        # Evaluate clean accuracy on both subsets using the eval_test function\n",
        "        # Note: eval_test currently calculates standard classification accuracy,\n",
        "        # not accuracy in an embedding space with ArcFace distance.\n",
        "        # If you want to evaluate with ArcFace, you need a different evaluation function.\n",
        "        # Assuming standard classification evaluation for now.\n",
        "        val_acc_f = 0.0\n",
        "        val_acc_m = 0.0\n",
        "        val_loss_f = 0.0\n",
        "        val_loss_m = 0.0\n",
        "\n",
        "        if val_loader_f and len(val_loader_f.dataset) > 0:\n",
        "            # eval_test is designed for 1D targets, but the DataLoader yields multi-dimensional targets.\n",
        "            # You need to adapt eval_test or the DataLoader to yield 1D targets for this evaluation.\n",
        "            # Or, pass the appropriate labels to eval_test.\n",
        "            # Let's modify eval_test slightly to extract the identity label.\n",
        "\n",
        "            # For consistency with the error source, let's assume eval_test needs 1D targets.\n",
        "            # We'll extract the labels before calling eval_test. This is not ideal as it\n",
        "            # involves iterating through the dataset again. A better approach would be to\n",
        "            # modify eval_test to accept multi-dimensional targets and extract the label internally.\n",
        "            # Given the constraint to fix the immediate error, let's assume eval_test needs 1D targets.\n",
        "            # However, eval_test takes a DataLoader, so modifying it is better.\n",
        "\n",
        "            # Let's adapt eval_test to extract the identity label.\n",
        "            # Need to redefine eval_test to handle the multi-dimensional target.\n",
        "            # **This requires modifying the eval_test function definition.**\n",
        "            # Assuming the `evaluate` function defined earlier (which takes arc_head and extracts labels)\n",
        "            # is the correct evaluation function for your setup, you should use that here.\n",
        "\n",
        "            # Replacing eval_test calls with the `evaluate` function used previously.\n",
        "            # This assumes `evaluate` is defined and available in this scope.\n",
        "            # The `evaluate` function also requires the arc_head, which is not passed to this `train` function.\n",
        "            # This highlights a mismatch in your code structure.\n",
        "            # Either the `train` function needs `arc_head` or the adversarial training part is meant for\n",
        "            # a model without ArcFace.\n",
        "\n",
        "            # Let's assume you want to adversarially train the simple ResNet18.\n",
        "            # In that case, the targets should be 1D class labels, and the model should output logits\n",
        "            # for the number of classes (identities). The ResNet18 is instantiated with num_classes=1000,\n",
        "            # which aligns with the idea of predicting identity.\n",
        "            # The `CelebA` dataset yields `identity_labels` which is already a tensor where the first column\n",
        "            # is the identity.\n",
        "\n",
        "            # Let's return to modifying `train_ep` and `LinfPGDAttack` to handle the multi-dimensional\n",
        "            # targets from the DataLoader and extract the first column for the loss calculation.\n",
        "            # This was done in the code block above this function definition.\n",
        "\n",
        "            # Now, for evaluation, we need a function that evaluates the model on the identity prediction task.\n",
        "            # The `evaluate` function defined earlier does this with the ArcFace setup.\n",
        "            # If you are using the simple ResNet18 directly for classification,\n",
        "            # you need an evaluation function that takes the model and a DataLoader,\n",
        "            # extracts the identity label, and calculates accuracy. Let's use `eval_test` but\n",
        "            # modify it to extract the target.\n",
        "\n",
        "            # Redefine eval_test here to handle CelebA targets\n",
        "            def eval_test_celeba(model, dataloader, device):\n",
        "                model.eval()\n",
        "                test_loss = 0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs, targets in dataloader:\n",
        "                        inputs, targets = inputs.to(device), targets.to(device)\n",
        "                        labels = targets[:, 0] # Extract identity label\n",
        "                        outputs = model(inputs)\n",
        "                        test_loss += F.cross_entropy(outputs, labels).item() * inputs.size(0)\n",
        "                        pred = outputs.max(1, keepdim=True)[1]\n",
        "                        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "                        total += inputs.size(0)\n",
        "                test_loss /= total if total > 0 else 1\n",
        "                accuracy = 100. * correct / total if total > 0 else 0\n",
        "\n",
        "                print(f'Test: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)')\n",
        "                return test_loss, accuracy\n",
        "\n",
        "            val_loss_f, val_acc_f = eval_test_celeba(model, val_loader_f, device)\n",
        "\n",
        "        if val_loader_m and len(val_loader_m.dataset) > 0:\n",
        "            val_loss_m, val_acc_m = eval_test_celeba(model, val_loader_m, device)\n",
        "\n",
        "        val_acc = (val_acc_f + val_acc_m) / 2\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        # save checkpoint if is a new best\n",
        "        if is_best:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Average accuracy: {val_acc:.2f}, female: {val_acc_f:.2f}, male: {val_acc_m:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "fLKbosPFkqTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "epsilon = 8/255\n",
        "training_mode = \"adv_train\" # Or 'natural' if you want to train naturally\n",
        "batch_size = 64\n",
        "\n",
        "proportions = [0.25, 0.5, 0.75]\n",
        "\n",
        "for proportion in proportions:\n",
        "    # Re-initialize model and attack for each proportion if needed, otherwise move outside loop\n",
        "    # If training separately for each proportion, re-initialization is correct.\n",
        "    model = ResNet18(num_classes=1000).to(device) # ResNet for identity classification\n",
        "    # Note: number of classes (1000) should match the number of unique identities\n",
        "    # we filtered initially by top 1000 identitites but this might be limiting perhaps?\n",
        "    # it gives very few examples on the test set\n",
        "\n",
        "\n",
        "    num_identity_classes = 1000 # Assuming the ResNet18 model is configured for 1000 classes\n",
        "    model = ResNet18(num_classes=num_identity_classes).to(device)\n",
        "\n",
        "    pgd = LinfPGDAttack(model, epsilon=epsilon, step_size = epsilon/10, steps = 10)\n",
        "\n",
        "    # train function definition already includes criterion and optimizer definition.\n",
        "    # Move best_acc outside the inner epoch loop within the train function.\n",
        "    # The train function saves checkpoint, so best_acc is managed internally.\n",
        "\n",
        "    train_loader = DataLoader(train_subsets[proportion], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader_f = None\n",
        "    val_loader_m = None\n",
        "\n",
        "    if proportion in test_subsets_f and len(test_subsets_f[proportion].indices) > 0:\n",
        "        val_loader_f = DataLoader(test_subsets_f[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "    if proportion in test_subsets_m and len(test_subsets_m[proportion].indices) > 0:\n",
        "        val_loader_m = DataLoader(test_subsets_m[proportion], batch_size=batch_size, shuffle=False) # Shuffle usually False for validation\n",
        "\n",
        "\n",
        "    # call the modified train function\n",
        "    train(model, train_loader=train_loader, mode=training_mode,\n",
        "          val_loader_f=val_loader_f, val_loader_m=val_loader_m,\n",
        "          pgd_attack=pgd, learning_rate=0.001,\n",
        "          checkpoint_path=f'model_adv_prop{int(proportion*100)}.pt', epochs=70) # Save checkpoints with proportion"
      ],
      "metadata": {
        "id": "-THEtjHpv_T_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc77347-7f51-4dd7-b404-a3fb77e90a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Train Epoch: 0 [00064/39936 (1%)]\t Loss: 6.991986\n",
            "Train Epoch: 0 [03264/39936 (33%)]\t Loss: 0.536099\n",
            "Train Epoch: 0 [06464/39936 (65%)]\t Loss: 0.628707\n",
            "Train Epoch: 0 [09664/39936 (97%)]\t Loss: 0.667225\n",
            "Test: Average loss: 0.3262, Accuracy: 346/377 (92%)\n",
            "Test: Average loss: 0.3451, Accuracy: 341/377 (90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 1 [00064/39936 (1%)]\t Loss: 0.614658\n",
            "Train Epoch: 1 [03264/39936 (33%)]\t Loss: 0.583970\n",
            "Train Epoch: 1 [06464/39936 (65%)]\t Loss: 0.703626\n",
            "Train Epoch: 1 [09664/39936 (97%)]\t Loss: 0.613131\n",
            "Test: Average loss: 0.6440, Accuracy: 285/377 (76%)\n",
            "Test: Average loss: 0.6484, Accuracy: 280/377 (74%)\n",
            "Average accuracy: 74.93, female: 75.60, male: 74.27\n",
            "Train Epoch: 2 [00064/39936 (1%)]\t Loss: 0.688969\n",
            "Train Epoch: 2 [03264/39936 (33%)]\t Loss: 0.693812\n",
            "Train Epoch: 2 [06464/39936 (65%)]\t Loss: 0.629332\n",
            "Train Epoch: 2 [09664/39936 (97%)]\t Loss: 0.655707\n",
            "Test: Average loss: 0.4370, Accuracy: 344/377 (91%)\n",
            "Test: Average loss: 0.4509, Accuracy: 337/377 (89%)\n",
            "Average accuracy: 90.32, female: 91.25, male: 89.39\n",
            "Train Epoch: 3 [00064/39936 (1%)]\t Loss: 0.615830\n",
            "Train Epoch: 3 [03264/39936 (33%)]\t Loss: 0.599249\n",
            "Train Epoch: 3 [06464/39936 (65%)]\t Loss: 0.625908\n",
            "Train Epoch: 3 [09664/39936 (97%)]\t Loss: 0.647885\n",
            "Test: Average loss: 0.4701, Accuracy: 346/377 (92%)\n",
            "Test: Average loss: 0.4788, Accuracy: 341/377 (90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 4 [00064/39936 (1%)]\t Loss: 0.628772\n",
            "Train Epoch: 4 [03264/39936 (33%)]\t Loss: 0.725800\n",
            "Train Epoch: 4 [06464/39936 (65%)]\t Loss: 0.615096\n",
            "Train Epoch: 4 [09664/39936 (97%)]\t Loss: 0.591036\n",
            "Test: Average loss: 0.3856, Accuracy: 346/377 (92%)\n",
            "Test: Average loss: 0.3988, Accuracy: 341/377 (90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 5 [00064/39936 (1%)]\t Loss: 0.635007\n",
            "Train Epoch: 5 [03264/39936 (33%)]\t Loss: 0.651074\n",
            "Train Epoch: 5 [06464/39936 (65%)]\t Loss: 0.656349\n",
            "Train Epoch: 5 [09664/39936 (97%)]\t Loss: 0.686168\n",
            "Test: Average loss: 0.3699, Accuracy: 346/377 (92%)\n",
            "Test: Average loss: 0.3862, Accuracy: 341/377 (90%)\n",
            "Average accuracy: 91.11, female: 91.78, male: 90.45\n",
            "Train Epoch: 6 [00064/39936 (1%)]\t Loss: 0.639416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R382yizsxyUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}